"""
LLM API å®¢æˆ·ç«¯ï¼šç»Ÿä¸€å°è£… LLM API è°ƒç”¨
æ”¯æŒ Azure OpenAI å’Œ OpenAIï¼Œæä¾›é‡è¯•ã€è¶…æ—¶ç­‰åŠŸèƒ½
"""

import logging
import time
import json
from typing import Optional, Dict, Any, List
from openai import AzureOpenAI, OpenAI

# ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from config import LLMConfig
except ImportError:
    from ..config import LLMConfig

logger = logging.getLogger(__name__)


class LLMClient:
    """
    LLM API ç»Ÿä¸€å®¢æˆ·ç«¯
    å°è£… Azure OpenAI å’Œ OpenAI API è°ƒç”¨
    """

    def __init__(self, provider: Optional[str] = None):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯

        Args:
            provider: 'azure'ã€'openai' æˆ– 'dashscope'ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        """
        self.provider = provider or LLMConfig.PROVIDER
        self.client = None
        self._initialize_client()

    def _initialize_client(self):
        """åˆå§‹åŒ– LLM API å®¢æˆ·ç«¯"""
        try:
            if self.provider == "azure":
                self.client = AzureOpenAI(
                    api_key=LLMConfig.AZURE_OPENAI_API_KEY,
                    api_version=LLMConfig.AZURE_OPENAI_API_VERSION,
                    azure_endpoint=LLMConfig.AZURE_OPENAI_ENDPOINT
                )
                self.model_name = LLMConfig.DEPLOYMENT_NAME
                logger.info(
                    f"âœ… Azure OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.model_name}"
                )

            elif self.provider == "openai":
                self.client = OpenAI(
                    api_key=LLMConfig.OPENAI_API_KEY
                )
                self.model_name = LLMConfig.OPENAI_MODEL
                logger.info(
                    f"âœ… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.model_name}"
                )

            elif self.provider == "dashscope":
                self.client = OpenAI(
                    api_key=LLMConfig.DASHSCOPE_API_KEY,
                    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
                )
                self.model_name = LLMConfig.DASHSCOPE_MODEL
                logger.info(
                    f"âœ… DashScope å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.model_name}"
                )

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ provider: {self.provider}")

        except Exception as e:
            logger.error(f"âŒ LLM å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        response_format: Optional[Dict[str, str]] = None
    ) -> str:
        """
        è°ƒç”¨ Chat Completion API

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼: [{"role": "user", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ Token æ•°
            response_format: å“åº”æ ¼å¼ï¼Œä¾‹å¦‚ {"type": "json_object"}

        Returns:
            LLM å“åº”æ–‡æœ¬
        """
        temperature = temperature or LLMConfig.TEMPERATURE
        max_tokens = max_tokens or LLMConfig.MAX_TOKENS

        for attempt in range(LLMConfig.MAX_RETRIES):
            try:
                logger.debug(
                    f"ğŸ”„ LLM API è°ƒç”¨ (å°è¯• {attempt + 1}/{LLMConfig.MAX_RETRIES})"
                )

                # æ„å»ºè¯·æ±‚å‚æ•°
                kwargs = {
                    "model": self.model_name,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }

                # æ·»åŠ å“åº”æ ¼å¼ï¼ˆå¦‚æœæŒ‡å®šï¼‰
                if response_format:
                    kwargs["response_format"] = response_format

                # è°ƒç”¨ API
                start_time = time.time()
                response = self.client.chat.completions.create(**kwargs)
                elapsed = time.time() - start_time

                # æå–å“åº”å†…å®¹
                content = response.choices[0].message.content
                usage = response.usage

                logger.info(
                    f"âœ… LLM API è°ƒç”¨æˆåŠŸ "
                    f"(è€—æ—¶: {elapsed:.2f}s, "
                    f"è¾“å…¥: {usage.prompt_tokens}, "
                    f"è¾“å‡º: {usage.completion_tokens})"
                )

                return content

            except Exception as e:
                logger.warning(
                    f"âš ï¸ LLM API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}): {e}"
                )

                if attempt < LLMConfig.MAX_RETRIES - 1:
                    wait_time = LLMConfig.RETRY_DELAY * (2 ** attempt)
                    logger.info(f"â³ ç­‰å¾… {wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                    raise

    def chat_with_system(
        self,
        system_prompt: str,
        user_prompt: str,
        **kwargs
    ) -> str:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯è°ƒç”¨ API

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™ chat æ–¹æ³•

        Returns:
            LLM å“åº”æ–‡æœ¬
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        return self.chat(messages, **kwargs)

    def chat_json(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """
        è°ƒç”¨ API å¹¶è¦æ±‚è¿”å› JSON æ ¼å¼

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            è§£æåçš„ JSON å¯¹è±¡
        """
        # ä½¿ç”¨ JSON æ¨¡å¼
        response = self.chat(
            messages,
            response_format={"type": "json_object"},
            **kwargs
        )

        try:
            return json.loads(response)
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
            logger.error(f"åŸå§‹å“åº”: {response}")
            raise

    def chat_json_with_system(
        self,
        system_prompt: str,
        user_prompt: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯è°ƒç”¨ API å¹¶è¿”å› JSON

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            è§£æåçš„ JSON å¯¹è±¡
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        return self.chat_json(messages, **kwargs)

    async def async_chat(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> str:
        """
        å¼‚æ­¥è°ƒç”¨ Chat APIï¼ˆç”¨äºæ‰¹é‡å¹¶å‘å¤„ç†ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLM å“åº”æ–‡æœ¬
        """
        # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¼‚æ­¥å°è£…
        # å®é™…ç”Ÿäº§ç¯å¢ƒä¸­å¯èƒ½éœ€è¦ä½¿ç”¨ httpx ç­‰å¼‚æ­¥åº“
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self.chat(messages, **kwargs)
        )

    def batch_chat(
        self,
        message_list: List[List[Dict[str, str]]],
        **kwargs
    ) -> List[str]:
        """
        æ‰¹é‡è°ƒç”¨ Chat APIï¼ˆä¸²è¡Œï¼‰

        Args:
            message_list: æ¶ˆæ¯åˆ—è¡¨çš„åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å“åº”åˆ—è¡¨
        """
        results = []
        total = len(message_list)

        for i, messages in enumerate(message_list, 1):
            logger.info(f"ğŸ“‹ æ‰¹é‡å¤„ç†è¿›åº¦: {i}/{total}")
            response = self.chat(messages, **kwargs)
            results.append(response)

        return results

    async def async_batch_chat(
        self,
        message_list: List[List[Dict[str, str]]],
        max_concurrent: int = 3,
        **kwargs
    ) -> List[str]:
        """
        æ‰¹é‡å¼‚æ­¥è°ƒç”¨ Chat APIï¼ˆå¹¶å‘ï¼‰

        Args:
            message_list: æ¶ˆæ¯åˆ—è¡¨çš„åˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            å“åº”åˆ—è¡¨
        """
        import asyncio
        from asyncio import Semaphore

        semaphore = Semaphore(max_concurrent)

        async def limited_chat(messages):
            async with semaphore:
                return await self.async_chat(messages, **kwargs)

        tasks = [limited_chat(messages) for messages in message_list]
        return await asyncio.gather(*tasks)

    def get_info(self) -> Dict[str, Any]:
        """
        è·å–å®¢æˆ·ç«¯ä¿¡æ¯

        Returns:
            åŒ…å« providerã€model ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        return {
            "provider": self.provider,
            "model": self.model_name,
            "temperature": LLMConfig.TEMPERATURE,
            "max_tokens": LLMConfig.MAX_TOKENS,
            "max_retries": LLMConfig.MAX_RETRIES
        }


# å…¨å±€å•ä¾‹
_global_llm_client = None


def get_llm_client() -> LLMClient:
    """
    è·å–å…¨å±€ LLM å®¢æˆ·ç«¯å•ä¾‹

    Returns:
        LLMClient å®ä¾‹
    """
    global _global_llm_client
    if _global_llm_client is None:
        _global_llm_client = LLMClient()
    return _global_llm_client


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)

    try:
        client = LLMClient()

        # æµ‹è¯•åŸºæœ¬è°ƒç”¨
        response = client.chat_with_system(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚",
            user_prompt="è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯ RAGã€‚"
        )
        print(f"\nå“åº”:\n{response}")

        # æµ‹è¯• JSON æ¨¡å¼
        json_response = client.chat_json_with_system(
            system_prompt="ä½ æ˜¯ä¸€ä¸ª JSON ç”Ÿæˆå™¨ã€‚è¯·å§‹ç»ˆè¿”å›æœ‰æ•ˆçš„ JSONã€‚",
            user_prompt='è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å« "name" å’Œ "description" å­—æ®µçš„ JSON å¯¹è±¡ï¼Œæè¿° RAG æŠ€æœ¯ã€‚'
        )
        print(f"\nJSON å“åº”:\n{json.dumps(json_response, indent=2, ensure_ascii=False)}")

        # å®¢æˆ·ç«¯ä¿¡æ¯
        info = client.get_info()
        print(f"\nå®¢æˆ·ç«¯ä¿¡æ¯:\n{json.dumps(info, indent=2)}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
