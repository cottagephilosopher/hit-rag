"""
RAG æ–‡æ¡£é¢„å¤„ç†ä¸»ç¨‹åº
æ•´åˆä¸‰é˜¶æ®µæµæ°´çº¿ï¼Œæä¾›å‘½ä»¤è¡Œæ¥å£
"""

import os
import sys
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config import (
    validate_config,
    get_config_summary,
    OutputConfig,
    LogConfig,
    PerformanceConfig
)
from processing_stages.stage1_baseline import Stage1Baseline
from processing_stages.stage2_clean_map import Stage2CleanMap
from processing_stages.stage3_refine_locate import Stage3RefineLocate


# é…ç½®æ—¥å¿—
def setup_logging(log_level: str = None, log_file: str = None):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = getattr(logging, log_level or LogConfig.LOG_LEVEL)
    log_format = LogConfig.LOG_FORMAT

    handlers = [logging.StreamHandler()]

    if log_file or LogConfig.LOG_FILE:
        file_handler = logging.FileHandler(
            log_file or LogConfig.LOG_FILE,
            encoding='utf-8'
        )
        handlers.append(file_handler)

    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=handlers
    )


logger = logging.getLogger(__name__)


class RAGPreprocessor:
    """RAG æ–‡æ¡£é¢„å¤„ç†å™¨ä¸»ç±»"""

    def __init__(self):
        self.stage1 = Stage1Baseline()
        self.stage2 = Stage2CleanMap()
        self.stage3 = Stage3RefineLocate()

    def process_file(self, input_file: str, output_dir: str = None) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ª Markdown æ–‡ä»¶

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        logger.info("=" * 80)
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
        logger.info("=" * 80)

        start_time = datetime.now()

        # 1. è¯»å–æ–‡ä»¶
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                markdown_text = f.read()
            logger.info(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸ: {len(markdown_text)} å­—ç¬¦")
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            raise

        # 2. é˜¶æ®µ1: åŸºçº¿å»ºç«‹ä¸ç²—åˆ‡
        logger.info("\n" + "=" * 80)
        stage1_result = self.stage1.process(markdown_text)

        # ä¿å­˜é˜¶æ®µ1ä¸­é—´ç»“æœ
        if OutputConfig.SAVE_INTERMEDIATE_RESULTS:
            self._save_intermediate_result(
                stage1_result,
                output_dir,
                OutputConfig.STAGE1_OUTPUT,
                serialize_tokens=False  # ä¸åºåˆ—åŒ– Token åˆ—è¡¨
            )

        # 3. é˜¶æ®µ2: æ™ºèƒ½æ¸…æ´—ä¸ Token æ˜ å°„
        logger.info("\n" + "=" * 80)
        if PerformanceConfig.ENABLE_ASYNC:
            import asyncio
            stage2_result = asyncio.run(self.stage2.async_process(stage1_result))
        else:
            stage2_result = self.stage2.process(stage1_result)

        # ä¿å­˜é˜¶æ®µ2ä¸­é—´ç»“æœ
        if OutputConfig.SAVE_INTERMEDIATE_RESULTS:
            self._save_intermediate_result(
                stage2_result,
                output_dir,
                OutputConfig.STAGE2_OUTPUT,
                serialize_tokens=False
            )

        # 4. é˜¶æ®µ3: ç²¾ç»†åˆ‡åˆ†ä¸æœ€ç»ˆå®šä½
        logger.info("\n" + "=" * 80)
        stage3_result = self.stage3.process(stage2_result)

        # 5. ä¿å­˜æœ€ç»ˆç»“æœ
        output_file = self._save_final_result(
            stage3_result,
            output_dir,
            input_file
        )

        # 6. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        elapsed = datetime.now() - start_time
        report = self._generate_report(
            input_file,
            output_file,
            stage1_result,
            stage2_result,
            stage3_result,
            elapsed
        )

        logger.info("\n" + "=" * 80)
        logger.info("å¤„ç†å®Œæˆ!")
        logger.info("=" * 80)
        logger.info(f"\n{report}")

        return {
            "input_file": input_file,
            "output_file": output_file,
            "final_chunks": stage3_result["final_chunks"],
            "statistics": stage3_result["statistics"],
            "processing_time": elapsed.total_seconds(),
            "report": report
        }

    def _save_intermediate_result(
        self,
        result: Dict[str, Any],
        output_dir: str,
        filename: str,
        serialize_tokens: bool = True
    ):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        output_dir = output_dir or OutputConfig.OUTPUT_DIR
        os.makedirs(output_dir, exist_ok=True)

        output_path = os.path.join(output_dir, filename)

        # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„å­—æ®µ
        serializable_result = self._make_serializable(result, serialize_tokens)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(
                serializable_result,
                f,
                indent=OutputConfig.JSON_INDENT,
                ensure_ascii=OutputConfig.JSON_ENSURE_ASCII
            )

        logger.info(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {output_path}")

    def _save_final_result(
        self,
        result: Dict[str, Any],
        output_dir: str,
        input_file: str
    ) -> str:
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        output_dir = output_dir or OutputConfig.OUTPUT_DIR
        os.makedirs(output_dir, exist_ok=True)

        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        input_name = Path(input_file).stem
        output_filename = f"{input_name}_final_chunks.json"
        output_path = os.path.join(output_dir, output_filename)

        # è¯»å–åŸå§‹æ–‡æœ¬ä»¥è®¡ç®—å­—ç¬¦ä½ç½®
        with open(input_file, 'r', encoding='utf-8') as f:
            original_text = f.read()

        # åˆå§‹åŒ–tokenizeræ¥è®¡ç®—å­—ç¬¦ä½ç½®
        from tokenizer.tokenizer_client import get_tokenizer
        tokenizer = get_tokenizer()
        base_tokens = tokenizer.encode(original_text)

        logger.info(f"ğŸ“ è®¡ç®—å­—ç¬¦ä½ç½® (åŸæ–‡å…± {len(base_tokens)} tokens, {len(original_text)} å­—ç¬¦)...")

        # ä¸ºæ¯ä¸ªchunkè®¡ç®—å­—ç¬¦ä½ç½®
        chunks_with_char_pos = []
        for i, chunk in enumerate(result["final_chunks"]):
            token_start = chunk["token_start"]
            token_end = chunk["token_end"]

            # ä»å¼€å¤´åˆ°token_startçš„æ–‡æœ¬
            if token_start > 0:
                prefix_tokens = base_tokens[:token_start]
                prefix_text = tokenizer.decode(prefix_tokens)
                char_start = len(prefix_text)
            else:
                char_start = 0

            # ä»å¼€å¤´åˆ°token_endçš„æ–‡æœ¬
            if token_end > 0 and token_end <= len(base_tokens):
                prefix_tokens = base_tokens[:token_end]
                prefix_text = tokenizer.decode(prefix_tokens)
                char_end = len(prefix_text)
            else:
                char_end = len(original_text)

            chunks_with_char_pos.append({
                "chunk_id": i + 1,
                "content": chunk["content"],
                "token_start": token_start,
                "token_end": token_end,
                "token_count": chunk["token_count"],
                "char_start": char_start,
                "char_end": char_end,
                "user_tag": chunk["user_tag"],
                "content_tags": chunk["content_tags"],
                "is_atomic": chunk["is_atomic"],
                "atomic_type": chunk["atomic_type"]
            })

        # å‡†å¤‡è¾“å‡ºæ•°æ®
        output_data = {
            "metadata": {
                "source_file": input_file,
                "processed_at": datetime.now().isoformat(),
                "total_chunks": len(result["final_chunks"]),
                "statistics": result["statistics"]
            },
            "chunks": chunks_with_char_pos
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(
                output_data,
                f,
                indent=OutputConfig.JSON_INDENT,
                ensure_ascii=OutputConfig.JSON_ENSURE_ASCII
            )

        logger.info(f"ğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜: {output_path}")
        return output_path

    def _make_serializable(
        self,
        data: Any,
        serialize_tokens: bool = True
    ) -> Any:
        """ä½¿æ•°æ®å¯åºåˆ—åŒ–"""
        if isinstance(data, dict):
            result = {}
            for key, value in data.items():
                # è·³è¿‡ Token åˆ—è¡¨ï¼ˆå¦‚æœä¸åºåˆ—åŒ–ï¼‰
                if not serialize_tokens and key in ['base_tokens', 'tokens']:
                    result[key] = f"<{len(value)} tokens>"
                else:
                    result[key] = self._make_serializable(value, serialize_tokens)
            return result
        elif isinstance(data, list):
            return [self._make_serializable(item, serialize_tokens) for item in data]
        else:
            return data

    def _generate_report(
        self,
        input_file: str,
        output_file: str,
        stage1_result: Dict[str, Any],
        stage2_result: Dict[str, Any],
        stage3_result: Dict[str, Any],
        elapsed
    ) -> str:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        s1_stats = stage1_result["statistics"]
        s2_stats = stage2_result["statistics"]
        s3_stats = stage3_result["statistics"]

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              RAG æ–‡æ¡£é¢„å¤„ç†æŠ¥å‘Š                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ è¾“å…¥æ–‡ä»¶: {Path(input_file).name:<45} â•‘
â•‘ è¾“å‡ºæ–‡ä»¶: {Path(output_file).name:<45} â•‘
â•‘ å¤„ç†æ—¶é—´: {elapsed.total_seconds():.2f} ç§’{' ' * 43}â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ é˜¶æ®µ1 - åŸºçº¿å»ºç«‹ä¸ç²—åˆ‡                                          â•‘
â•‘   åŸå§‹ Tokens: {s1_stats['total_tokens']:<46} â•‘
â•‘   åŸå§‹å­—ç¬¦æ•°: {s1_stats['total_chars']:<47} â•‘
â•‘   Mid-Chunks: {s1_stats['mid_chunk_count']:<48} â•‘
â•‘   å¹³å‡ Tokens: {s1_stats['avg_chunk_tokens']:.1f}{' ' * 45}â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ é˜¶æ®µ2 - æ™ºèƒ½æ¸…æ´—ä¸Tokenæ˜ å°„                                     â•‘
â•‘   Clean-Chunks: {s2_stats['clean_chunk_count']:<45} â•‘
â•‘   æ¸…æ´—å Tokens: {s2_stats['total_cleaned_tokens']:<44} â•‘
â•‘   å¹³å‡ Tokens: {s2_stats['avg_chunk_tokens']:.1f}{' ' * 45}â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ é˜¶æ®µ3 - ç²¾ç»†åˆ‡åˆ†ä¸æœ€ç»ˆå®šä½                                      â•‘
â•‘   æœ€ç»ˆå—æ•°é‡: {s3_stats['total_chunks']:<47} â•‘
â•‘   å¹³å‡ Tokens: {s3_stats['avg_tokens']:.1f}{' ' * 45}â•‘
â•‘   Token èŒƒå›´: {s3_stats['min_tokens']}-{s3_stats['max_tokens']}{' ' * 45}â•‘
â•‘   ATOMIC å—: {s3_stats['atomic_chunks']:<48} â•‘
â•‘   éªŒè¯é€šè¿‡ç‡: {s3_stats['validation_pass_rate']:.1%}{' ' * 45}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        return report


def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='RAG æ–‡æ¡£é¢„å¤„ç†å·¥å…· - æ™ºèƒ½æ¸…æ´—ä¸è¯­ä¹‰åˆ‡åˆ†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py input.md
  python main.py input.md -o ./output
  python main.py input.md --log-level DEBUG
        """
    )

    parser.add_argument(
        'input_file',
        help='è¾“å…¥çš„ Markdown æ–‡ä»¶è·¯å¾„'
    )

    parser.add_argument(
        '-o', '--output-dir',
        help=f'è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: {OutputConfig.OUTPUT_DIR})',
        default=None
    )

    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help=f'æ—¥å¿—çº§åˆ« (é»˜è®¤: {LogConfig.LOG_LEVEL})',
        default=None
    )

    parser.add_argument(
        '--log-file',
        help=f'æ—¥å¿—æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {LogConfig.LOG_FILE})',
        default=None
    )

    parser.add_argument(
        '--validate-config',
        action='store_true',
        help='éªŒè¯é…ç½®å¹¶é€€å‡º'
    )

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    setup_logging(args.log_level, args.log_file)

    try:
        # éªŒè¯é…ç½®
        logger.info("éªŒè¯é…ç½®...")
        validate_config()
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")

        # æ˜¾ç¤ºé…ç½®æ‘˜è¦
        config_summary = get_config_summary()
        logger.info(f"\né…ç½®æ‘˜è¦: {json.dumps(config_summary, indent=2, ensure_ascii=False)}")

        if args.validate_config:
            logger.info("âœ… é…ç½®éªŒè¯å®Œæˆï¼Œé€€å‡º")
            return 0

        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not os.path.exists(args.input_file):
            logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
            return 1

        # åˆ›å»ºå¤„ç†å™¨å¹¶æ‰§è¡Œ
        preprocessor = RAGPreprocessor()
        result = preprocessor.process_file(
            args.input_file,
            args.output_dir
        )

        logger.info(f"\nâœ… å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
        return 0

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        return 130

    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
