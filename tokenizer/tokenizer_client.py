"""
Tokenizer å®¢æˆ·ç«¯ï¼šç»Ÿä¸€å°è£… tiktoken æˆ–å…¶ä»–åˆ†è¯å™¨
æä¾› Token ç¼–ç ã€è§£ç ã€è®¡æ•°ç­‰åŠŸèƒ½
"""

import tiktoken
import logging
from typing import List, Optional

# ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from config import TokenizerConfig
except ImportError:
    from ..config import TokenizerConfig

logger = logging.getLogger(__name__)


class TokenizerClient:
    """
    Tokenizer å®¢æˆ·ç«¯
    å°è£… tiktokenï¼Œæä¾›ç»Ÿä¸€çš„ Token æ“ä½œæ¥å£
    """

    def __init__(self, encoding_name: Optional[str] = None):
        """
        åˆå§‹åŒ– Tokenizer

        Args:
            encoding_name: tiktoken ç¼–ç åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        """
        self.encoding_name = encoding_name or TokenizerConfig.ENCODING_NAME
        self._encoder = None
        self._initialize_encoder()

    def _initialize_encoder(self):
        """åˆå§‹åŒ–ç¼–ç å™¨"""
        try:
            self._encoder = tiktoken.get_encoding(self.encoding_name)
            logger.info(f"âœ… Tokenizer åˆå§‹åŒ–æˆåŠŸ: {self.encoding_name}")

            # å°è¯•è·å–ç‰ˆæœ¬ï¼ˆ0.5.2ç‰ˆæœ¬æ²¡æœ‰__version__å±æ€§ï¼‰
            try:
                version = tiktoken.__version__
                logger.info(f"ğŸ“¦ tiktoken ç‰ˆæœ¬: {version}")

                # éªŒè¯ç‰ˆæœ¬
                if version != TokenizerConfig.TIKTOKEN_VERSION:
                    logger.warning(
                        f"âš ï¸ tiktoken ç‰ˆæœ¬ä¸åŒ¹é…ï¼"
                        f"å½“å‰: {version}, "
                        f"æœŸæœ›: {TokenizerConfig.TIKTOKEN_VERSION}"
                    )
            except AttributeError:
                # 0.5.2 åŠæ›´æ—©ç‰ˆæœ¬æ²¡æœ‰ __version__ å±æ€§
                logger.info(f"ğŸ“¦ tiktoken ç‰ˆæœ¬: 0.5.2 (æˆ–æ›´æ—©)")

        except Exception as e:
            logger.error(f"âŒ Tokenizer åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def encode(self, text: str) -> List[int]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸º Token åˆ—è¡¨

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            Token ID åˆ—è¡¨
        """
        if not text:
            return []
        try:
            return self._encoder.encode(text)
        except Exception as e:
            logger.error(f"âŒ ç¼–ç å¤±è´¥: {e}")
            raise

    def decode(self, tokens: List[int]) -> str:
        """
        å°† Token åˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬

        Args:
            tokens: Token ID åˆ—è¡¨

        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        if not tokens:
            return ""
        try:
            return self._encoder.decode(tokens)
        except Exception as e:
            logger.error(f"âŒ è§£ç å¤±è´¥: {e}")
            raise

    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„ Token æ•°é‡

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            Token æ•°é‡
        """
        return len(self.encode(text))

    def encode_batch(self, texts: List[str]) -> List[List[int]]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            Token ID åˆ—è¡¨çš„åˆ—è¡¨
        """
        return [self.encode(text) for text in texts]

    def decode_batch(self, token_lists: List[List[int]]) -> List[str]:
        """
        æ‰¹é‡è§£ç  Token

        Args:
            token_lists: Token ID åˆ—è¡¨çš„åˆ—è¡¨

        Returns:
            è§£ç åçš„æ–‡æœ¬åˆ—è¡¨
        """
        return [self.decode(tokens) for tokens in token_lists]

    def get_token_at_position(self, text: str, char_position: int) -> Optional[int]:
        """
        è·å–æŒ‡å®šå­—ç¬¦ä½ç½®æ‰€åœ¨çš„ Token ç´¢å¼•

        Args:
            text: æ–‡æœ¬
            char_position: å­—ç¬¦ä½ç½®

        Returns:
            Token ç´¢å¼•ï¼Œå¦‚æœä½ç½®æ— æ•ˆåˆ™è¿”å› None
        """
        if char_position < 0 or char_position > len(text):
            return None

        # é€æ­¥ç¼–ç ï¼Œæ‰¾åˆ°å¯¹åº”ä½ç½®
        tokens = self.encode(text)
        current_pos = 0

        for token_idx, token_id in enumerate(tokens):
            token_text = self.decode([token_id])
            token_len = len(token_text)

            if current_pos <= char_position < current_pos + token_len:
                return token_idx

            current_pos += token_len

        return len(tokens) - 1 if tokens else None

    def get_char_to_token_mapping(self, text: str) -> List[int]:
        """
        æ„å»ºå­—ç¬¦åˆ° Token çš„æ˜ å°„

        Args:
            text: æ–‡æœ¬

        Returns:
            é•¿åº¦ä¸º len(text) çš„åˆ—è¡¨ï¼Œæ¯ä¸ªä½ç½®å­˜å‚¨å¯¹åº”çš„ Token ç´¢å¼•
        """
        tokens = self.encode(text)
        char_to_token = []
        current_token_idx = 0

        for token_id in tokens:
            token_text = self.decode([token_id])
            for _ in token_text:
                char_to_token.append(current_token_idx)
            current_token_idx += 1

        return char_to_token

    def get_token_to_char_mapping(self, text: str) -> List[tuple]:
        """
        æ„å»º Token åˆ°å­—ç¬¦ä½ç½®çš„æ˜ å°„

        Args:
            text: æ–‡æœ¬

        Returns:
            åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (start_char, end_char) å…ƒç»„
        """
        tokens = self.encode(text)
        token_to_char = []
        current_pos = 0

        for token_id in tokens:
            token_text = self.decode([token_id])
            token_len = len(token_text)
            token_to_char.append((current_pos, current_pos + token_len))
            current_pos += token_len

        return token_to_char

    def split_text_by_tokens(
        self,
        text: str,
        max_tokens: int,
        overlap_tokens: int = 0
    ) -> List[str]:
        """
        æŒ‰ç…§ Token æ•°é‡åˆ‡åˆ†æ–‡æœ¬

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_tokens: æ¯å—æœ€å¤§ Token æ•°
            overlap_tokens: é‡å  Token æ•°

        Returns:
            åˆ‡åˆ†åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        tokens = self.encode(text)
        chunks = []
        start = 0

        while start < len(tokens):
            end = min(start + max_tokens, len(tokens))
            chunk_tokens = tokens[start:end]
            chunk_text = self.decode(chunk_tokens)
            chunks.append(chunk_text)

            if end >= len(tokens):
                break

            start = end - overlap_tokens

        return chunks

    def find_token_sequence(
        self,
        base_tokens: List[int],
        search_tokens: List[int]
    ) -> Optional[int]:
        """
        åœ¨åŸºç¡€ Token åºåˆ—ä¸­æŸ¥æ‰¾å­åºåˆ—çš„èµ·å§‹ä½ç½®

        Args:
            base_tokens: åŸºç¡€ Token åºåˆ—
            search_tokens: è¦æŸ¥æ‰¾çš„ Token å­åºåˆ—

        Returns:
            èµ·å§‹ Token ç´¢å¼•ï¼Œæœªæ‰¾åˆ°è¿”å› None
        """
        if not search_tokens:
            return None

        search_len = len(search_tokens)
        base_len = len(base_tokens)

        for i in range(base_len - search_len + 1):
            if base_tokens[i:i + search_len] == search_tokens:
                return i

        return None

    def get_info(self) -> dict:
        """
        è·å– Tokenizer ä¿¡æ¯

        Returns:
            åŒ…å«ç¼–ç åç§°ã€ç‰ˆæœ¬ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å°è¯•è·å–ç‰ˆæœ¬
        try:
            current_version = tiktoken.__version__
        except AttributeError:
            current_version = "0.5.2 (æˆ–æ›´æ—©)"

        return {
            "encoding_name": self.encoding_name,
            "tiktoken_version": current_version,
            "expected_version": TokenizerConfig.TIKTOKEN_VERSION,
            "version_match": str(current_version) == TokenizerConfig.TIKTOKEN_VERSION
        }


# å…¨å±€å•ä¾‹
_global_tokenizer = None


def get_tokenizer() -> TokenizerClient:
    """
    è·å–å…¨å±€ Tokenizer å•ä¾‹

    Returns:
        TokenizerClient å®ä¾‹
    """
    global _global_tokenizer
    if _global_tokenizer is None:
        _global_tokenizer = TokenizerClient()
    return _global_tokenizer


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)

    tokenizer = TokenizerClient()

    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚This is a test."
    print(f"\næµ‹è¯•æ–‡æœ¬: {test_text}")

    # ç¼–ç 
    tokens = tokenizer.encode(test_text)
    print(f"Token æ•°é‡: {len(tokens)}")
    print(f"Tokens: {tokens}")

    # è§£ç 
    decoded = tokenizer.decode(tokens)
    print(f"è§£ç æ–‡æœ¬: {decoded}")
    print(f"è§£ç æ­£ç¡®: {decoded == test_text}")

    # Token è®¡æ•°
    count = tokenizer.count_tokens(test_text)
    print(f"\nToken è®¡æ•°: {count}")

    # å­—ç¬¦åˆ° Token æ˜ å°„
    char_to_token = tokenizer.get_char_to_token_mapping(test_text)
    print(f"\nå­—ç¬¦åˆ° Token æ˜ å°„ (å‰20ä¸ªå­—ç¬¦): {char_to_token[:20]}")

    # Token åˆ°å­—ç¬¦æ˜ å°„
    token_to_char = tokenizer.get_token_to_char_mapping(test_text)
    print(f"\nToken åˆ°å­—ç¬¦æ˜ å°„: {token_to_char}")

    # ä¿¡æ¯
    info = tokenizer.get_info()
    print(f"\nTokenizer ä¿¡æ¯:")
    for key, value in info.items():
        print(f"  {key}: {value}")
