"""
é˜¶æ®µ2: æ™ºèƒ½æ¸…æ´—ä¸ Token æ˜ å°„
ä½¿ç”¨ LLM æ ‡è®°æ‚è´¨ï¼Œæ¸…æ´—æ–‡æœ¬ï¼Œå¹¶æ‰§è¡Œåå‘ Token æ˜ å°„
"""

import logging
import re
import json
import os
from typing import List, Dict, Any

# ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from tokenizer.tokenizer_client import get_tokenizer
    from tokenizer.token_mapper import TokenMapper
    from llm_api.llm_client import get_llm_client
    from llm_api.prompt_templates import get_combined_clean_and_tag_prompts
    from config import PerformanceConfig
except ImportError:
    from ..tokenizer.tokenizer_client import get_tokenizer
    from ..tokenizer.token_mapper import TokenMapper
    from ..llm_api.llm_client import get_llm_client
    from ..llm_api.prompt_templates import get_combined_clean_and_tag_prompts
    from ..config import PerformanceConfig

logger = logging.getLogger(__name__)


class Stage2CleanMap:
    """
    é˜¶æ®µ2: æ™ºèƒ½æ¸…æ´—ä¸ Token æ˜ å°„
    """

    def __init__(self):
        self.tokenizer = get_tokenizer()
        self.mapper = TokenMapper(self.tokenizer)
        self.llm_client = get_llm_client()
        self._cached_tags = None  # ç¼“å­˜ç³»ç»Ÿæ ‡ç­¾

    def _fetch_system_tags(self) -> List[str]:
        """
        ä» API æœåŠ¡å™¨è·å–ç³»ç»Ÿç°æœ‰æ ‡ç­¾

        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        if self._cached_tags is not None:
            return self._cached_tags

        try:
            import requests

            # ä»ç¯å¢ƒå˜é‡è·å– API æœåŠ¡å™¨åœ°å€
            api_host = os.getenv("API_SERVER_HOST", "http://localhost:8000")
            response = requests.get(f"{api_host}/api/tags/all", timeout=5)

            if response.status_code == 200:
                tags_data = response.json()
                # æå–æ ‡ç­¾åç§°
                tags = [tag["name"] for tag in tags_data]
                self._cached_tags = tags
                logger.info(f"âœ… ä» API æœåŠ¡å™¨è·å–äº† {len(tags)} ä¸ªç³»ç»Ÿæ ‡ç­¾")
                return tags
            else:
                logger.warning(f"âš ï¸ è·å–ç³»ç»Ÿæ ‡ç­¾å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []

        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•è¿æ¥åˆ° API æœåŠ¡å™¨è·å–æ ‡ç­¾: {e}")
            return []

    def process(self, stage1_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†é˜¶æ®µ2: æ¸…æ´—å¹¶æ˜ å°„ Token

        Args:
            stage1_result: é˜¶æ®µ1çš„è¾“å‡ºç»“æœ

        Returns:
            åŒ…å« Clean-Chunks çš„å­—å…¸
        """
        logger.info("=" * 60)
        logger.info("é˜¶æ®µ2: å¼€å§‹æ™ºèƒ½æ¸…æ´—ä¸ Token æ˜ å°„")
        logger.info("=" * 60)

        base_tokens = stage1_result["base_tokens"]

        # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„æ ‘
        if "structure_tree" in stage1_result and stage1_result["structure_tree"]:
            logger.info("ğŸŒ³ æ£€æµ‹åˆ°ç»“æ„æ ‘ï¼Œè·³è¿‡LLMæ¸…æ´—ï¼Œç›´æ¥ä¼ é€’ç»™Stage3")
            logger.info(f"   ç»“æ„æ ‘åŒ…å« {len(stage1_result['structure_tree'])} ä¸ªç« èŠ‚èŠ‚ç‚¹")

            # ç›´æ¥ä¼ é€’ç»“æ„æ ‘è·¯å¾„ï¼Œä¸è¿›è¡ŒLLMæ¸…æ´—
            result = {
                "base_tokens": base_tokens,
                "structure_tree": stage1_result["structure_tree"],
                "original_text": stage1_result["original_text"],
                "clean_chunks": [],  # Empty for structure path
                "statistics": {
                    "mid_chunk_count": 0,
                    "clean_chunk_count": 0,
                    "total_cleaned_tokens": 0,
                    "avg_chunk_tokens": 0,
                    "structure_nodes": len(stage1_result["structure_tree"])
                }
            }

            logger.info("âœ… ç»“æ„æ ‘ä¼ é€’å®Œæˆï¼Œè·³è¿‡ä¼ ç»Ÿæ¸…æ´—æµç¨‹")
            return result

        # åŸæœ‰çš„LLMæ¸…æ´—è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
        logger.info("ğŸ“ ä½¿ç”¨ä¼ ç»ŸLLMæ¸…æ´—è·¯å¾„")
        mid_chunks = stage1_result["mid_chunks"]

        # å¤„ç†æ¯ä¸ª Mid-Chunk
        clean_chunks = []
        for i, mid_chunk in enumerate(mid_chunks, 1):
            logger.info(f"\nå¤„ç† Mid-Chunk {i}/{len(mid_chunks)}...")

            try:
                clean_chunk = self._process_mid_chunk(
                    mid_chunk,
                    base_tokens
                )
                clean_chunks.append(clean_chunk)
                logger.info(
                    f"âœ… Mid-Chunk {i} å¤„ç†å®Œæˆ "
                    f"(Token: {clean_chunk['token_start']}-{clean_chunk['token_end']})"
                )

            except Exception as e:
                logger.error(f"âŒ Mid-Chunk {i} å¤„ç†å¤±è´¥: {e}")
                # åˆ›å»ºä¸€ä¸ªä¿ç•™åŸæ–‡çš„ fallback chunk
                fallback_chunk = self._create_fallback_chunk(mid_chunk, base_tokens)
                clean_chunks.append(fallback_chunk)

        # æ„å»ºç»“æœï¼ˆä¼ é€’ç»“æ„æ ‘å’ŒåŸå§‹æ–‡æœ¬ç»™Stage3ï¼‰
        result = {
            "base_tokens": base_tokens,
            "clean_chunks": clean_chunks,
            "structure_tree": stage1_result.get("structure_tree"),  # æ–°å¢ï¼šä¼ é€’ç»“æ„æ ‘
            "original_text": stage1_result.get("original_text"),    # æ–°å¢ï¼šä¼ é€’åŸå§‹æ–‡æœ¬
            "statistics": {
                "mid_chunk_count": len(mid_chunks),
                "clean_chunk_count": len(clean_chunks),
                "total_cleaned_tokens": sum(c["token_count"] for c in clean_chunks),
                "avg_chunk_tokens": sum(c["token_count"] for c in clean_chunks) / len(clean_chunks) if clean_chunks else 0
            }
        }

        logger.info(f"\né˜¶æ®µ2ç»Ÿè®¡:")
        logger.info(f"  å¤„ç†çš„ Mid-Chunks: {result['statistics']['mid_chunk_count']}")
        logger.info(f"  ç”Ÿæˆçš„ Clean-Chunks: {result['statistics']['clean_chunk_count']}")
        logger.info(f"  æ¸…æ´—åæ€» Tokens: {result['statistics']['total_cleaned_tokens']}")
        logger.info(f"  å¹³å‡ Chunk Tokens: {result['statistics']['avg_chunk_tokens']:.1f}")

        return result

    def _process_mid_chunk(
        self,
        mid_chunk: Dict[str, Any],
        base_tokens: List[int]
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ª Mid-Chunk

        Args:
            mid_chunk: Mid-Chunk æ•°æ®
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Clean-Chunk å­—å…¸
        """
        chunk_text = mid_chunk["text"]
        mid_token_start = mid_chunk["token_start"]

        # 1. è°ƒç”¨ LLM è¿›è¡Œæ¸…æ´—å’Œæ ‡ç­¾æå–
        logger.debug("ğŸ”„ è°ƒç”¨ LLM è¿›è¡Œæ¸…æ´—å’Œæ ‡ç­¾æå–...")
        llm_result = self._call_llm_for_clean_and_tag(chunk_text)

        marked_text = llm_result.get("marked_text", chunk_text)
        user_tag = llm_result.get("user_tag", "æœªåˆ†ç±»")
        content_tags = llm_result.get("content_tags", [])

        # 2. ç§»é™¤ JUNK æ ‡ç­¾ï¼Œå¾—åˆ°æ¸…æ´—åçš„æ–‡æœ¬
        cleaned_text = self._remove_junk_tags(marked_text)
        logger.debug(f"æ¸…æ´—å‰: {len(chunk_text)} å­—ç¬¦, æ¸…æ´—å: {len(cleaned_text)} å­—ç¬¦")

        # 3. æ‰§è¡Œåå‘ Token æ˜ å°„
        logger.debug("ğŸ”„ æ‰§è¡Œåå‘ Token æ˜ å°„...")
        clean_token_start, clean_token_end, clean_tokens = self.mapper.reverse_map_cleaned_text(
            original_text=chunk_text,
            cleaned_text=cleaned_text,
            base_token_start=mid_token_start,
            base_tokens=base_tokens
        )

        # 4. éªŒè¯ Token èŒƒå›´
        is_valid = self.mapper.validate_token_range(
            clean_token_start,
            clean_token_end,
            base_tokens,
            cleaned_text
        )

        return {
            "text": cleaned_text,
            "original_text": chunk_text,
            "marked_text": marked_text,
            "token_start": clean_token_start,
            "token_end": clean_token_end,
            "token_count": len(clean_tokens),
            "tokens": clean_tokens,
            "user_tag": user_tag,
            "content_tags": content_tags,
            "validation_passed": is_valid,
            "char_count": len(cleaned_text)
        }

    def _call_llm_for_clean_and_tag(self, chunk_text: str) -> Dict[str, Any]:
        """
        è°ƒç”¨ LLM è¿›è¡Œæ¸…æ´—å’Œæ ‡ç­¾æå–

        Args:
            chunk_text: æ–‡æ¡£ç‰‡æ®µ

        Returns:
            åŒ…å« marked_text, user_tag, content_tags çš„å­—å…¸
        """
        try:
            # è·å–ç³»ç»Ÿç°æœ‰æ ‡ç­¾
            existing_tags = self._fetch_system_tags()

            # ä½¿ç”¨ç°æœ‰æ ‡ç­¾ç”Ÿæˆ prompt
            system_prompt, user_prompt = get_combined_clean_and_tag_prompts(
                chunk_text,
                existing_tags=existing_tags
            )

            # è°ƒç”¨ LLM (JSON æ¨¡å¼)
            response = self.llm_client.chat_json_with_system(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )

            # éªŒè¯å“åº”
            if not isinstance(response, dict):
                raise ValueError("LLM å“åº”ä¸æ˜¯æœ‰æ•ˆçš„å­—å…¸")

            marked_text = response.get("marked_text", chunk_text)
            user_tag = response.get("user_tag", "æœªåˆ†ç±»")
            content_tags = response.get("content_tags", [])

            # éªŒè¯ content_tags æ˜¯åˆ—è¡¨
            if not isinstance(content_tags, list):
                content_tags = []

            return {
                "marked_text": marked_text,
                "user_tag": user_tag,
                "content_tags": content_tags
            }

        except Exception as e:
            logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return {
                "marked_text": chunk_text,
                "user_tag": "æœªåˆ†ç±»",
                "content_tags": []
            }

    def _remove_junk_tags(self, marked_text: str) -> str:
        """
        ç§»é™¤ JUNK æ ‡ç­¾åŠå…¶å†…å®¹

        Args:
            marked_text: åŒ…å« JUNK æ ‡ç­¾çš„æ–‡æœ¬

        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        # ç§»é™¤ <JUNK type="...">...</JUNK>
        pattern = r'<JUNK\s+type="[^"]*">.*?</JUNK>'
        cleaned = re.sub(pattern, '', marked_text, flags=re.DOTALL)

        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™æœ€å¤šä¸€ä¸ªç©ºè¡Œï¼‰
        cleaned = re.sub(r'\n\n\n+', '\n\n', cleaned)

        # æ¸…ç†é¦–å°¾ç©ºç™½
        cleaned = cleaned.strip()

        # åˆå¹¶æ ‡é¢˜ä¸è¡¨æ ¼ï¼ˆç¡®ä¿è¡¨æ ¼å‰çš„æ ‡é¢˜ç´§é‚»è¡¨æ ¼ï¼‰
        cleaned = self._merge_title_with_table(cleaned)

        return cleaned

    def _merge_title_with_table(self, text: str) -> str:
        """
        åˆå¹¶æ ‡é¢˜ä¸è¡¨æ ¼ï¼Œåˆ é™¤å®ƒä»¬ä¹‹é—´çš„ç©ºè¡Œ

        æ¨¡å¼: "æ ‡é¢˜\n\n<table>" -> "æ ‡é¢˜\n<table>"

        Args:
            text: æ¸…æ´—åçš„æ–‡æœ¬

        Returns:
            åˆå¹¶åçš„æ–‡æœ¬
        """
        # åŒ¹é…ï¼šæ ‡é¢˜è¡Œ + ä¸€ä¸ªæˆ–å¤šä¸ªç©ºè¡Œ + è¡¨æ ¼å¼€å§‹
        # æ ‡é¢˜æ¨¡å¼ï¼š
        # 1. Markdown æ ‡é¢˜ï¼ˆ# å¼€å¤´ï¼‰
        # 2. ç¼–å·æ ‡é¢˜ï¼ˆå¦‚ 1.2äº§å“è§„æ ¼ï¼‰
        # 3. ç®€å•æ ‡é¢˜ï¼ˆå•è¡Œï¼Œåé¢ç´§è·Ÿç©ºè¡Œå’Œè¡¨æ ¼ï¼‰

        patterns = [
            # æ¨¡å¼1: Markdown æ ‡é¢˜ + ç©ºè¡Œ + è¡¨æ ¼
            (r'(^#{1,6}\s+.+?)\n\n+(<table>)', r'\1\n\2'),
            # æ¨¡å¼2: ç¼–å·æ ‡é¢˜ï¼ˆ1.2 è¿™ç§ï¼‰ + ç©ºè¡Œ + è¡¨æ ¼
            (r'(^\d+\.[\d\.]*\s*.+?)\n\n+(<table>)', r'\1\n\2'),
            # æ¨¡å¼3: ä»»æ„å•è¡Œæ–‡å­— + è‡³å°‘2ä¸ªç©ºè¡Œ + è¡¨æ ¼ï¼ˆä¿å®ˆåŒ¹é…ï¼Œé¿å…è¯¯åˆå¹¶æ®µè½ï¼‰
            (r'(^[^\n]{1,30})\n\n\n+(<table>)', r'\1\n\2'),
        ]

        result = text
        for pattern, replacement in patterns:
            result = re.sub(pattern, replacement, result, flags=re.MULTILINE)

        logger.debug("âœ… å·²åˆå¹¶æ ‡é¢˜ä¸è¡¨æ ¼ä¹‹é—´çš„ç©ºè¡Œ")
        return result

    def _create_fallback_chunk(
        self,
        mid_chunk: Dict[str, Any],
        base_tokens: List[int]
    ) -> Dict[str, Any]:
        """
        åˆ›å»º fallback Clean-Chunkï¼ˆä¿ç•™åŸæ–‡ï¼‰

        Args:
            mid_chunk: Mid-Chunk æ•°æ®
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Clean-Chunk å­—å…¸
        """
        chunk_text = mid_chunk["text"]
        tokens = self.tokenizer.encode(chunk_text)

        return {
            "text": chunk_text,
            "original_text": chunk_text,
            "marked_text": chunk_text,
            "token_start": mid_chunk["token_start"],
            "token_end": mid_chunk["token_end"],
            "token_count": len(tokens),
            "tokens": tokens,
            "user_tag": "æœªåˆ†ç±»",
            "content_tags": [],
            "validation_passed": True,
            "char_count": len(chunk_text),
            "is_fallback": True
        }

    async def async_process(self, stage1_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¼‚æ­¥å¤„ç†é˜¶æ®µ2ï¼ˆå¹¶å‘å¤„ç†å¤šä¸ª Mid-Chunksï¼‰

        Args:
            stage1_result: é˜¶æ®µ1çš„è¾“å‡ºç»“æœ

        Returns:
            åŒ…å« Clean-Chunks çš„å­—å…¸
        """
        if not PerformanceConfig.ENABLE_ASYNC:
            return self.process(stage1_result)

        logger.info("=" * 60)
        logger.info("é˜¶æ®µ2: å¼€å§‹æ™ºèƒ½æ¸…æ´—ä¸ Token æ˜ å°„ï¼ˆå¼‚æ­¥æ¨¡å¼ï¼‰")
        logger.info("=" * 60)

        base_tokens = stage1_result["base_tokens"]

        # æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„æ ‘
        if "structure_tree" in stage1_result and stage1_result["structure_tree"]:
            logger.info("ğŸŒ³ æ£€æµ‹åˆ°ç»“æ„æ ‘ï¼Œè·³è¿‡LLMæ¸…æ´—ï¼Œç›´æ¥ä¼ é€’ç»™Stage3")
            logger.info(f"   ç»“æ„æ ‘åŒ…å« {len(stage1_result['structure_tree'])} ä¸ªç« èŠ‚èŠ‚ç‚¹")

            # ç›´æ¥ä¼ é€’ç»“æ„æ ‘è·¯å¾„ï¼Œä¸è¿›è¡ŒLLMæ¸…æ´—
            result = {
                "base_tokens": base_tokens,
                "structure_tree": stage1_result["structure_tree"],
                "original_text": stage1_result["original_text"],
                "clean_chunks": [],  # Empty for structure path
                "statistics": {
                    "mid_chunk_count": 0,
                    "clean_chunk_count": 0,
                    "total_cleaned_tokens": 0,
                    "avg_chunk_tokens": 0,
                    "structure_nodes": len(stage1_result["structure_tree"])
                }
            }

            logger.info("âœ… ç»“æ„æ ‘ä¼ é€’å®Œæˆï¼Œè·³è¿‡ä¼ ç»Ÿæ¸…æ´—æµç¨‹")
            return result

        # åŸæœ‰çš„LLMæ¸…æ´—è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
        logger.info("ğŸ“ ä½¿ç”¨ä¼ ç»ŸLLMæ¸…æ´—è·¯å¾„ï¼ˆå¼‚æ­¥ï¼‰")
        mid_chunks = stage1_result["mid_chunks"]

        # æ‰¹é‡å¼‚æ­¥å¤„ç†
        import asyncio
        from asyncio import Semaphore

        semaphore = Semaphore(PerformanceConfig.MAX_CONCURRENT_REQUESTS)

        async def process_one(mid_chunk, index):
            async with semaphore:
                logger.info(f"å¤„ç† Mid-Chunk {index}/{len(mid_chunks)}...")
                try:
                    # åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡ŒåŒæ­¥ä»£ç 
                    loop = asyncio.get_event_loop()
                    result = await loop.run_in_executor(
                        None,
                        self._process_mid_chunk,
                        mid_chunk,
                        base_tokens
                    )
                    logger.info(f"âœ… Mid-Chunk {index} å¤„ç†å®Œæˆ")
                    return result
                except Exception as e:
                    logger.error(f"âŒ Mid-Chunk {index} å¤„ç†å¤±è´¥: {e}")
                    return self._create_fallback_chunk(mid_chunk, base_tokens)

        tasks = [process_one(chunk, i + 1) for i, chunk in enumerate(mid_chunks)]
        clean_chunks = await asyncio.gather(*tasks)

        # æ„å»ºç»“æœ
        result = {
            "base_tokens": base_tokens,
            "clean_chunks": clean_chunks,
            "statistics": {
                "mid_chunk_count": len(mid_chunks),
                "clean_chunk_count": len(clean_chunks),
                "total_cleaned_tokens": sum(c["token_count"] for c in clean_chunks),
                "avg_chunk_tokens": sum(c["token_count"] for c in clean_chunks) / len(clean_chunks) if clean_chunks else 0
            }
        }

        logger.info(f"\né˜¶æ®µ2ç»Ÿè®¡:")
        logger.info(f"  å¤„ç†çš„ Mid-Chunks: {result['statistics']['mid_chunk_count']}")
        logger.info(f"  ç”Ÿæˆçš„ Clean-Chunks: {result['statistics']['clean_chunk_count']}")

        return result


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # æ¨¡æ‹Ÿé˜¶æ®µ1çš„è¾“å‡º
    try:
        from tokenizer.tokenizer_client import get_tokenizer
    except ImportError:
        from ..tokenizer.tokenizer_client import get_tokenizer

    tokenizer = get_tokenizer()

    test_text = """é¡µçœ‰ï¼šå…¬å¸æœºå¯† | 2024å¹´1æœˆ

# äº§å“ä»‹ç»
è¿™æ˜¯ä¸€ä¸ª RAG ç³»ç»Ÿçš„æ–‡æ¡£é¢„å¤„ç†å·¥å…·ã€‚

âš ï¸ è­¦å‘Šï¼šæ­¤æ–‡æ¡£ä»…ä¾›å†…éƒ¨ä½¿ç”¨

æœ¬å·¥å…·æ”¯æŒæ™ºèƒ½æ¸…æ´—å’Œè¯­ä¹‰åˆ‡åˆ†ã€‚
"""

    base_tokens = tokenizer.encode(test_text)

    stage1_result = {
        "base_tokens": base_tokens,
        "original_text": test_text,
        "mid_chunks": [
            {
                "text": test_text,
                "token_start": 0,
                "token_end": len(base_tokens),
                "token_count": len(base_tokens),
                "char_count": len(test_text)
            }
        ]
    }

    # å¤„ç†é˜¶æ®µ2
    processor = Stage2CleanMap()
    result = processor.process(stage1_result)

    print(f"\n=== é˜¶æ®µ2å¤„ç†ç»“æœ ===")
    print(f"Clean-Chunks: {result['statistics']['clean_chunk_count']}")
    for i, chunk in enumerate(result["clean_chunks"], 1):
        print(f"\nChunk {i}:")
        print(f"  Token èŒƒå›´: [{chunk['token_start']}:{chunk['token_end']}]")
        print(f"  ç”¨æˆ·æ ‡ç­¾: {chunk['user_tag']}")
        print(f"  å†…å®¹æ ‡ç­¾: {chunk['content_tags']}")
        print(f"  æ¸…æ´—åå†…å®¹:\n{chunk['text']}")
