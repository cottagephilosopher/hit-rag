"""
é˜¶æ®µ3: ç²¾ç»†åˆ‡åˆ†ä¸æœ€ç»ˆå®šä½
ä½¿ç”¨ LLM å¯¹æ¸…æ´—åçš„æ–‡æœ¬è¿›è¡Œç²¾ç»†åˆ‡åˆ†ï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„ Token ç»å¯¹ç´¢å¼•
"""

import logging
import re
import json
from typing import List, Dict, Any

# ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from tokenizer.tokenizer_client import get_tokenizer
    from tokenizer.token_mapper import TokenMapper
    from llm_api.llm_client import get_llm_client
    from llm_api.prompt_templates import get_chunking_prompts
    from config import ChunkConfig, ValidationConfig
except ImportError:
    from ..tokenizer.tokenizer_client import get_tokenizer
    from ..tokenizer.token_mapper import TokenMapper
    from ..llm_api.llm_client import get_llm_client
    from ..llm_api.prompt_templates import get_chunking_prompts
    from ..config import ChunkConfig, ValidationConfig

logger = logging.getLogger(__name__)


class Stage3RefineLocate:
    """
    é˜¶æ®µ3: ç²¾ç»†åˆ‡åˆ†ä¸æœ€ç»ˆå®šä½
    """

    def __init__(self):
        self.tokenizer = get_tokenizer()
        self.mapper = TokenMapper(self.tokenizer)
        self.llm_client = get_llm_client()

    def process(self, stage2_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†é˜¶æ®µ3: ç²¾ç»†åˆ‡åˆ†å¹¶å®šä½

        Args:
            stage2_result: é˜¶æ®µ2çš„è¾“å‡ºç»“æœ

        Returns:
            åŒ…å«æœ€ç»ˆ RAG Chunks çš„å­—å…¸
        """
        logger.info("=" * 60)
        logger.info("é˜¶æ®µ3: å¼€å§‹ç²¾ç»†åˆ‡åˆ†ä¸æœ€ç»ˆå®šä½")
        logger.info("=" * 60)

        base_tokens = stage2_result["base_tokens"]

        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„æ ‘ï¼ˆæ–°ç­–ç•¥ï¼‰
        if "structure_tree" in stage2_result and stage2_result["structure_tree"]:
            logger.info("ğŸŒ³ ä½¿ç”¨ç»“æ„æ ‘è¿›è¡Œç»“æ„åŒ–åˆ‡åˆ†")
            structure_tree = stage2_result["structure_tree"]
            original_text = stage2_result["original_text"]
            final_chunks = self._structure_based_chunking(
                structure_tree,
                original_text,
                base_tokens
            )
        else:
            logger.info("ğŸ“ ä½¿ç”¨ä¼ ç»ŸClean-Chunkåˆ‡åˆ†ï¼ˆå‘åå…¼å®¹ï¼‰")
            clean_chunks = stage2_result["clean_chunks"]

            # å¤„ç†æ¯ä¸ª Clean-Chunk
            final_chunks = []
            for i, clean_chunk in enumerate(clean_chunks, 1):
                logger.info(f"\nå¤„ç† Clean-Chunk {i}/{len(clean_chunks)}...")

                try:
                    chunks = self._process_clean_chunk(
                        clean_chunk,
                        base_tokens
                    )
                    final_chunks.extend(chunks)
                    logger.info(
                        f"âœ… Clean-Chunk {i} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæœ€ç»ˆå—"
                    )

                except Exception as e:
                    logger.error(f"âŒ Clean-Chunk {i} å¤„ç†å¤±è´¥: {e}")
                    # åˆ›å»º fallback chunk
                    fallback = self._create_fallback_final_chunk(clean_chunk)
                    final_chunks.append(fallback)

        # Token æº¢å‡ºæ ¡éªŒï¼ˆç¬¬ä¸€æ¬¡ï¼šåˆå¹¶å‰ï¼‰
        overflow_info = []
        if ValidationConfig.CHECK_TOKEN_OVERFLOW:
            overflow_info = self._validate_token_overflow(final_chunks)

        # è‡ªåŠ¨ä¿®å¤è¶…é™å—
        if overflow_info:
            final_chunks = self._auto_fix_overflow_chunks(final_chunks, overflow_info)

        # åˆå¹¶è¿‡å°çš„ chunks
        final_chunks = self._merge_small_chunks(final_chunks, base_tokens)

        # Token æº¢å‡ºæ ¡éªŒï¼ˆç¬¬äºŒæ¬¡ï¼šåˆå¹¶åï¼‰
        # åˆå¹¶æ“ä½œå¯èƒ½ä¼šäº§ç”Ÿæ–°çš„è¶…é™å—ï¼Œéœ€è¦å†æ¬¡éªŒè¯å’Œä¿®å¤
        if ValidationConfig.CHECK_TOKEN_OVERFLOW:
            overflow_info_after_merge = self._validate_token_overflow(final_chunks)
            if overflow_info_after_merge:
                logger.info("æ£€æµ‹åˆ°åˆå¹¶åäº§ç”Ÿçš„è¶…é™å—ï¼Œå¼€å§‹ä¿®å¤...")
                final_chunks = self._auto_fix_overflow_chunks(final_chunks, overflow_info_after_merge)

        # æ£€æµ‹ token gap
        self._validate_token_continuity(final_chunks)

        # æ„å»ºç»“æœ
        result = {
            "final_chunks": final_chunks,
            "statistics": self._calculate_statistics(final_chunks)
        }

        logger.info(f"\né˜¶æ®µ3ç»Ÿè®¡:")
        stats = result["statistics"]
        logger.info(f"  æœ€ç»ˆå—æ•°é‡: {stats['total_chunks']}")
        logger.info(f"  å¹³å‡ Token æ•°: {stats['avg_tokens']:.1f}")
        logger.info(f"  Token èŒƒå›´: {stats['min_tokens']}-{stats['max_tokens']}")
        logger.info(f"  ATOMIC å—æ•°é‡: {stats['atomic_chunks']}")
        logger.info(f"  éªŒè¯é€šè¿‡ç‡: {stats['validation_pass_rate']:.1%}")

        return result

    def _process_clean_chunk(
        self,
        clean_chunk: Dict[str, Any],
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ª Clean-Chunk

        Args:
            clean_chunk: Clean-Chunk æ•°æ®
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Final-Chunk åˆ—è¡¨
        """
        chunk_text = clean_chunk["text"]
        clean_token_start = clean_chunk["token_start"]
        clean_tokens = clean_chunk["tokens"]
        user_tag = clean_chunk["user_tag"]
        content_tags = clean_chunk["content_tags"]

        # ä¼°ç®— Token æ•°é‡ï¼ˆä½¿ç”¨ token èŒƒå›´ï¼Œè€Œä¸æ˜¯ tokens æ•°ç»„é•¿åº¦ï¼‰
        estimated_tokens = clean_chunk["token_end"] - clean_chunk["token_start"]

        # æ£€æµ‹æ˜¯å¦åŒ…å«ç‰¹æ®Šç»“æ„ï¼ˆè¡¨æ ¼ã€ä»£ç å—ç­‰ï¼‰
        has_special_structure = self._contains_special_structure(chunk_text)

        # æ£€æµ‹æ˜¯å¦åŒ…å«æ­¥éª¤åºåˆ—
        has_step_sequence = self._contains_step_sequence(chunk_text)

        # æ–°ç­–ç•¥ï¼šè¯­ä¹‰å®Œæ•´æ€§ä¼˜å…ˆ
        # 1. å¦‚æœåœ¨ MIN ~ TARGET èŒƒå›´å†…ï¼Œä¸”æ— ç‰¹æ®Šç»“æ„ï¼Œç›´æ¥è¿”å›ï¼ˆç†æƒ³å¤§å°ï¼‰
        if (ChunkConfig.FINAL_MIN_TOKENS <= estimated_tokens <= ChunkConfig.FINAL_TARGET_TOKENS
            and not has_special_structure
            and not has_step_sequence):
            logger.debug(f"Clean-Chunk åœ¨ç†æƒ³èŒƒå›´å†… ({estimated_tokens} tokens)ï¼Œæ— éœ€åˆ‡åˆ†")
            return [self._create_final_chunk(
                content=chunk_text,
                token_start=clean_token_start,
                token_end=clean_chunk["token_end"],
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=False,
                atomic_type=None,
                base_tokens=base_tokens,
                context_tags=[]
            )]

        # 2. å¦‚æœåœ¨ TARGET ~ MAX èŒƒå›´å†…ï¼Œä¸”æ— ç‰¹æ®Šç»“æ„ï¼Œä¹Ÿç›´æ¥è¿”å›ï¼ˆå¯æ¥å—å¤§å°ï¼‰
        if (ChunkConfig.FINAL_TARGET_TOKENS < estimated_tokens <= ChunkConfig.FINAL_MAX_TOKENS
            and not has_special_structure
            and not has_step_sequence):
            logger.debug(f"Clean-Chunk ç•¥å¤§ä½†å¯æ¥å— ({estimated_tokens} tokens)ï¼Œç›´æ¥ä¿ç•™")
            return [self._create_final_chunk(
                content=chunk_text,
                token_start=clean_token_start,
                token_end=clean_chunk["token_end"],
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=False,
                atomic_type=None,
                base_tokens=base_tokens,
                context_tags=[]
            )]

        # 3. å¦‚æœåœ¨ MAX ~ HARD_LIMIT èŒƒå›´å†…ï¼Œæ ‡è®°ä¸º ATOMIC-CONTENTï¼ˆä¼˜å…ˆä¿è¯è¯­ä¹‰å®Œæ•´æ€§ï¼‰
        if (ChunkConfig.FINAL_MAX_TOKENS < estimated_tokens <= ChunkConfig.FINAL_HARD_LIMIT
            and not has_special_structure
            and not has_step_sequence):
            logger.warning(
                f"âš ï¸ Clean-Chunk è¶…è¿‡å»ºè®®æœ€å¤§å€¼ä½†æœªè¾¾ç¡¬æ€§ä¸Šé™ ({estimated_tokens} tokens)ï¼Œ"
                f"æ ‡è®°ä¸º ATOMIC-CONTENT ä»¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€§"
            )
            return [self._create_final_chunk(
                content=chunk_text,
                token_start=clean_token_start,
                token_end=clean_chunk["token_end"],
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=True,
                atomic_type="content",
                base_tokens=base_tokens,
                context_tags=["è¯­ä¹‰å®Œæ•´", "è¶…å¤§æ®µè½"]
            )]

        # 4. å¦‚æœè¶…è¿‡ç¡¬æ€§ä¸Šé™ï¼Œå¿…é¡»è°ƒç”¨ LLM åˆ‡åˆ†
        if estimated_tokens > ChunkConfig.FINAL_HARD_LIMIT:
            logger.warning(
                f"âš ï¸ Clean-Chunk è¶…è¿‡ç¡¬æ€§ä¸Šé™ ({estimated_tokens} > {ChunkConfig.FINAL_HARD_LIMIT})ï¼Œ"
                f"å¿…é¡»è¿›è¡Œåˆ‡åˆ†"
            )

        # 5. ã€ä¼˜å…ˆã€‘æ ‡é¢˜ä¼˜å…ˆåˆ‡åˆ†ç­–ç•¥ - åœ¨ATOMICæ£€æµ‹ä¹‹å‰æ‰§è¡Œ
        # å¦‚æœ chunk è¶…è¿‡ TARGET_TOKENS ä¸”åŒ…å«ç« èŠ‚æ ‡é¢˜ï¼Œä¼˜å…ˆåœ¨æ ‡é¢˜å¤„åˆ‡åˆ†
        if estimated_tokens > ChunkConfig.FINAL_TARGET_TOKENS:
            headers = self._detect_headers_in_chunk(chunk_text)
            if headers:
                # å°è¯•åœ¨æ ‡é¢˜ä½ç½®åˆ‡åˆ†
                header_based_chunks = self._split_at_headers(
                    chunk_text,
                    clean_token_start,
                    clean_tokens,
                    headers,
                    user_tag,
                    content_tags,
                    base_tokens
                )
                if header_based_chunks:
                    logger.debug(f"âœ… åŸºäºæ ‡é¢˜åˆ‡åˆ†æˆåŠŸï¼Œç”Ÿæˆ {len(header_based_chunks)} ä¸ªå­å—ï¼Œè·³è¿‡ATOMICæ£€æµ‹")
                    return header_based_chunks

        # 6. å¦‚æœæ£€æµ‹åˆ°æ­¥éª¤åºåˆ—ï¼Œä¸”åœ¨åˆç†èŒƒå›´å†…ï¼Œç›´æ¥æ ‡è®°ä¸º ATOMIC-STEP
        if has_step_sequence and estimated_tokens <= ChunkConfig.FINAL_HARD_LIMIT:
            logger.debug(
                f"æ£€æµ‹åˆ°æ­¥éª¤åºåˆ— ({estimated_tokens} tokens)ï¼Œæ ‡è®°ä¸º ATOMIC-STEP ä¿æŒå®Œæ•´æ€§"
            )
            return [self._create_final_chunk(
                content=chunk_text,
                token_start=clean_token_start,
                token_end=clean_chunk["token_end"],
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=True,
                atomic_type="step",
                base_tokens=base_tokens,
                context_tags=self._extract_step_sequence_context_tags(chunk_text)
            )]

        # 7. å¦‚æœåŒ…å«ç‰¹æ®Šç»“æ„ï¼ˆè¡¨æ ¼/ä»£ç å—ï¼‰ï¼Œéœ€è¦ LLM åˆ‡åˆ†ä»¥è¯†åˆ« ATOMIC å—
        if has_special_structure:
            logger.debug(f"æ£€æµ‹åˆ°ç‰¹æ®Šç»“æ„ï¼ˆè¡¨æ ¼/ä»£ç å—ï¼‰ï¼Œéœ€è¦ LLM åˆ‡åˆ†ä»¥è¯†åˆ« ATOMIC å—")

        # è°ƒç”¨ LLM è¿›è¡Œç²¾ç»†åˆ‡åˆ†
        logger.debug(f"ğŸ”„ è°ƒç”¨ LLM è¿›è¡Œç²¾ç»†åˆ‡åˆ† ({estimated_tokens} tokens)...")
        llm_chunks = self._call_llm_for_chunking(chunk_text, estimated_tokens)

        # å¤„ç†åˆ‡åˆ†ç»“æœå¹¶å®šä½
        final_chunks = []
        for llm_chunk in llm_chunks:
            try:
                # æå– ATOMIC æ ‡ç­¾å’Œä¸Šä¸‹æ–‡æ ‡ç­¾
                content, is_atomic, atomic_type, context_tags = self._extract_atomic_tag(
                    llm_chunk["content"]
                )

                # å®šä½ Token ç´¢å¼•
                token_start, token_end = self.mapper.locate_final_chunk(
                    clean_chunk_text=chunk_text,
                    clean_chunk_tokens=clean_tokens,
                    clean_chunk_token_start=clean_token_start,
                    final_chunk_text=content
                )

                # åˆ›å»ºæœ€ç»ˆå—
                final_chunk = self._create_final_chunk(
                    content=content,
                    token_start=token_start,
                    token_end=token_end,
                    user_tag=user_tag,
                    content_tags=content_tags,
                    is_atomic=is_atomic,
                    atomic_type=atomic_type,
                    context_tags=context_tags,
                    base_tokens=base_tokens
                )

                final_chunks.append(final_chunk)

            except Exception as e:
                logger.error(f"âŒ å¤„ç†åˆ‡åˆ†å—å¤±è´¥: {e}")
                continue

        # å¦‚æœåˆ‡åˆ†å¤±è´¥ï¼Œè¿”å›åŸå§‹å—
        if not final_chunks:
            logger.warning("âš ï¸ LLM åˆ‡åˆ†å¤±è´¥ï¼Œè¿”å›åŸå§‹ Clean-Chunk")
            final_chunks = [self._create_final_chunk(
                content=chunk_text,
                token_start=clean_token_start,
                token_end=clean_chunk["token_end"],
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=False,
                atomic_type=None,
                base_tokens=base_tokens,
                context_tags=[]
            )]

        return final_chunks

    def _call_llm_for_chunking(
        self,
        chunk_text: str,
        estimated_tokens: int
    ) -> List[Dict[str, Any]]:
        """
        è°ƒç”¨ LLM è¿›è¡Œç²¾ç»†åˆ‡åˆ†

        Args:
            chunk_text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
            estimated_tokens: ä¼°ç®—çš„ Token æ•°

        Returns:
            åˆ‡åˆ†ç»“æœåˆ—è¡¨
        """
        try:
            system_prompt, user_prompt = get_chunking_prompts(
                chunk_text,
                estimated_tokens
            )

            # è°ƒç”¨ LLM (JSON æ¨¡å¼)
            response = self.llm_client.chat_json_with_system(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )

            # æå– chunks
            chunks = response.get("chunks", [])

            if not chunks:
                raise ValueError("LLM æœªè¿”å›ä»»ä½•åˆ‡åˆ†ç»“æœ")

            logger.debug(f"âœ… LLM è¿”å› {len(chunks)} ä¸ªåˆ‡åˆ†å—")
            return chunks

        except Exception as e:
            logger.error(f"âŒ LLM åˆ‡åˆ†è°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºå•ä¸ªå—
            return [{
                "content": chunk_text,
                "is_atomic": False,
                "atomic_type": None
            }]

    def _contains_special_structure(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ç‰¹æ®Šç»“æ„ï¼ˆè¡¨æ ¼ã€ä»£ç å—ã€å…¬å¼ç­‰ï¼‰

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            æ˜¯å¦åŒ…å«ç‰¹æ®Šç»“æ„
        """
        import re

        # æ£€æµ‹è¡¨æ ¼ï¼ˆHTML table æ ‡ç­¾ï¼‰
        if re.search(r'<table[^>]*>.*?</table>', text, re.DOTALL):
            logger.debug("æ£€æµ‹åˆ° HTML è¡¨æ ¼")
            return True

        # æ£€æµ‹ Markdown è¡¨æ ¼ï¼ˆè‡³å°‘2è¡Œï¼ŒåŒ…å« | å’Œ -ï¼‰
        lines = text.split('\n')
        table_like_lines = [l for l in lines if '|' in l]
        if len(table_like_lines) >= 2:
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†éš”è¡Œï¼ˆå¦‚ |-----|-----| æˆ– | --- | --- |ï¼‰
            separator_lines = [l for l in table_like_lines
                             if re.search(r'\|[\s\-:]+\|', l) and '-' in l]
            if separator_lines:
                logger.debug("æ£€æµ‹åˆ° Markdown è¡¨æ ¼")
                return True

        # æ£€æµ‹ä»£ç å—
        if re.search(r'```[\s\S]*?```', text):
            logger.debug("æ£€æµ‹åˆ°ä»£ç å—")
            return True

        # æ£€æµ‹å…¬å¼ï¼ˆLaTeX æˆ– $$ åŒ…å›´ï¼‰
        if re.search(r'\$\$[\s\S]+?\$\$', text) or re.search(r'\\begin\{equation\}', text):
            logger.debug("æ£€æµ‹åˆ°æ•°å­¦å…¬å¼")
            return True

        return False

    def _contains_step_sequence(self, text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«ç¼–å·æ­¥éª¤åºåˆ—

        è¯†åˆ«æ¨¡å¼ï¼š
        - (1)(2)(3)... æ ¼å¼
        - 1. 2. 3. ... æ ¼å¼
        - 1) 2) 3) ... æ ¼å¼
        - â‘  â‘¡ â‘¢ ... æ ¼å¼ï¼ˆåœ†åœˆæ•°å­—ï¼‰

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            æ˜¯å¦åŒ…å«æ­¥éª¤åºåˆ—
        """
        import re

        # æ¨¡å¼1: (1)(2)(3) æ ¼å¼ - è‡³å°‘3ä¸ªè¿ç»­æ­¥éª¤
        pattern1 = r'\((\d+)\)[^\(]*\((\d+)\)[^\(]*\((\d+)\)'
        if re.search(pattern1, text):
            logger.debug("æ£€æµ‹åˆ° (1)(2)(3) æ ¼å¼çš„æ­¥éª¤åºåˆ—")
            return True

        # æ¨¡å¼2: 1. 2. 3. æ ¼å¼ï¼ˆè¡Œé¦–ï¼‰ - è‡³å°‘3ä¸ªè¿ç»­æ­¥éª¤
        lines = text.split('\n')
        numbered_lines = []
        for line in lines:
            match = re.match(r'^\s*(\d+)\.\s+', line)
            if match:
                numbered_lines.append(int(match.group(1)))

        # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘3ä¸ªè¿ç»­çš„æ•°å­—
        if len(numbered_lines) >= 3:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­åºåˆ—ï¼ˆå…è®¸éƒ¨åˆ†è¿ç»­ï¼‰
            for i in range(len(numbered_lines) - 2):
                if (numbered_lines[i+1] == numbered_lines[i] + 1 and
                    numbered_lines[i+2] == numbered_lines[i] + 2):
                    logger.debug("æ£€æµ‹åˆ° 1. 2. 3. æ ¼å¼çš„æ­¥éª¤åºåˆ—")
                    return True

        # æ¨¡å¼3: 1) 2) 3) æ ¼å¼
        pattern3 = r'(\d+)\)[^\d\)]*(\d+)\)[^\d\)]*(\d+)\)'
        if re.search(pattern3, text):
            logger.debug("æ£€æµ‹åˆ° 1) 2) 3) æ ¼å¼çš„æ­¥éª¤åºåˆ—")
            return True

        # æ¨¡å¼4: åœ†åœˆæ•°å­— â‘  â‘¡ â‘¢
        circle_nums = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']
        found_circle_nums = [cn for cn in circle_nums if cn in text]
        if len(found_circle_nums) >= 3:
            logger.debug("æ£€æµ‹åˆ°åœ†åœˆæ•°å­—æ ¼å¼çš„æ­¥éª¤åºåˆ—")
            return True

        return False

    def _extract_atomic_tag(self, content: str) -> tuple:
        """
        æå– ATOMIC æ ‡ç­¾

        Args:
            content: å†…å®¹æ–‡æœ¬

        Returns:
            (clean_content, is_atomic, atomic_type, context_tags) å…ƒç»„
        """
        import re

        # åŒ¹é… ATOMIC æ ‡ç­¾
        atomic_pattern = r'<ATOMIC-(\w+)>(.*?)</ATOMIC-\1>'
        match = re.search(atomic_pattern, content, re.DOTALL)

        if match:
            atomic_type = match.group(1).lower()
            clean_content = match.group(2)

            # æå–ä¸Šä¸‹æ–‡æ ‡ç­¾ï¼ˆç”¨äºè¡¨æ ¼/æ­¥éª¤/å†…å®¹å¬å›ï¼‰
            if atomic_type == "step":
                context_tags = self._extract_step_sequence_context_tags(clean_content)
            elif atomic_type == "content":
                # ATOMIC-CONTENT: æå–æ ‡é¢˜å’Œå…³é”®è¯ä½œä¸ºä¸Šä¸‹æ–‡
                context_tags = self._extract_content_context_tags(clean_content)
            else:
                context_tags = self._extract_table_context_tags(clean_content, atomic_type)

            return clean_content, True, atomic_type, context_tags

        return content, False, None, []

    def _extract_table_context_tags(self, content: str, atomic_type: str) -> list:
        """
        æå–è¡¨æ ¼/ä»£ç å—/æ­¥éª¤åºåˆ—çš„ä¸Šä¸‹æ–‡æ ‡ç­¾

        Args:
            content: ATOMIC å—å†…å®¹
            atomic_type: ATOMIC ç±»å‹ï¼ˆtable, code, link, formulaï¼‰

        Returns:
            ä¸Šä¸‹æ–‡æ ‡ç­¾åˆ—è¡¨
        """
        import re

        tags = []

        # è¡¨æ ¼ç±»å‹ï¼šæå–æ ‡é¢˜å’ŒåŠ ç²—æ–‡æœ¬
        if atomic_type == "table":
            lines = content.split('\n')
            for line in lines:
                line = line.strip()

                # åŒ¹é…æ ‡é¢˜ï¼ˆ# æ ‡é¢˜ï¼‰
                title_match = re.match(r'^#+\s+(.+)$', line)
                if title_match:
                    title_text = title_match.group(1).strip()
                    tags.append(title_text)

                # åŒ¹é…åŠ ç²—æ–‡æœ¬ï¼ˆ**æ–‡æœ¬**ï¼‰
                bold_matches = re.findall(r'\*\*(.+?)\*\*', line)
                tags.extend(bold_matches)

        # å»é‡å¹¶è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾
        unique_tags = []
        for tag in tags:
            if tag and tag not in unique_tags:
                unique_tags.append(tag)

        return unique_tags[:3]  # æœ€å¤šè¿”å›3ä¸ªæ ‡ç­¾

    def _detect_headers_in_chunk(self, text: str) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„ Markdown æ ‡é¢˜ï¼Œç”¨äºæ ‡é¢˜ä¼˜å…ˆåˆ‡åˆ†

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            æ ‡é¢˜åˆ—è¡¨ï¼ŒåŒ…å«ä½ç½®å’Œçº§åˆ«ä¿¡æ¯
        """
        import re
        headers = []

        # åŒ¹é… Markdown æ ‡é¢˜ï¼š# å¼€å¤´ï¼Œæ”¯æŒ 1-6 çº§
        header_pattern = r'^(#{1,6})\s+(.+)$'

        lines = text.split('\n')
        current_pos = 0

        for line in lines:
            match = re.match(header_pattern, line.strip())
            if match:
                level = len(match.group(1))  # # çš„æ•°é‡
                header_text = match.group(2).strip()

                # è®¡ç®—æ ‡é¢˜åœ¨æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®
                char_position = text.find(line, current_pos)
                if char_position != -1:
                    char_position_ratio = char_position / len(text) if len(text) > 0 else 0.0

                    # åªè®°å½•ä½ç½®æ¯”ä¾‹ > 0.2 çš„æ ‡é¢˜ï¼ˆä¸åœ¨æœ€å¼€å¤´ï¼‰
                    # è¿™æ ·å¯ä»¥åœ¨åˆé€‚çš„åœ°æ–¹åˆ‡åˆ†
                    if char_position_ratio > 0.2:
                        headers.append({
                            "level": level,
                            "text": header_text,
                            "char_position": char_position,
                            "char_position_ratio": char_position_ratio,
                            "line": line
                        })

            current_pos += len(line) + 1  # +1 for \n

        return headers

    def _split_at_headers(
        self,
        chunk_text: str,
        clean_token_start: int,
        clean_tokens: List[int],
        headers: List[Dict[str, Any]],
        user_tag: str,
        content_tags: List[str],
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        åœ¨æ ‡é¢˜ä½ç½®åˆ‡åˆ†æ–‡æœ¬

        ç­–ç•¥ï¼š
        1. æ‰¾åˆ°ä½ç½®æœ€ä¼˜çš„æ ‡é¢˜ä½œä¸ºåˆ‡åˆ†ç‚¹ï¼ˆ30%-70%ä½ç½®ï¼‰
        2. åœ¨æ ‡é¢˜ä¹‹å‰åˆ‡åˆ†ï¼Œç¡®ä¿æ ‡é¢˜ä½äºæ–°å—å¼€å¤´
        3. é€’å½’å¤„ç†åˆ‡åˆ†åçš„å­å—

        Args:
            chunk_text: è¦åˆ‡åˆ†çš„æ–‡æœ¬
            clean_token_start: Token èµ·å§‹ä½ç½®
            clean_tokens: Token åºåˆ—
            headers: æ£€æµ‹åˆ°çš„æ ‡é¢˜åˆ—è¡¨
            user_tag: ç”¨æˆ·æ ‡ç­¾
            content_tags: å†…å®¹æ ‡ç­¾
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Final-Chunk åˆ—è¡¨ï¼Œå¦‚æœåˆ‡åˆ†å¤±è´¥è¿”å› None
        """
        if not headers:
            return None

        # æ‰¾åˆ°æœ€ä¼˜åˆ‡åˆ†ç‚¹ï¼šä½ç½®åœ¨ 30%-70% ä¹‹é—´çš„æ ‡é¢˜
        best_header = None
        for header in headers:
            ratio = header["char_position_ratio"]
            if 0.3 <= ratio <= 0.7:
                if not best_header or abs(ratio - 0.5) < abs(best_header["char_position_ratio"] - 0.5):
                    best_header = header

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç†æƒ³ä½ç½®çš„æ ‡é¢˜ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡é¢˜
        if not best_header:
            best_header = headers[0]

        split_pos = best_header["char_position"]

        # åœ¨æ ‡é¢˜ä¹‹å‰åˆ‡åˆ†ï¼ˆç¡®ä¿æ ‡é¢˜åœ¨ä¸‹ä¸€ä¸ªå—çš„å¼€å¤´ï¼‰
        before_text = chunk_text[:split_pos].strip()
        after_text = chunk_text[split_pos:].strip()

        if not before_text or not after_text:
            # åˆ‡åˆ†å¤±è´¥ï¼Œå—å¤ªå°
            return None

        logger.debug(
            f"ğŸ“ åœ¨æ ‡é¢˜å¤„åˆ‡åˆ†: '{best_header['text']}' "
            f"(ä½ç½®: {best_header['char_position_ratio']:.1%})"
        )

        # åˆ›å»ºä¸¤ä¸ªå­å—
        final_chunks = []

        try:
            # ç¬¬ä¸€ä¸ªå—ï¼ˆæ ‡é¢˜ä¹‹å‰çš„å†…å®¹ï¼‰
            before_tokens = self.tokenizer.encode(before_text)
            before_token_start = clean_token_start
            before_token_end = before_token_start + len(before_tokens)

            final_chunks.append(self._create_final_chunk(
                content=before_text,
                token_start=before_token_start,
                token_end=before_token_end,
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=False,
                atomic_type=None,
                base_tokens=base_tokens,
                context_tags=[]
            ))

            # ç¬¬äºŒä¸ªå—ï¼ˆæ ‡é¢˜åŠä¹‹åçš„å†…å®¹ï¼‰
            after_tokens = self.tokenizer.encode(after_text)
            after_token_start = before_token_end
            after_token_end = after_token_start + len(after_tokens)

            final_chunks.append(self._create_final_chunk(
                content=after_text,
                token_start=after_token_start,
                token_end=after_token_end,
                user_tag=user_tag,
                content_tags=content_tags,
                is_atomic=False,
                atomic_type=None,
                base_tokens=base_tokens,
                context_tags=[f"æ ‡é¢˜: {best_header['text']}"]
            ))

            return final_chunks

        except Exception as e:
            logger.warning(f"âš ï¸ æ ‡é¢˜åˆ‡åˆ†å¤±è´¥: {e}")
            return None

    def _extract_step_sequence_context_tags(self, content: str) -> list:
        """
        æå–æ­¥éª¤åºåˆ—çš„ä¸Šä¸‹æ–‡æ ‡ç­¾

        æå–ç­–ç•¥ï¼š
        - æå–æ­¥éª¤åºåˆ—å‰çš„æ ‡é¢˜
        - æå–æ¯ä¸ªæ­¥éª¤çš„å…³é”®æ“ä½œåŠ¨è¯
        - æå–åŠ ç²—çš„å…³é”®æœ¯è¯­

        Args:
            content: æ­¥éª¤åºåˆ—å†…å®¹

        Returns:
            ä¸Šä¸‹æ–‡æ ‡ç­¾åˆ—è¡¨
        """
        import re

        tags = []

        lines = content.split('\n')

        # æå–æ ‡é¢˜
        for line in lines:
            line = line.strip()
            title_match = re.match(r'^#+\s+(.+)$', line)
            if title_match:
                title_text = title_match.group(1).strip()
                tags.append(title_text)

        # æå–æ­¥éª¤ä¸­çš„å…³é”®åŠ¨è¯ï¼ˆå¦‚ï¼šå®‰è£…ã€é…ç½®ã€å¯åŠ¨ç­‰ï¼‰
        action_verbs = ['å®‰è£…', 'é…ç½®', 'å¯åŠ¨', 'è®¾ç½®', 'è¿æ¥', 'æ‰“å¼€', 'å…³é—­',
                       'åˆ›å»º', 'åˆ é™¤', 'ä¿®æ”¹', 'æ£€æŸ¥', 'æµ‹è¯•', 'è¿è¡Œ', 'æ‰§è¡Œ']

        for verb in action_verbs:
            if verb in content:
                tags.append(verb)

        # æå–åŠ ç²—æ–‡æœ¬
        bold_matches = re.findall(r'\*\*(.+?)\*\*', content)
        tags.extend(bold_matches)

        # å»é‡å¹¶è¿”å›å‰5ä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾
        unique_tags = []
        for tag in tags:
            if tag and tag not in unique_tags and len(tag) <= 10:  # è¿‡æ»¤è¿‡é•¿çš„æ ‡ç­¾
                unique_tags.append(tag)

        return unique_tags[:5]  # æ­¥éª¤åºåˆ—è¿”å›æ›´å¤šæ ‡ç­¾ï¼ˆæœ€å¤š5ä¸ªï¼‰

    def _extract_content_context_tags(self, content: str) -> list:
        """
        æå– ATOMIC-CONTENT çš„ä¸Šä¸‹æ–‡æ ‡ç­¾
        
        æå–ç­–ç•¥ï¼š
        - æå–æ‰€æœ‰æ ‡é¢˜ï¼ˆ#, ##, ### ç­‰ï¼‰
        - æå–åŠ ç²—çš„å…³é”®æœ¯è¯­
        - é™åˆ¶æ ‡ç­¾é•¿åº¦å’Œæ•°é‡
        
        Args:
            content: ATOMIC-CONTENT å†…å®¹
            
        Returns:
            ä¸Šä¸‹æ–‡æ ‡ç­¾åˆ—è¡¨
        """
        import re
        
        tags = []
        lines = content.split('\n')
        
        # æå–æ‰€æœ‰æ ‡é¢˜
        for line in lines:
            line = line.strip()
            title_match = re.match(r'^#+\s+(.+)$', line)
            if title_match:
                title_text = title_match.group(1).strip()
                tags.append(title_text)
        
        # æå–åŠ ç²—æ–‡æœ¬
        bold_matches = re.findall(r'\*\*(.+?)\*\*', content)
        tags.extend(bold_matches)
        
        # å»é‡å¹¶è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾
        unique_tags = []
        for tag in tags:
            if tag and tag not in unique_tags and len(tag) <= 15:  # è¿‡æ»¤è¿‡é•¿çš„æ ‡ç­¾
                unique_tags.append(tag)
        
        return unique_tags[:3]  # ATOMIC-CONTENT è¿”å›æœ€å¤š3ä¸ªæ ‡ç­¾


    def _create_final_chunk(
        self,
        content: str,
        token_start: int,
        token_end: int,
        user_tag: str,
        content_tags: List[str],
        is_atomic: bool,
        atomic_type: str,
        base_tokens: List[int],
        context_tags: List[str] = None
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºæœ€ç»ˆ RAG Chunk

        Args:
            content: å†…å®¹æ–‡æœ¬
            token_start: èµ·å§‹ Token ç´¢å¼•
            token_end: ç»“æŸ Token ç´¢å¼•
            user_tag: ç”¨æˆ·å®šä¹‰æ ‡ç­¾
            content_tags: å†…å®¹æ¨ç†æ ‡ç­¾
            is_atomic: æ˜¯å¦ä¸º ATOMIC å—
            atomic_type: ATOMIC ç±»å‹
            base_tokens: åŸºçº¿ Token åºåˆ—
            context_tags: ä¸Šä¸‹æ–‡æ ‡ç­¾ï¼ˆç”¨äºè¡¨æ ¼å¬å›ï¼‰

        Returns:
            Final-Chunk å­—å…¸
        """
        token_count = token_end - token_start

        # éªŒè¯ Token èŒƒå›´
        is_valid = self.mapper.validate_token_range(
            token_start,
            token_end,
            base_tokens,
            content
        )

        chunk = {
            "content": content,
            "token_start": token_start,
            "token_end": token_end,
            "token_count": token_count,
            "user_tag": user_tag,
            "content_tags": content_tags,
            "is_atomic": is_atomic,
            "atomic_type": atomic_type,
            "validation_passed": is_valid,
            "char_count": len(content)
        }

        # ä¸º ATOMIC è¡¨æ ¼æ·»åŠ ä¸Šä¸‹æ–‡æ ‡ç­¾
        if is_atomic and atomic_type == "table" and context_tags:
            chunk["table_context_tags"] = context_tags
            logger.debug(f"æå–è¡¨æ ¼ä¸Šä¸‹æ–‡æ ‡ç­¾: {context_tags}")

        return chunk

    def _create_fallback_final_chunk(
        self,
        clean_chunk: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        åˆ›å»º fallback æœ€ç»ˆå—

        Args:
            clean_chunk: Clean-Chunk æ•°æ®

        Returns:
            Final-Chunk å­—å…¸
        """
        return {
            "content": clean_chunk["text"],
            "token_start": clean_chunk["token_start"],
            "token_end": clean_chunk["token_end"],
            "token_count": clean_chunk["token_count"],
            "user_tag": clean_chunk.get("user_tag", "æœªåˆ†ç±»"),
            "content_tags": clean_chunk.get("content_tags", []),
            "is_atomic": False,
            "atomic_type": None,
            "validation_passed": True,
            "char_count": clean_chunk["char_count"],
            "is_fallback": True
        }

    def _validate_token_overflow(self, final_chunks: List[Dict[str, Any]]):
        """
        éªŒè¯ Token æº¢å‡ºæƒ…å†µ

        Args:
            final_chunks: Final-Chunk åˆ—è¡¨
        """
        logger.info("\næ‰§è¡Œ Token æº¢å‡ºæ ¡éªŒ...")

        overflow_chunks = []
        underflow_chunks = []

        for i, chunk in enumerate(final_chunks):
            token_count = chunk["token_count"]
            is_atomic = chunk["is_atomic"]

            # è¯¦ç»†è°ƒè¯•æ—¥å¿—
            logger.debug(
                f"æ£€æŸ¥ Chunk {i+1}: token_count={token_count}, is_atomic={is_atomic}, "
                f"FINAL_MAX_TOKENS={ChunkConfig.FINAL_MAX_TOKENS}"
            )

            # æ£€æŸ¥è¶…è¿‡æœ€å¤§é™åˆ¶
            if token_count > ChunkConfig.FINAL_MAX_TOKENS:
                if is_atomic:
                    # ATOMIC å—å…è®¸è¶…è¿‡é™åˆ¶
                    if token_count > ChunkConfig.ATOMIC_MAX_TOKENS:
                        logger.warning(
                            f"âš ï¸ Chunk {i+1}: ATOMIC å—è¶…è¿‡é™åˆ¶ "
                            f"({token_count} > {ChunkConfig.ATOMIC_MAX_TOKENS})"
                        )
                        overflow_chunks.append((i+1, token_count, "ATOMIC"))
                    else:
                        logger.debug(
                            f"âœ… Chunk {i+1}: ATOMIC å— ({token_count} tokens)"
                        )
                else:
                    # é ATOMIC å—è¶…è¿‡é™åˆ¶
                    logger.error(
                        f"âŒ Chunk {i+1}: é ATOMIC å—è¶…è¿‡æœ€å¤§é™åˆ¶ "
                        f"({token_count} > {ChunkConfig.FINAL_MAX_TOKENS})"
                    )
                    overflow_chunks.append((i+1, token_count, "NORMAL"))

                    if ValidationConfig.STRICT_MODE:
                        raise ValueError(
                            f"ä¸¥æ ¼æ¨¡å¼ï¼šChunk {i+1} è¶…è¿‡æœ€å¤§ Token é™åˆ¶"
                        )

            # æ£€æŸ¥å°äºæœ€å°é™åˆ¶
            elif token_count < ChunkConfig.FINAL_MIN_TOKENS:
                logger.warning(
                    f"âš ï¸ Chunk {i+1}: å°äºæœ€å°é™åˆ¶ "
                    f"({token_count} < {ChunkConfig.FINAL_MIN_TOKENS})"
                )
                underflow_chunks.append((i+1, token_count))

        # è¾“å‡ºæ‘˜è¦
        if overflow_chunks:
            logger.warning(f"å‘ç° {len(overflow_chunks)} ä¸ªæº¢å‡ºå—")
            # è¿”å›æº¢å‡ºå—ä¿¡æ¯ä¾›åç»­ä¿®å¤ä½¿ç”¨
            return overflow_chunks
        if underflow_chunks:
            logger.warning(f"å‘ç° {len(underflow_chunks)} ä¸ªè¿‡å°å—")

        if not overflow_chunks and not underflow_chunks:
            logger.info("âœ… æ‰€æœ‰å—çš„ Token æ•°é‡å‡åœ¨åˆç†èŒƒå›´å†…")

        return []

    def _auto_fix_overflow_chunks(
        self,
        chunks: List[Dict[str, Any]],
        overflow_info: List[tuple]
    ) -> List[Dict[str, Any]]:
        """
        è‡ªåŠ¨ä¿®å¤è¶…é™çš„é ATOMIC å—

        å¯¹äºè¶…è¿‡ FINAL_MAX_TOKENS ä½†æœªè¶…è¿‡ FINAL_HARD_LIMIT çš„é ATOMIC å—ï¼Œ
        è‡ªåŠ¨æ ‡è®°ä¸º ATOMIC-CONTENT ä»¥ä¿æŒè¯­ä¹‰å®Œæ•´æ€§

        Args:
            chunks: Final-Chunk åˆ—è¡¨
            overflow_info: æº¢å‡ºå—ä¿¡æ¯ [(chunk_id, token_count, type), ...]

        Returns:
            ä¿®å¤åçš„ Final-Chunk åˆ—è¡¨
        """
        if not overflow_info:
            return chunks

        logger.info("\nå¼€å§‹è‡ªåŠ¨ä¿®å¤è¶…é™å—...")

        fixed_count = 0
        fixed_chunks = chunks.copy()

        for chunk_id, token_count, overflow_type in overflow_info:
            if overflow_type == "NORMAL":
                chunk_index = chunk_id - 1

                # æ£€æŸ¥æ˜¯å¦åœ¨ç¡¬æ€§ä¸Šé™ä»¥å†…
                if token_count <= ChunkConfig.FINAL_HARD_LIMIT:
                    logger.info(
                        f"ğŸ”§ ä¿®å¤ Chunk {chunk_id}: {token_count} tokens "
                        f"â†’ æ ‡è®°ä¸º ATOMIC-CONTENTï¼ˆè¯­ä¹‰å®Œæ•´æ€§ä¼˜å…ˆï¼‰"
                    )

                    # æ ‡è®°ä¸º ATOMIC-CONTENT
                    fixed_chunks[chunk_index]["is_atomic"] = True
                    fixed_chunks[chunk_index]["atomic_type"] = "content"

                    # æå–ä¸Šä¸‹æ–‡æ ‡ç­¾
                    content = fixed_chunks[chunk_index]["content"]
                    context_tags = self._extract_content_context_tags(content)
                    if context_tags:
                        fixed_chunks[chunk_index]["content_tags"].extend(context_tags)
                        # å»é‡
                        fixed_chunks[chunk_index]["content_tags"] = list(set(
                            fixed_chunks[chunk_index]["content_tags"]
                        ))

                    fixed_count += 1
                else:
                    logger.error(
                        f"âŒ Chunk {chunk_id} è¶…è¿‡ç¡¬æ€§ä¸Šé™ ({token_count} > {ChunkConfig.FINAL_HARD_LIMIT})ï¼Œ"
                        f"æ— æ³•è‡ªåŠ¨ä¿®å¤ï¼Œå»ºè®®æ£€æŸ¥æºæ–‡æ¡£æˆ–è°ƒæ•´åˆ‡åˆ†ç­–ç•¥"
                    )

        if fixed_count > 0:
            logger.info(f"âœ… å·²è‡ªåŠ¨ä¿®å¤ {fixed_count} ä¸ªè¶…é™å—")

        return fixed_chunks

    def _merge_small_chunks(
        self,
        chunks: List[Dict[str, Any]],
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        åˆå¹¶è¿‡å°çš„ chunksï¼ˆå°äº 100 tokens ä¸”é ATOMICï¼‰

        Args:
            chunks: Final-Chunk åˆ—è¡¨
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            åˆå¹¶åçš„ Final-Chunk åˆ—è¡¨
        """
        if not chunks:
            return chunks

        if not ChunkConfig.ENABLE_SMALL_CHUNK_MERGE:
            logger.info("å° chunk åˆå¹¶åŠŸèƒ½å·²ç¦ç”¨")
            return chunks

        MIN_CHUNK_SIZE = ChunkConfig.SMALL_CHUNK_THRESHOLD
        merged = []
        i = 0

        while i < len(chunks):
            current = chunks[i]

            # å¦‚æœå½“å‰ chunk å°äºé˜ˆå€¼ã€é ATOMICï¼Œä¸”åé¢è¿˜æœ‰ chunk
            if (current["token_count"] < MIN_CHUNK_SIZE and
                not current["is_atomic"] and
                i + 1 < len(chunks)):

                next_chunk = chunks[i + 1]

                # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸ä¸‹ä¸€ä¸ª chunk åˆå¹¶ï¼ˆé ATOMIC æˆ–åˆå¹¶åä¸è¶…è¿‡æœ€å¤§é™åˆ¶ï¼‰
                merged_token_count = current["token_count"] + next_chunk["token_count"]

                if (not next_chunk["is_atomic"] and
                    merged_token_count <= ChunkConfig.FINAL_MAX_TOKENS):

                    # åˆå¹¶å†…å®¹
                    merged_content = current["content"] + "\n\n" + next_chunk["content"]

                    # åˆ›å»ºåˆå¹¶åçš„ chunk
                    merged_chunk = self._create_final_chunk(
                        content=merged_content,
                        token_start=current["token_start"],
                        token_end=next_chunk["token_end"],
                        user_tag=current["user_tag"],
                        content_tags=current["content_tags"],
                        is_atomic=False,
                        atomic_type=None,
                        base_tokens=base_tokens,
                        context_tags=[]
                    )

                    logger.info(
                        f"âœ… åˆå¹¶å° chunk: {current['token_count']} + {next_chunk['token_count']} "
                        f"= {merged_chunk['token_count']} tokens"
                    )

                    merged.append(merged_chunk)
                    i += 2  # è·³è¿‡å·²åˆå¹¶çš„ä¸¤ä¸ª chunk
                    continue

            # æ— æ³•åˆå¹¶ï¼Œä¿ç•™å½“å‰ chunk
            merged.append(current)
            i += 1

        return merged

    def _validate_token_continuity(self, chunks: List[Dict[str, Any]]):
        """
        éªŒè¯ token åºåˆ—çš„è¿ç»­æ€§ï¼Œæ£€æµ‹å¤§çš„ token gap

        Args:
            chunks: Final-Chunk åˆ—è¡¨
        """
        if len(chunks) < 2:
            return

        if not ValidationConfig.CHECK_TOKEN_CONTINUITY:
            return

        logger.info("\næ‰§è¡Œ Token è¿ç»­æ€§æ£€æµ‹...")

        GAP_THRESHOLD = ValidationConfig.TOKEN_GAP_THRESHOLD
        gaps_found = []

        for i in range(len(chunks) - 1):
            current = chunks[i]
            next_chunk = chunks[i + 1]

            gap = next_chunk["token_start"] - current["token_end"]

            if gap > GAP_THRESHOLD:
                logger.warning(
                    f"âš ï¸ æ£€æµ‹åˆ° Token Gap: Chunk {i+1} -> Chunk {i+2}\n"
                    f"   Chunk {i+1} ç»“æŸäº token {current['token_end']}\n"
                    f"   Chunk {i+2} èµ·å§‹äº token {next_chunk['token_start']}\n"
                    f"   Gap å¤§å°: {gap} tokens\n"
                    f"   å¯èƒ½åŸå› : é˜¶æ®µ2æ¸…æ´—æ—¶åˆ é™¤äº†ä¸­é—´å†…å®¹ï¼ˆå›¾ç‰‡é“¾æ¥ã€æ‚è´¨ç­‰ï¼‰"
                )
                gaps_found.append({
                    "between_chunks": (i+1, i+2),
                    "gap_size": gap,
                    "chunk1_end": current["token_end"],
                    "chunk2_start": next_chunk["token_start"]
                })
            elif gap < 0:
                logger.error(
                    f"âŒ Token é‡å é”™è¯¯: Chunk {i+1} -> Chunk {i+2}\n"
                    f"   Chunk {i+1} ç»“æŸäº token {current['token_end']}\n"
                    f"   Chunk {i+2} èµ·å§‹äº token {next_chunk['token_start']}\n"
                    f"   é‡å : {-gap} tokens"
                )
            elif gap > 0:
                logger.debug(
                    f"âœ“ å° gap: Chunk {i+1} -> Chunk {i+2}, gap={gap} tokens (æ­£å¸¸ï¼Œæ¸…æ´—å¯¼è‡´)"
                )

        if gaps_found:
            logger.warning(f"å…±æ£€æµ‹åˆ° {len(gaps_found)} å¤„è¾ƒå¤§ Token Gap")
        else:
            logger.info("âœ… æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„ Token Gap")

    def _structure_based_chunking(
        self,
        structure_tree: List[Dict[str, Any]],
        original_text: str,
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºæ–‡æ¡£ç»“æ„æ ‘è¿›è¡Œåˆ‡åˆ†

        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆåœ¨é¡¶å±‚ç« èŠ‚è¾¹ç•Œåˆ‡åˆ† (hierarchy_level=1: # 1, # 2, # 3)
        2. å¦‚æœé¡¶å±‚ç« èŠ‚è¿‡å¤§ï¼Œåœ¨äºŒçº§ç« èŠ‚è¾¹ç•Œåˆ‡åˆ† (hierarchy_level=2: # 1.1, # 1.2)
        3. è¡¨æ ¼å¿…é¡»ä¸å…¶æ ‡é¢˜ä¿æŒåœ¨ä¸€èµ·
        4. æ­¥éª¤åºåˆ—ä¿æŒå®Œæ•´
        5. æ¯ä¸ªchunkå¿…é¡»ä»¥æ ‡é¢˜å¼€å¤´

        Args:
            structure_tree: æ–‡æ¡£ç»“æ„æ ‘
            original_text: åŸå§‹æ–‡æœ¬
            base_tokens: Tokenåºåˆ—

        Returns:
            Final chunksåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹ç»“æ„åŒ–åˆ‡åˆ†ï¼Œå…± {len(structure_tree)} ä¸ªç« èŠ‚èŠ‚ç‚¹")

        final_chunks = []
        i = 0

        while i < len(structure_tree):
            node = structure_tree[i]

            # ç‰¹æ®Šå¤„ç†ï¼šæ£€æµ‹ç›®å½•åŒºåŸŸ
            if node.get('is_toc', False):
                # æ”¶é›†æ•´ä¸ªç›®å½•åŒºåŸŸï¼ˆä» toc_start åˆ° toc_endï¼‰
                toc_start_idx = node.get('toc_start_idx')
                toc_end_idx = node.get('toc_end_idx')

                if toc_start_idx is not None and toc_end_idx is not None:
                    # æ”¶é›†ç›®å½•åŒºåŸŸçš„æ‰€æœ‰èŠ‚ç‚¹
                    toc_nodes = []
                    for k in range(len(structure_tree)):
                        if structure_tree[k].get('toc_start_idx') == toc_start_idx:
                            if structure_tree[k].get('is_toc') or structure_tree[k].get('is_toc_part'):
                                toc_nodes.append(structure_tree[k])

                    if toc_nodes:
                        # è®¡ç®—æ•´ä¸ªç›®å½•åŒºåŸŸçš„èŒƒå›´
                        toc_text_start = toc_nodes[0]['char_start']
                        toc_text_end = toc_nodes[-1]['char_end']
                        toc_full_text = original_text[toc_text_start:toc_text_end]
                        toc_tokens = len(self.tokenizer.encode(toc_full_text))

                        logger.info(f"ğŸ“‘ å¤„ç†ç›®å½•åŒºåŸŸ: åŒ…å« {len(toc_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œçº¦ {toc_tokens} tokens")

                        # æ£€æŸ¥ ATOMIC ç¡¬æ€§ä¸Šé™ (3000 tokens)
                        if toc_tokens > ChunkConfig.ATOMIC_MAX_TOKENS:
                            logger.warning(f"âš ï¸ ç›®å½•åŒºåŸŸè¶…è¿‡ ATOMIC_MAX_TOKENS ({toc_tokens} > {ChunkConfig.ATOMIC_MAX_TOKENS})ï¼Œå¼ºåˆ¶åˆ‡åˆ†")
                            # å¼ºåˆ¶åˆ‡åˆ†ï¼šæŒ‰èŠ‚ç‚¹é€ä¸ªæ·»åŠ ï¼Œç¡®ä¿æ¯ä¸ªchunkä¸è¶…è¿‡3000 tokens
                            current_toc_nodes = []
                            current_toc_tokens = 0

                            for toc_node in toc_nodes:
                                node_text = original_text[toc_node['char_start']:toc_node['char_end']]
                                node_tokens = len(self.tokenizer.encode(node_text))

                                # å¦‚æœåŠ ä¸Šå½“å‰èŠ‚ç‚¹ä¼šè¶…é™ï¼Œå…ˆä¿å­˜å½“å‰chunk
                                if current_toc_nodes and current_toc_tokens + node_tokens > ChunkConfig.ATOMIC_MAX_TOKENS:
                                    # åˆ›å»ºå½“å‰çš„TOC chunk
                                    merged_text_start = current_toc_nodes[0]['char_start']
                                    merged_text_end = current_toc_nodes[-1]['char_end']
                                    merged_text = original_text[merged_text_start:merged_text_end]

                                    chunk = self._create_section_chunk(
                                        merged_text,
                                        merged_text_start,
                                        current_toc_nodes[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºæ ‡é¢˜
                                        base_tokens,
                                        original_text
                                    )
                                    final_chunks.append(chunk)
                                    logger.debug(f"  âœ… ç›®å½•éƒ¨åˆ†chunk ({current_toc_tokens} tokens)")

                                    # é‡ç½®
                                    current_toc_nodes = []
                                    current_toc_tokens = 0

                                # æ·»åŠ å½“å‰èŠ‚ç‚¹
                                current_toc_nodes.append(toc_node)
                                current_toc_tokens += node_tokens

                            # å¤„ç†å‰©ä½™èŠ‚ç‚¹
                            if current_toc_nodes:
                                merged_text_start = current_toc_nodes[0]['char_start']
                                merged_text_end = current_toc_nodes[-1]['char_end']
                                merged_text = original_text[merged_text_start:merged_text_end]

                                chunk = self._create_section_chunk(
                                    merged_text,
                                    merged_text_start,
                                    current_toc_nodes[0],
                                    base_tokens,
                                    original_text
                                )
                                final_chunks.append(chunk)
                                logger.debug(f"  âœ… ç›®å½•æœ€åéƒ¨åˆ†chunk ({current_toc_tokens} tokens)")
                        else:
                            # æœªè¶…é™ï¼Œä½œä¸ºå•ä¸ªATOMIC chunk
                            chunk = self._create_section_chunk(
                                toc_full_text,
                                toc_text_start,
                                node,  # ä½¿ç”¨ç›®å½•æ ‡é¢˜èŠ‚ç‚¹
                                base_tokens,
                                original_text
                            )
                            final_chunks.append(chunk)
                            logger.debug(f"  âœ… ç›®å½•ä½œä¸ºATOMIC chunk ({toc_tokens} tokens)")

                        # è·³è¿‡æ‰€æœ‰ç›®å½•ç›¸å…³çš„èŠ‚ç‚¹
                        while i < len(structure_tree) and (
                            structure_tree[i].get('is_toc') or structure_tree[i].get('is_toc_part')
                        ):
                            i += 1
                        continue
                    else:
                        # æ²¡æœ‰æ‰¾åˆ°ç›®å½•èŠ‚ç‚¹ï¼ŒæŒ‰å•ä¸ªæ ‡é¢˜å¤„ç†
                        section_text = original_text[node['char_start']:node['char_end']]
                        estimated_tokens = len(self.tokenizer.encode(section_text))
                        chunk = self._create_section_chunk(
                            section_text,
                            node['char_start'],
                            node,
                            base_tokens,
                            original_text
                        )
                        final_chunks.append(chunk)
                        logger.debug(f"  âœ… ç›®å½•æ ‡é¢˜ä½œä¸ºATOMIC chunk ({estimated_tokens} tokens)")
                        i += 1
                        continue
                else:
                    # åªæœ‰ç›®å½•æ ‡é¢˜ï¼Œæ²¡æœ‰åŒºåŸŸ
                    section_text = original_text[node['char_start']:node['char_end']]
                    estimated_tokens = len(self.tokenizer.encode(section_text))
                    chunk = self._create_section_chunk(
                        section_text,
                        node['char_start'],
                        node,
                        base_tokens,
                        original_text
                    )
                    final_chunks.append(chunk)
                    logger.debug(f"  âœ… ç›®å½•æ ‡é¢˜ä½œä¸ºATOMIC chunk ({estimated_tokens} tokens)")
                    i += 1
                    continue

            # è·³è¿‡ç›®å½•å†…å®¹èŠ‚ç‚¹ï¼ˆå·²åœ¨ä¸Šé¢å¤„ç†ï¼‰
            if node.get('is_toc_part', False):
                i += 1
                continue

            # è·å–å½“å‰èŠ‚ç‚¹åŠå…¶æ‰€æœ‰å­èŠ‚ç‚¹
            section_nodes = [node]
            j = i + 1

            # æ”¶é›†æ‰€æœ‰å­èŠ‚ç‚¹
            # ç­–ç•¥1: åŸºäºç¼–å·ï¼ˆå¦‚æœæœ‰ç¼–å·ï¼‰
            if node['number']:
                while j < len(structure_tree):
                    next_node = structure_tree[j]
                    if next_node['number'] and next_node['number'].startswith(node['number'] + '.'):
                        section_nodes.append(next_node)
                        j += 1
                    else:
                        break
            else:
                # ç­–ç•¥2: åŸºäºå±‚çº§å…³ç³»ï¼ˆæ— ç¼–å·çš„çˆ¶æ ‡é¢˜ï¼‰
                # æ”¶é›†æ‰€æœ‰å±‚çº§æ›´æ·±çš„å­èŠ‚ç‚¹ï¼Œç›´åˆ°é‡åˆ°åŒçº§æˆ–æ›´é«˜çº§çš„èŠ‚ç‚¹
                current_level = node['hierarchy_level']
                while j < len(structure_tree):
                    next_node = structure_tree[j]
                    # å¦‚æœä¸‹ä¸€ä¸ªèŠ‚ç‚¹å±‚çº§æ›´æ·±ï¼Œè¯´æ˜æ˜¯å­èŠ‚ç‚¹
                    if next_node['hierarchy_level'] > current_level:
                        section_nodes.append(next_node)
                        j += 1
                    else:
                        # é‡åˆ°åŒçº§æˆ–æ›´é«˜çº§çš„èŠ‚ç‚¹ï¼Œåœæ­¢
                        break

            # è®¡ç®—æ•´ä¸ªç« èŠ‚çš„èŒƒå›´
            section_start = node['char_start']
            section_end = section_nodes[-1]['char_end']
            section_text = original_text[section_start:section_end]

            # ä¼°ç®—tokenæ•°
            estimated_tokens = len(self.tokenizer.encode(section_text))

            # å¦‚æœåªæœ‰æ ‡é¢˜æ²¡æœ‰å†…å®¹ï¼ˆtoken < 50ï¼‰ï¼Œè·³è¿‡è¿™ä¸ªç©ºèŠ‚ç‚¹
            if len(section_nodes) == 1 and estimated_tokens < 50:
                logger.warning(f"âš ï¸ è·³è¿‡ç©ºæ ‡é¢˜èŠ‚ç‚¹: {node['title']} (ä»… {estimated_tokens} tokens)")
                i = j
                continue

            logger.debug(f"å¤„ç†ç« èŠ‚: {node['number']} {node['title']} "
                        f"(å±‚çº§:{node['hierarchy_level']}, åŒ…å«{len(section_nodes)}ä¸ªèŠ‚ç‚¹, "
                        f"çº¦{estimated_tokens} tokens)")

            # å†³ç­–ï¼šæ˜¯å¦éœ€è¦åˆ‡åˆ†è¿™ä¸ªç« èŠ‚
            if estimated_tokens <= ChunkConfig.FINAL_MAX_TOKENS:
                # æ•´ä¸ªç« èŠ‚ä½œä¸ºä¸€ä¸ªchunk
                chunk = self._create_section_chunk(
                    section_text,
                    section_start,
                    node,
                    base_tokens,
                    original_text
                )
                final_chunks.append(chunk)
                logger.debug(f"  âœ… æ•´ç« èŠ‚ä½œä¸ºä¸€ä¸ªchunk ({estimated_tokens} tokens)")
                i = j
            else:
                # ç« èŠ‚è¿‡å¤§ï¼Œéœ€è¦ç»†åˆ†
                logger.debug(f"  âš ï¸ ç« èŠ‚è¿‡å¤§ï¼Œè¿›è¡Œå­ç« èŠ‚åˆ‡åˆ†...")
                sub_chunks = self._split_large_section(
                    section_nodes,
                    original_text,
                    base_tokens
                )
                final_chunks.extend(sub_chunks)
                i = j

        logger.info(f"âœ… ç»“æ„åŒ–åˆ‡åˆ†å®Œæˆï¼Œç”Ÿæˆ {len(final_chunks)} ä¸ªchunks")
        return final_chunks

    def _create_section_chunk(
        self,
        section_text: str,
        char_start: int,
        node: Dict[str, Any],
        base_tokens: List[int],
        original_text: str
    ) -> Dict[str, Any]:
        """
        ä»ç« èŠ‚åˆ›å»ºchunk

        Args:
            section_text: ç« èŠ‚æ–‡æœ¬
            char_start: å­—ç¬¦èµ·å§‹ä½ç½®
            node: ç« èŠ‚èŠ‚ç‚¹ä¿¡æ¯
            base_tokens: Tokenåºåˆ—
            original_text: åŸå§‹æ–‡æ¡£å…¨æ–‡

        Returns:
            Final chunk
        """
        # ç¼–ç è·å–tokenä¿¡æ¯
        section_tokens = self.tokenizer.encode(section_text)
        token_count = len(section_tokens)

        # ä½¿ç”¨å­—ç¬¦ä½ç½®è®¡ç®—tokenä½ç½®ï¼ˆåŸºäºåŸå§‹æ–‡æœ¬ï¼‰
        # è®¡ç®—ä»æ–‡æ¡£å¼€å§‹åˆ°å½“å‰ç« èŠ‚å¼€å§‹çš„tokenæ•°é‡
        text_before = original_text[:char_start]
        tokens_before = self.tokenizer.encode(text_before)
        token_start = len(tokens_before)
        token_end = token_start + token_count

        # ç¡®å®šæ˜¯å¦ä¸ºATOMIC
        is_atomic = False
        atomic_type = None

        # ä¼˜å…ˆçº§1: ç›®å½• (å¿…é¡»ä¿æŒå®Œæ•´)
        if node.get('is_toc', False):
            is_atomic = True
            atomic_type = "toc"
        # ä¼˜å…ˆçº§2: è¡¨æ ¼è¶…è¿‡é™åˆ¶
        elif node['has_table'] and token_count > ChunkConfig.FINAL_MAX_TOKENS:
            is_atomic = True
            atomic_type = "table"
        # ä¼˜å…ˆçº§3: æ­¥éª¤åºåˆ— (ä¿æŒé€»è¾‘è¿ç»­æ€§)
        elif node['has_steps']:
            is_atomic = True
            atomic_type = "step"
        # ä¼˜å…ˆçº§4: å†…å®¹è¶…è¿‡é™åˆ¶
        elif token_count > ChunkConfig.FINAL_MAX_TOKENS:
            is_atomic = True
            atomic_type = "content"

        # ç”Ÿæˆæ ‡ç­¾
        content_tags = []
        if node['number']:
            content_tags.append(f"ç« èŠ‚{node['number']}")
        if node.get('is_toc', False):
            content_tags.append("ç›®å½•")
        if node['has_table']:
            content_tags.append("è¡¨æ ¼")
        if node['has_code']:
            content_tags.append("ä»£ç ")
        if node['has_steps']:
            content_tags.append("æ­¥éª¤")

        return {
            "content": section_text,
            "char_start": char_start,
            "char_end": char_start + len(section_text),
            "token_start": token_start,
            "token_end": token_end,
            "token_count": token_count,
            "is_atomic": is_atomic,
            "atomic_type": atomic_type,
            "user_tag": node.get('title', ''),
            "content_tags": content_tags[:5],
            "context_tags": [f"hierarchy_level_{node['hierarchy_level']}"],
            "section_number": node.get('number'),
            "section_title": node.get('title'),
            "validation_passed": True,  # æ–°å¢ï¼šç”¨äºç»Ÿè®¡
            "validation_notes": []       # æ–°å¢ï¼šç”¨äºè®°å½•éªŒè¯ä¿¡æ¯
        }

    def _split_large_section(
        self,
        section_nodes: List[Dict[str, Any]],
        original_text: str,
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        åˆ‡åˆ†è¿‡å¤§çš„ç« èŠ‚

        ç­–ç•¥ï¼šåœ¨å­ç« èŠ‚è¾¹ç•Œåˆ‡åˆ†

        Args:
            section_nodes: ç« èŠ‚èŠ‚ç‚¹åˆ—è¡¨ï¼ˆçˆ¶èŠ‚ç‚¹+æ‰€æœ‰å­èŠ‚ç‚¹ï¼‰
            original_text: åŸå§‹æ–‡æœ¬
            base_tokens: Tokenåºåˆ—

        Returns:
            Final chunksåˆ—è¡¨
        """
        chunks = []
        parent_node = section_nodes[0]

        # æŒ‰äºŒçº§æ ‡é¢˜åˆ‡åˆ†
        current_group = []
        current_start_node = None

        for node in section_nodes:
            # å¦‚æœæ˜¯é¡¶å±‚èŠ‚ç‚¹æˆ–äºŒçº§èŠ‚ç‚¹çš„å¼€å§‹
            if node == parent_node or (node['hierarchy_level'] == parent_node['hierarchy_level'] + 1):
                # ä¿å­˜ä¹‹å‰çš„ç»„
                if current_group:
                    group_text = original_text[current_start_node['char_start']:current_group[-1]['char_end']]
                    chunk = self._create_section_chunk(
                        group_text,
                        current_start_node['char_start'],
                        current_start_node,
                        base_tokens,
                        original_text
                    )
                    chunks.append(chunk)

                # å¼€å§‹æ–°ç»„
                current_group = [node]
                current_start_node = node
            else:
                # æ·»åŠ åˆ°å½“å‰ç»„
                current_group.append(node)

        # å¤„ç†æœ€åä¸€ç»„
        if current_group and current_start_node:
            group_text = original_text[current_start_node['char_start']:current_group[-1]['char_end']]
            chunk = self._create_section_chunk(
                group_text,
                current_start_node['char_start'],
                current_start_node,
                base_tokens,
                original_text
            )
            chunks.append(chunk)

        logger.debug(f"    å­ç« èŠ‚åˆ‡åˆ†: {len(section_nodes)} ä¸ªèŠ‚ç‚¹ â†’ {len(chunks)} ä¸ªchunks")
        return chunks

    def _calculate_statistics(self, final_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è®¡ç®—ç»Ÿè®¡ä¿¡æ¯

        Args:
            final_chunks: Final-Chunk åˆ—è¡¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not final_chunks:
            return {
                "total_chunks": 0,
                "avg_tokens": 0,
                "min_tokens": 0,
                "max_tokens": 0,
                "atomic_chunks": 0,
                "validation_pass_rate": 0
            }

        token_counts = [c["token_count"] for c in final_chunks]
        atomic_count = sum(1 for c in final_chunks if c["is_atomic"])
        validated_count = sum(1 for c in final_chunks if c["validation_passed"])

        return {
            "total_chunks": len(final_chunks),
            "avg_tokens": sum(token_counts) / len(token_counts),
            "min_tokens": min(token_counts),
            "max_tokens": max(token_counts),
            "atomic_chunks": atomic_count,
            "validation_pass_rate": validated_count / len(final_chunks)
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    try:
        from tokenizer.tokenizer_client import get_tokenizer
    except ImportError:
        from ..tokenizer.tokenizer_client import get_tokenizer

    tokenizer = get_tokenizer()

    # æ¨¡æ‹Ÿé˜¶æ®µ2çš„è¾“å‡º
    test_text = """# RAG ç³»ç»Ÿä»‹ç»
è¿™æ˜¯ä¸€ä¸ªæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿã€‚å®ƒå¯ä»¥æé«˜ LLM çš„å‡†ç¡®æ€§ã€‚

## ä¸»è¦ç‰¹ç‚¹
1. æ™ºèƒ½æ£€ç´¢
2. è¯­ä¹‰åŒ¹é…
3. ä¸Šä¸‹æ–‡å¢å¼º
"""

    base_tokens = tokenizer.encode(test_text)
    clean_tokens = base_tokens

    stage2_result = {
        "base_tokens": base_tokens,
        "clean_chunks": [
            {
                "text": test_text,
                "token_start": 0,
                "token_end": len(base_tokens),
                "token_count": len(clean_tokens),
                "tokens": clean_tokens,
                "user_tag": "æŠ€æœ¯æ–‡æ¡£",
                "content_tags": ["RAG", "æ£€ç´¢", "ç”Ÿæˆ"],
                "validation_passed": True,
                "char_count": len(test_text)
            }
        ]
    }

    # å¤„ç†é˜¶æ®µ3
    processor = Stage3RefineLocate()
    result = processor.process(stage2_result)

    print(f"\n=== é˜¶æ®µ3å¤„ç†ç»“æœ ===")
    print(f"æœ€ç»ˆå—æ•°é‡: {result['statistics']['total_chunks']}")
    for i, chunk in enumerate(result["final_chunks"], 1):
        print(f"\nChunk {i}:")
        print(f"  Token: [{chunk['token_start']}:{chunk['token_end']}] ({chunk['token_count']})")
        print(f"  æ ‡ç­¾: {chunk['user_tag']} | {chunk['content_tags']}")
        print(f"  ATOMIC: {chunk['is_atomic']}")
        print(f"  å†…å®¹: {chunk['content'][:100]}...")
