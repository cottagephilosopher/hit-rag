"""
é˜¶æ®µ1: åŸºçº¿å»ºç«‹ä¸ç²—åˆ‡
å»ºç«‹ Token ç»å¯¹ç´¢å¼•åŸºçº¿ï¼Œå¹¶æŒ‰å¥å­è¾¹ç•Œç²—åˆ‡æˆ Mid-Chunks
"""

import logging
import re
from typing import List, Dict, Any, Tuple

# ä½¿ç”¨ç»å¯¹å¯¼å…¥
try:
    from tokenizer.tokenizer_client import get_tokenizer
    from tokenizer.token_mapper import TokenMapper
    from config import ChunkConfig
except ImportError:
    from ..tokenizer.tokenizer_client import get_tokenizer
    from ..tokenizer.token_mapper import TokenMapper
    from ..config import ChunkConfig

logger = logging.getLogger(__name__)


class Stage1Baseline:
    """
    é˜¶æ®µ1: åŸºçº¿å»ºç«‹ä¸ç²—åˆ‡
    """

    def __init__(self):
        self.tokenizer = get_tokenizer()
        self.mapper = TokenMapper(self.tokenizer)

    def process(self, markdown_text: str) -> Dict[str, Any]:
        """
        å¤„ç†é˜¶æ®µ1: å»ºç«‹åŸºçº¿å¹¶ç²—åˆ‡

        Args:
            markdown_text: åŸå§‹ Markdown æ–‡æœ¬

        Returns:
            åŒ…å«åŸºçº¿å’Œ Mid-Chunks çš„å­—å…¸
        """
        logger.info("=" * 60)
        logger.info("é˜¶æ®µ1: å¼€å§‹å»ºç«‹åŸºçº¿ä¸ç²—åˆ‡")
        logger.info("=" * 60)

        # 1. å»ºç«‹ Token ç»å¯¹ç´¢å¼•åŸºçº¿
        base_tokens, original_text = self.mapper.build_baseline(markdown_text)
        logger.info(f"âœ… åŸºçº¿å»ºç«‹å®Œæˆ: {len(base_tokens)} tokens, {len(original_text)} å­—ç¬¦")

        # 2. æ„å»ºæ–‡æ¡£ç»“æ„æ ‘
        structure_tree = self.build_document_structure_tree(original_text)
        logger.info(f"âœ… ç»“æ„æ ‘æ„å»ºå®Œæˆ: {len(structure_tree)} ä¸ªç« èŠ‚")

        # 3. ç²—åˆ‡æˆ Mid-Chunks (ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œä½†åç»­åº”ä½¿ç”¨ç»“æ„æ ‘)
        mid_chunks = self._coarse_split(original_text, base_tokens)
        logger.info(f"âœ… ç²—åˆ‡å®Œæˆ: {len(mid_chunks)} ä¸ª Mid-Chunks")

        # 4. æ„å»ºç»“æœ
        result = {
            "base_tokens": base_tokens,
            "original_text": original_text,
            "structure_tree": structure_tree,  # æ–°å¢ï¼šæ–‡æ¡£ç»“æ„æ ‘
            "mid_chunks": mid_chunks,
            "statistics": {
                "total_tokens": len(base_tokens),
                "total_chars": len(original_text),
                "mid_chunk_count": len(mid_chunks),
                "avg_chunk_tokens": sum(c["token_count"] for c in mid_chunks) / len(mid_chunks) if mid_chunks else 0,
                "structure_nodes": len(structure_tree)  # æ–°å¢ï¼šç»“æ„æ ‘èŠ‚ç‚¹æ•°
            }
        }

        logger.info(f"\né˜¶æ®µ1ç»Ÿè®¡:")
        logger.info(f"  æ€» Tokens: {result['statistics']['total_tokens']}")
        logger.info(f"  æ€»å­—ç¬¦æ•°: {result['statistics']['total_chars']}")
        logger.info(f"  Mid-Chunk æ•°é‡: {result['statistics']['mid_chunk_count']}")
        logger.info(f"  å¹³å‡ Chunk Tokens: {result['statistics']['avg_chunk_tokens']:.1f}")

        return result

    def _coarse_split(
        self,
        text: str,
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        ç²—åˆ‡æ–‡æœ¬æˆ Mid-Chunks

        Args:
            text: åŸå§‹æ–‡æœ¬
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Mid-Chunk åˆ—è¡¨
        """
        # å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_into_paragraphs(text)
        logger.debug(f"æ–‡æœ¬åˆ†å‰²ä¸º {len(paragraphs)} ä¸ªæ®µè½/å—")

        mid_chunks = []
        current_chunk_paras = []
        current_chunk_chars = 0
        current_token_start = 0

        for para in paragraphs:
            para_chars = len(para["text"])
            para_type = para.get("type", "paragraph")

            # æ£€æŸ¥å•ä¸ªæ®µè½æ˜¯å¦è¿‡å¤§
            if para_chars > ChunkConfig.MID_CHUNK_MAX_CHARS:
                # å¦‚æœå½“å‰æœ‰ç´¯ç§¯çš„æ®µè½ï¼Œå…ˆä¿å­˜
                if current_chunk_paras:
                    chunk = self._create_mid_chunk(
                        current_chunk_paras,
                        current_token_start,
                        base_tokens
                    )
                    mid_chunks.append(chunk)
                    current_token_start = chunk["token_end"]
                    current_chunk_paras = []
                    current_chunk_chars = 0

                # ç‰¹æ®Šå—ï¼ˆè¡¨æ ¼ã€ä»£ç å—ï¼‰ä¸åˆ‡åˆ†ï¼Œä½œä¸ºæ•´ä½“ä¿ç•™
                if para_type in ["html_table", "markdown_table", "code_block"]:
                    logger.debug(f"âš ï¸ å¤§å‹ç‰¹æ®Šå— ({para_type}, {para_chars} å­—ç¬¦)ï¼Œä½œä¸ºæ•´ä½“ä¿ç•™")
                    chunk = self._create_mid_chunk_from_text(
                        para["text"],
                        current_token_start,
                        base_tokens
                    )
                    mid_chunks.append(chunk)
                    current_token_start = chunk["token_end"]
                else:
                    # æ™®é€šæ®µè½ï¼šæŒ‰å¥å­åˆ‡åˆ†
                    large_para_chunks = self._split_large_paragraph(
                        para["text"],
                        current_token_start,
                        base_tokens
                    )
                    mid_chunks.extend(large_para_chunks)
                    if large_para_chunks:
                        current_token_start = large_para_chunks[-1]["token_end"]

            # æ£€æŸ¥æ·»åŠ å½“å‰æ®µè½æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            elif current_chunk_chars + para_chars > ChunkConfig.MID_CHUNK_MAX_CHARS:
                # ä¿å­˜å½“å‰ chunk
                if current_chunk_paras:
                    chunk = self._create_mid_chunk(
                        current_chunk_paras,
                        current_token_start,
                        base_tokens
                    )
                    mid_chunks.append(chunk)
                    current_token_start = chunk["token_end"]

                # å¼€å§‹æ–° chunk
                current_chunk_paras = [para]
                current_chunk_chars = para_chars

            else:
                # æ·»åŠ åˆ°å½“å‰ chunk
                current_chunk_paras.append(para)
                current_chunk_chars += para_chars

        # å¤„ç†å‰©ä½™çš„æ®µè½
        if current_chunk_paras:
            chunk = self._create_mid_chunk(
                current_chunk_paras,
                current_token_start,
                base_tokens
            )
            mid_chunks.append(chunk)

        return mid_chunks

    def _split_into_paragraphs(self, text: str) -> List[Dict[str, str]]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½å’Œç‰¹æ®Šå—ï¼ˆä¿ç•™æ ¼å¼ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« text å’Œ type
        """
        paragraphs = []

        # è¯†åˆ«ç‰¹æ®Šå—ï¼ˆä»£ç å—ã€è¡¨æ ¼ç­‰ï¼‰
        special_blocks = self._identify_special_blocks(text)

        if not special_blocks:
            # æ²¡æœ‰ç‰¹æ®Šå—ï¼ŒæŒ‰åŒæ¢è¡Œåˆ†å‰²
            parts = re.split(r'\n\n+', text)
            for part in parts:
                if part.strip():
                    paragraphs.append({
                        "text": part,
                        "type": "paragraph"
                    })
            return paragraphs

        # æœ‰ç‰¹æ®Šå—ï¼Œéœ€è¦æŒ‰é¡ºåºå¤„ç†
        current_pos = 0
        for block_start, block_end, block_type in special_blocks:
            # å¤„ç†ç‰¹æ®Šå—ä¹‹å‰çš„æ™®é€šæ–‡æœ¬
            if block_start > current_pos:
                normal_text = text[current_pos:block_start]
                parts = re.split(r'\n\n+', normal_text)
                for part in parts:
                    if part.strip():
                        paragraphs.append({
                            "text": part,
                            "type": "paragraph"
                        })

            # æ·»åŠ ç‰¹æ®Šå—
            paragraphs.append({
                "text": text[block_start:block_end],
                "type": block_type
            })

            current_pos = block_end

        # å¤„ç†æœ€åä¸€ä¸ªç‰¹æ®Šå—ä¹‹åçš„æ–‡æœ¬
        if current_pos < len(text):
            normal_text = text[current_pos:]
            parts = re.split(r'\n\n+', normal_text)
            for part in parts:
                if part.strip():
                    paragraphs.append({
                        "text": part,
                        "type": "paragraph"
                    })

        # åå¤„ç†ï¼šåˆå¹¶è¡¨æ ¼å‰çš„æ ‡é¢˜æ®µè½
        paragraphs = self._merge_title_with_table_paragraphs(paragraphs)

        return paragraphs

    def _merge_title_with_table_paragraphs(self, paragraphs: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        åˆå¹¶è¡¨æ ¼å‰çš„æ ‡é¢˜æ®µè½
        
        å¦‚æœæ£€æµ‹åˆ°ï¼šparagraph + table çš„æ¨¡å¼ï¼Œä¸” paragraph æ˜¯æ ‡é¢˜ï¼Œåˆ™åˆå¹¶
        
        Args:
            paragraphs: æ®µè½åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ®µè½åˆ—è¡¨
        """
        if len(paragraphs) < 2:
            return paragraphs
        
        merged = []
        i = 0
        
        while i < len(paragraphs):
            current = paragraphs[i]
            
            # æ£€æŸ¥ä¸‹ä¸€ä¸ªæ˜¯å¦æ˜¯è¡¨æ ¼
            if i + 1 < len(paragraphs):
                next_para = paragraphs[i + 1]
                
                # å¦‚æœå½“å‰æ˜¯ paragraphï¼Œä¸‹ä¸€ä¸ªæ˜¯è¡¨æ ¼
                if (current["type"] == "paragraph" and 
                    next_para["type"] in ["html_table", "markdown_table"]):
                    
                    # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦æ˜¯æ ‡é¢˜
                    text = current["text"].strip()
                    is_title = (
                        # Markdown æ ‡é¢˜
                        text.startswith('#') or
                        # ç¼–å·æ ‡é¢˜ï¼ˆå¦‚ "1.2äº§å“è§„æ ¼"ï¼‰
                        bool(re.match(r'^\d+\.[\d\.]*\s*.+$', text, re.MULTILINE)) or
                        # çŸ­æ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
                        (len(text) < 50 and '\n' not in text)
                    )
                    
                    if is_title:
                        # åˆå¹¶æ ‡é¢˜å’Œè¡¨æ ¼
                        merged_text = current["text"] + "\n\n" + next_para["text"]
                        merged.append({
                            "text": merged_text,
                            "type": next_para["type"]  # ä¿æŒè¡¨æ ¼ç±»å‹
                        })
                        logger.debug(f"âœ… Stage1: åˆå¹¶æ ‡é¢˜ä¸è¡¨æ ¼: {text[:30]}...")
                        i += 2  # è·³è¿‡ä¸¤ä¸ªæ®µè½
                        continue
            
            # å¦åˆ™ä¿ç•™åŸæ®µè½
            merged.append(current)
            i += 1
        
        return merged

    def _identify_special_blocks(self, text: str) -> List[Tuple[int, int, str]]:
        """
        è¯†åˆ«æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå—ï¼ˆä»£ç å—ã€è¡¨æ ¼ç­‰ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            (start, end, type) å…ƒç»„åˆ—è¡¨
        """
        blocks = []

        # è¯†åˆ«ä»£ç å—
        code_pattern = r'```[\s\S]*?```'
        for match in re.finditer(code_pattern, text):
            blocks.append((match.start(), match.end(), "code_block"))

        # è¯†åˆ« HTML è¡¨æ ¼ï¼ˆ<table>...</table>ï¼‰
        html_table_pattern = r'<table[^>]*>.*?</table>'
        for match in re.finditer(html_table_pattern, text, re.DOTALL | re.IGNORECASE):
            blocks.append((match.start(), match.end(), "html_table"))

        # è¯†åˆ« Markdown è¡¨æ ¼ï¼ˆè¿ç»­çš„ | ... | è¡Œï¼‰
        table_pattern = r'(\|.+\|\s*\n)+'
        for match in re.finditer(table_pattern, text):
            blocks.append((match.start(), match.end(), "markdown_table"))

        # æŒ‰å¼€å§‹ä½ç½®æ’åº
        blocks.sort(key=lambda x: x[0])

        return blocks

    def _split_large_paragraph(
        self,
        para_text: str,
        token_start: int,
        base_tokens: List[int]
    ) -> List[Dict[str, Any]]:
        """
        å°†è¶…å¤§æ®µè½æŒ‰å¥å­åˆ‡åˆ†

        Args:
            para_text: æ®µè½æ–‡æœ¬
            token_start: èµ·å§‹ Token ç´¢å¼•
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Mid-Chunk åˆ—è¡¨
        """
        # æŒ‰å¥å­åˆ†å‰²
        sentences = self._split_into_sentences(para_text)

        chunks = []
        current_sentences = []
        current_chars = 0

        for sentence in sentences:
            sent_chars = len(sentence)

            if current_chars + sent_chars > ChunkConfig.MID_CHUNK_MAX_CHARS and current_sentences:
                # ä¿å­˜å½“å‰ chunk
                chunk_text = "".join(current_sentences)
                chunk = self._create_mid_chunk_from_text(
                    chunk_text,
                    token_start,
                    base_tokens
                )
                chunks.append(chunk)
                token_start = chunk["token_end"]

                # å¼€å§‹æ–° chunk
                current_sentences = [sentence]
                current_chars = sent_chars
            else:
                current_sentences.append(sentence)
                current_chars += sent_chars

        # å¤„ç†å‰©ä½™å¥å­
        if current_sentences:
            chunk_text = "".join(current_sentences)
            chunk = self._create_mid_chunk_from_text(
                chunk_text,
                token_start,
                base_tokens
            )
            chunks.append(chunk)

        return chunks

    def _split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆå¥å­

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            å¥å­åˆ—è¡¨
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«å¥å­ç»“æŸç¬¦
        # ä¿ç•™æ¢è¡Œç¬¦ä½œä¸ºå¥å­è¾¹ç•Œ
        sentences = []
        current_sentence = []

        # å¥å­ç»“æŸç¬¦
        sentence_ends = r'[ã€‚ï¼ï¼Ÿ.!?\n]'

        for char in text:
            current_sentence.append(char)
            if re.match(sentence_ends, char):
                sentences.append("".join(current_sentence))
                current_sentence = []

        # å¤„ç†å‰©ä½™å­—ç¬¦
        if current_sentence:
            sentences.append("".join(current_sentence))

        return sentences

    def _create_mid_chunk(
        self,
        paragraphs: List[Dict[str, str]],
        token_start: int,
        base_tokens: List[int]
    ) -> Dict[str, Any]:
        """
        ä»æ®µè½åˆ—è¡¨åˆ›å»º Mid-Chunk

        Args:
            paragraphs: æ®µè½åˆ—è¡¨
            token_start: èµ·å§‹ Token ç´¢å¼•
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Mid-Chunk å­—å…¸
        """
        chunk_text = "\n\n".join(p["text"] for p in paragraphs)
        return self._create_mid_chunk_from_text(chunk_text, token_start, base_tokens)

    def _create_mid_chunk_from_text(
        self,
        chunk_text: str,
        token_start: int,
        base_tokens: List[int]
    ) -> Dict[str, Any]:
        """
        ä»æ–‡æœ¬åˆ›å»º Mid-Chunk

        Args:
            chunk_text: Chunk æ–‡æœ¬
            token_start: èµ·å§‹ Token ç´¢å¼•
            base_tokens: åŸºçº¿ Token åºåˆ—

        Returns:
            Mid-Chunk å­—å…¸
        """
        # ç¼–ç  chunk æ–‡æœ¬
        chunk_tokens = self.tokenizer.encode(chunk_text)
        token_end = token_start + len(chunk_tokens)

        # éªŒè¯ Token èŒƒå›´
        is_valid = self.mapper.validate_token_range(
            token_start,
            token_end,
            base_tokens,
            chunk_text
        )

        if not is_valid:
            logger.warning(f"âš ï¸ Mid-Chunk Token èŒƒå›´éªŒè¯å¤±è´¥")

        # æ£€æµ‹æ ‡é¢˜ä½ç½®
        headers = self._detect_headers(chunk_text)

        return {
            "text": chunk_text,
            "token_start": token_start,
            "token_end": token_end,
            "token_count": len(chunk_tokens),
            "char_count": len(chunk_text),
            "validation_passed": is_valid,
            "headers": headers  # æ–°å¢ï¼šæ ‡é¢˜ä¿¡æ¯
        }

    def _detect_headers(self, text: str) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„ Markdown æ ‡é¢˜

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            æ ‡é¢˜åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            - level: æ ‡é¢˜çº§åˆ« (1-6)
            - text: æ ‡é¢˜æ–‡æœ¬
            - char_position: æ ‡é¢˜åœ¨æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®
            - char_position_ratio: æ ‡é¢˜ä½ç½®å æ¯” (0.0-1.0)
        """
        headers = []

        # åŒ¹é… Markdown æ ‡é¢˜ï¼š# å¼€å¤´ï¼Œæ”¯æŒ 1-6 çº§
        # åŒæ—¶åŒ¹é…ç¼–å·æ ‡é¢˜å¦‚ "# 1.2 æ ‡é¢˜" æˆ– "## 3.1.2 æ ‡é¢˜"
        header_pattern = r'^(#{1,6})\s+(.+)$'

        lines = text.split('\n')
        current_pos = 0

        for line in lines:
            match = re.match(header_pattern, line.strip())
            if match:
                level = len(match.group(1))  # # çš„æ•°é‡
                header_text = match.group(2).strip()

                # è®¡ç®—æ ‡é¢˜åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®
                char_position = text.find(line, current_pos)
                if char_position != -1:
                    char_position_ratio = char_position / len(text) if len(text) > 0 else 0.0

                    headers.append({
                        "level": level,
                        "text": header_text,
                        "char_position": char_position,
                        "char_position_ratio": char_position_ratio
                    })

            current_pos += len(line) + 1  # +1 for \n

        return headers

    def build_document_structure_tree(self, text: str) -> List[Dict[str, Any]]:
        """
        æ„å»ºæ–‡æ¡£ç»“æ„æ ‘ï¼ŒåŸºäºæ ‡é¢˜ç¼–å·è¯†åˆ«å±‚çº§å…³ç³»

        Args:
            text: æ–‡æ¡£æ–‡æœ¬

        Returns:
            ç»“æ„æ ‘èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªèŠ‚ç‚¹åŒ…å«ï¼š
            - level: Markdownæ ‡é¢˜çº§åˆ« (1-6, å³#çš„æ•°é‡)
            - hierarchy_level: é€»è¾‘å±‚çº§ (åŸºäºç¼–å·: 1, 1.1, 1.2.1)
            - number: ç¼–å· (å¦‚ "1", "1.2", "3.1.2")
            - title: æ ‡é¢˜æ–‡æœ¬
            - char_start: èµ·å§‹å­—ç¬¦ä½ç½®
            - char_end: ç»“æŸå­—ç¬¦ä½ç½® (ä¸‹ä¸€ä¸ªåŒçº§æˆ–ä¸Šçº§æ ‡é¢˜çš„ä½ç½®)
            - children: å­èŠ‚ç‚¹åˆ—è¡¨
            - has_table: æ˜¯å¦åŒ…å«è¡¨æ ¼
            - has_code: æ˜¯å¦åŒ…å«ä»£ç å—
            - has_steps: æ˜¯å¦åŒ…å«æ­¥éª¤åºåˆ—
        """
        # æå–æ‰€æœ‰æ ‡é¢˜
        header_pattern = r'^(#{1,6})\s+(.+)$'
        headers = []

        for match in re.finditer(header_pattern, text, re.MULTILINE):
            level = len(match.group(1))
            full_title = match.group(2).strip()
            char_start = match.start()

            # å°è¯•æå–ç¼–å· (å¦‚ "1", "1.2", "3.1.2")
            number_match = re.match(r'^([\d\.]+)\s*(.*)$', full_title)
            if number_match:
                number = number_match.group(1).rstrip('.')
                title = number_match.group(2).strip()
                # è®¡ç®—é€»è¾‘å±‚çº§ï¼š1=é¡¶å±‚, 1.1=äºŒçº§, 1.2.1=ä¸‰çº§
                hierarchy_level = len(number.split('.'))
            else:
                number = None
                title = full_title
                # æ— ç¼–å·çš„æ ‡é¢˜ï¼Œä½¿ç”¨Markdownå±‚çº§
                hierarchy_level = level

            headers.append({
                'level': level,
                'hierarchy_level': hierarchy_level,
                'number': number,
                'title': title,
                'full_title': full_title,
                'char_start': char_start,
                'char_end': None  # ç¨åè®¡ç®—
            })

        # è®¡ç®—æ¯ä¸ªæ ‡é¢˜çš„å†…å®¹èŒƒå›´
        for i in range(len(headers)):
            if i < len(headers) - 1:
                headers[i]['char_end'] = headers[i + 1]['char_start']
            else:
                headers[i]['char_end'] = len(text)

        # æ£€æµ‹ç›®å½•åŒºåŸŸï¼šä»"ç›®å½•"æ ‡é¢˜åˆ°æ­£æ–‡å¼€å§‹çš„æ•´ä¸ªåŒºåŸŸ
        # ç­–ç•¥ï¼šè¯†åˆ«å¸¦é¡µç çš„ç›®å½•æ¡ç›®,æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸å¸¦é¡µç çš„æ­£æ–‡æ ‡é¢˜
        toc_start_idx = None
        toc_end_idx = None

        def is_toc_entry(title_text):
            """æ£€æµ‹æ ‡é¢˜æ˜¯å¦æ˜¯ç›®å½•æ¡ç›®(é€šå¸¸æœ«å°¾æœ‰é¡µç æ•°å­—)"""
            # ç§»é™¤æ ‡é¢˜æ ‡è®°
            clean = title_text.strip().lstrip('#').strip()
            # æ£€æŸ¥æ˜¯å¦ä»¥æ•°å­—ç»“å°¾(é¡µç )
            # ä¾‹å¦‚: "å®‰å…¨ä¿¡æ¯ 2", "äº§å“ç®€ä»‹ 6", "äº§å“ç»´æŠ¤ 30"
            return bool(re.search(r'\s+\d+\s*$', clean))

        for i, header in enumerate(headers):
            if re.search(r'ç›®å½•|contents|catalogue|table\s+of\s+contents', header['title'], re.IGNORECASE):
                toc_start_idx = i
                # ä»ç›®å½•æ ‡é¢˜åæŸ¥æ‰¾TOCæ¡ç›®
                for j in range(i + 1, len(headers)):
                    current_title = headers[j]['title']

                    # æ£€æŸ¥å½“å‰æ ‡é¢˜æ˜¯å¦æ˜¯TOCæ¡ç›®
                    if not is_toc_entry(current_title):
                        # ä¸æ˜¯TOCæ¡ç›®(æ²¡æœ‰é¡µç ),å¯èƒ½æ˜¯æ­£æ–‡å¼€å§‹
                        # å†æ£€æŸ¥å†…å®¹:å¦‚æœæœ‰å¤§é‡æ­£æ–‡,ç¡®è®¤æ˜¯æ­£æ–‡ç« èŠ‚
                        section_content = text[headers[j]['char_start']:headers[j]['char_end']]
                        content_lines = [line for line in section_content.split('\n')
                                       if line.strip() and not line.strip().startswith('#')
                                       and not re.match(r'^\s*[\d\.]+\s+', line)
                                       and not re.match(r'^\s*\d+\.\s*', line)  # æ’é™¤ç¼–å·åˆ—è¡¨
                                       and len(line.strip()) > 10]  # æ’é™¤è¿‡çŸ­çš„è¡Œ

                        if len(content_lines) > 3 or len(section_content) > 200:
                            # æœ‰å®è´¨å†…å®¹,ç¡®è®¤TOCç»“æŸ
                            toc_end_idx = j
                            logger.info(f"ğŸ“‘ æ£€æµ‹åˆ°ç›®å½•åŒºåŸŸ: ä»æ ‡é¢˜ #{i+1} '{header['title']}' åˆ°æ ‡é¢˜ #{j} '{headers[j]['title']}' (é¦–ä¸ªä¸å¸¦é¡µç çš„æ­£æ–‡æ ‡é¢˜)")
                            break
                    # å¦‚æœæ˜¯TOCæ¡ç›®,ç»§ç»­æ£€æŸ¥ä¸‹ä¸€ä¸ªæ ‡é¢˜

                # å¦‚æœä¸€ç›´æ²¡æ‰¾åˆ°ç»“æŸä½ç½®,å–æœ€åä¸€ä¸ªTOCæ¡ç›®
                if toc_end_idx is None and toc_start_idx is not None:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªå¸¦é¡µç çš„æ ‡é¢˜
                    for j in range(len(headers) - 1, i, -1):
                        if is_toc_entry(headers[j]['title']):
                            toc_end_idx = j + 1  # TOCç»“æŸåœ¨æœ€åä¸€ä¸ªæ¡ç›®çš„ä¸‹ä¸€ä¸ªä½ç½®
                            logger.info(f"ğŸ“‘ æ£€æµ‹åˆ°ç›®å½•åŒºåŸŸ: ä»æ ‡é¢˜ #{i+1} '{header['title']}' åˆ°æ ‡é¢˜ #{j+1} (æœ€åä¸€ä¸ªTOCæ¡ç›®)")
                            break

                break  # åªå¤„ç†ç¬¬ä¸€ä¸ªç›®å½•

        # åˆ†ææ¯ä¸ªç« èŠ‚çš„å†…å®¹ç‰¹å¾
        for i, header in enumerate(headers):
            section_content = text[header['char_start']:header['char_end']]

            # æ£€æµ‹è¡¨æ ¼
            header['has_table'] = '<table>' in section_content or bool(re.search(r'\|.*\|.*\|', section_content))
            header['table_count'] = section_content.count('<table>') + len(re.findall(r'\n\|.*\|.*\|\n', section_content))

            # æ£€æµ‹ä»£ç å—
            header['has_code'] = '```' in section_content

            # æ£€æµ‹æ­¥éª¤åºåˆ—
            header['has_steps'] = bool(re.search(r'\(1\)|\(2\)|\(3\)|^1\.|^2\.|^3\.', section_content, re.MULTILINE))

            # æ ‡è®°ç›®å½•åŒºåŸŸ
            is_toc = False
            is_toc_part = False  # æ˜¯å¦æ˜¯ç›®å½•çš„ä¸€éƒ¨åˆ†ï¼ˆä½†ä¸æ˜¯å¼€å§‹æ ‡é¢˜ï¼‰

            if toc_start_idx is not None and toc_end_idx is not None:
                if i == toc_start_idx:
                    is_toc = True  # ç›®å½•å¼€å§‹æ ‡é¢˜
                elif toc_start_idx < i < toc_end_idx:
                    is_toc_part = True  # ç›®å½•å†…å®¹çš„ä¸€éƒ¨åˆ†
            elif toc_start_idx is not None and i == toc_start_idx:
                # åªæœ‰ç›®å½•æ ‡é¢˜ï¼Œæ²¡æœ‰æ‰¾åˆ°ç»“æŸä½ç½®
                is_toc = True

            header['is_toc'] = is_toc
            header['is_toc_part'] = is_toc_part
            header['toc_start_idx'] = toc_start_idx
            header['toc_end_idx'] = toc_end_idx

            # è®¡ç®—å†…å®¹é•¿åº¦
            header['content_length'] = header['char_end'] - header['char_start']

        logger.info(f"âœ… æ„å»ºæ–‡æ¡£ç»“æ„æ ‘: {len(headers)} ä¸ªç« èŠ‚")
        for h in headers[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.debug(f"  {'  ' * (h['hierarchy_level']-1)}{h['number'] or ''} {h['title']} "
                        f"({h['content_length']} chars, è¡¨æ ¼:{h['table_count']})")

        return headers


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # æµ‹è¯•æ–‡æœ¬
    test_md = """# æ–‡æ¡£æ ‡é¢˜

è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ã€‚åŒ…å«å¤šä¸ªå¥å­ã€‚è¿™æ˜¯ç¬¬ä¸‰å¥ã€‚

## ç¬¬ä¸€èŠ‚

è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚

```python
def hello():
    print("Hello, World!")
```

è¿™æ˜¯ä»£ç å—åçš„å†…å®¹ã€‚

| åˆ—1 | åˆ—2 |
|-----|-----|
| A   | B   |
| C   | D   |

è¡¨æ ¼åçš„å†…å®¹ã€‚
"""

    processor = Stage1Baseline()
    result = processor.process(test_md)

    print(f"\n=== é˜¶æ®µ1å¤„ç†ç»“æœ ===")
    print(f"Total Tokens: {result['statistics']['total_tokens']}")
    print(f"Mid-Chunks: {result['statistics']['mid_chunk_count']}")
    print(f"\nMid-Chunks è¯¦æƒ…:")
    for i, chunk in enumerate(result["mid_chunks"], 1):
        print(f"\nChunk {i}:")
        print(f"  Token èŒƒå›´: [{chunk['token_start']}:{chunk['token_end']}]")
        print(f"  Token æ•°: {chunk['token_count']}")
        print(f"  å­—ç¬¦æ•°: {chunk['char_count']}")
        print(f"  éªŒè¯: {'âœ…' if chunk['validation_passed'] else 'âŒ'}")
        print(f"  å†…å®¹é¢„è§ˆ: {chunk['text'][:100]}...")
