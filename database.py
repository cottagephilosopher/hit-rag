"""
æ•°æ®åº“æ“ä½œæ¨¡å—
æä¾› SQLite æ•°æ®åº“çš„è¿æ¥ã€åˆå§‹åŒ–å’ŒåŸºç¡€æ“ä½œ
"""

import sqlite3
import json
import threading
import os
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List
from contextlib import contextmanager
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
DB_FILE = Path(os.getenv("DB_FILE", ".dbs/rag_preprocessor.db"))

# æ•°æ®åº“é”ï¼ˆç”¨äºå¹¶å‘æ§åˆ¶ï¼‰
_DB_LOCK = threading.Lock()


def _json_dump(obj: Any) -> str:
    """å°†å¯¹è±¡åºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸²"""
    if obj is None:
        return None
    return json.dumps(obj, ensure_ascii=False)


def _json_load(text: Optional[str]) -> Any:
    """å°†JSONå­—ç¬¦ä¸²ååºåˆ—åŒ–ä¸ºå¯¹è±¡"""
    if text is None:
        return None
    try:
        return json.loads(text)
    except (json.JSONDecodeError, TypeError):
        return None


@contextmanager
def get_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰"""
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row  # è¿”å›å­—å…¸æ ¼å¼
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()


def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåˆ›å»ºè¡¨å’Œç´¢å¼•ï¼‰"""
    # æ‰§è¡Œæ–‡æ¡£ç›¸å…³è¡¨çš„ schema
    schema_file = Path(__file__).parent / ".dbs/schema.sql"
    if not schema_file.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_file}")

    with open(schema_file, 'r', encoding='utf-8') as f:
        schema_sql = f.read()

    # æ‰§è¡Œå¯¹è¯ç›¸å…³è¡¨çš„ schema
    chat_schema_file = Path(__file__).parent / ".dbs/chat_schema.sql"
    chat_schema_sql = ""
    if chat_schema_file.exists():
        with open(chat_schema_file, 'r', encoding='utf-8') as f:
            chat_schema_sql = f.read()

    with _DB_LOCK:
        with get_connection() as conn:
            # æ‰§è¡Œæ–‡æ¡£è¡¨ schema
            conn.executescript(schema_sql)
            # æ‰§è¡Œå¯¹è¯è¡¨ schemaï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if chat_schema_sql:
                conn.executescript(chat_schema_sql)

    print(f"âœ… Database initialized at: {DB_FILE}")


# ============================================
# Document æ“ä½œ
# ============================================

def create_document(
    filename: str,
    source_path: str,
    status: str = 'pending'
) -> int:
    """åˆ›å»ºæ–‡æ¡£è®°å½•"""
    with _DB_LOCK:
        with get_connection() as conn:
            cursor = conn.execute(
                """
                INSERT INTO documents (filename, source_path, status)
                VALUES (?, ?, ?)
                """,
                (filename, source_path, status)
            )
            return cursor.lastrowid


def get_document(document_id: int) -> Optional[Dict[str, Any]]:
    """æ ¹æ®IDè·å–æ–‡æ¡£"""
    with get_connection() as conn:
        row = conn.execute(
            "SELECT * FROM documents WHERE id = ?",
            (document_id,)
        ).fetchone()
        return dict(row) if row else None


def get_document_by_filename(filename: str) -> Optional[Dict[str, Any]]:
    """æ ¹æ®æ–‡ä»¶åè·å–æ–‡æ¡£"""
    with get_connection() as conn:
        row = conn.execute(
            "SELECT * FROM documents WHERE filename = ?",
            (filename,)
        ).fetchone()
        return dict(row) if row else None


def update_document(
    document_id: int,
    *,
    status: Optional[str] = None,
    total_chunks: Optional[int] = None,
    total_tokens: Optional[int] = None,
    processed_at: Optional[datetime] = None,
    error_message: Optional[str] = None
):
    """æ›´æ–°æ–‡æ¡£ä¿¡æ¯"""
    fields = []
    params = []

    if status is not None:
        fields.append("status = ?")
        params.append(status)

    if total_chunks is not None:
        fields.append("total_chunks = ?")
        params.append(total_chunks)

    if total_tokens is not None:
        fields.append("total_tokens = ?")
        params.append(total_tokens)

    if processed_at is not None:
        fields.append("processed_at = ?")
        params.append(processed_at)

    if error_message is not None:
        fields.append("error_message = ?")
        params.append(error_message)

    if not fields:
        return

    params.append(document_id)

    with _DB_LOCK:
        with get_connection() as conn:
            conn.execute(
                f"UPDATE documents SET {', '.join(fields)} WHERE id = ?",
                params
            )


# ============================================
# Chunk æ“ä½œ
# ============================================

def create_chunk(
    document_id: int,
    chunk_id: int,
    content: str,
    token_start: int,
    token_end: int,
    token_count: int,
    *,
    char_start: Optional[int] = None,
    char_end: Optional[int] = None,
    user_tag: Optional[str] = None,
    content_tags: Optional[List[str]] = None,
    is_atomic: bool = False,
    atomic_type: Optional[str] = None,
    status: int = 0
) -> int:
    """åˆ›å»ºchunkè®°å½•"""
    with _DB_LOCK:
        with get_connection() as conn:
            cursor = conn.execute(
                """
                INSERT INTO document_chunks (
                    document_id, chunk_id, content,
                    token_start, token_end, token_count,
                    char_start, char_end, user_tag, content_tags,
                    is_atomic, atomic_type, status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    document_id, chunk_id, content,
                    token_start, token_end, token_count,
                    char_start, char_end, user_tag,
                    _json_dump(content_tags) if content_tags else None,
                    is_atomic, atomic_type, status
                )
            )
            return cursor.lastrowid


def get_chunk(chunk_id: int) -> Optional[Dict[str, Any]]:
    """æ ¹æ®IDè·å–chunkï¼ˆåŒ…å«source_fileï¼‰"""
    with get_connection() as conn:
        row = conn.execute(
            """
            SELECT c.*, d.filename as source_file
            FROM document_chunks c
            LEFT JOIN documents d ON c.document_id = d.id
            WHERE c.id = ?
            """,
            (chunk_id,)
        ).fetchone()

        if not row:
            return None

        chunk = dict(row)
        chunk['content_tags'] = _json_load(chunk.get('content_tags'))
        return chunk


def get_chunk_by_chunk_id(chunk_id: str) -> Optional[Dict[str, Any]]:
    """æ ¹æ® chunk_idï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰è·å– chunk"""
    with get_connection() as conn:
        row = conn.execute(
            """
            SELECT c.*, d.filename as source_file
            FROM document_chunks c
            LEFT JOIN documents d ON c.document_id = d.id
            WHERE c.chunk_id = ?
            """,
            (chunk_id,)
        ).fetchone()

        if not row:
            return None

        chunk = dict(row)
        chunk['content_tags'] = _json_load(chunk.get('content_tags'))
        return chunk


def get_chunks_by_document(document_id: int) -> List[Dict[str, Any]]:
    """è·å–æ–‡æ¡£çš„æ‰€æœ‰chunks"""
    with get_connection() as conn:
        rows = conn.execute(
            """
            SELECT * FROM document_chunks
            WHERE document_id = ?
            ORDER BY chunk_id ASC
            """,
            (document_id,)
        ).fetchall()

        chunks = []
        for row in rows:
            chunk = dict(row)
            chunk['content_tags'] = _json_load(chunk.get('content_tags'))

            # å¦‚æœæœ‰ç¼–è¾‘å†…å®¹ï¼Œä½¿ç”¨ç¼–è¾‘å†…å®¹ï¼›å¦åˆ™ä½¿ç”¨åŸå§‹å†…å®¹
            # ä½†ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œæˆ‘ä»¬åœ¨è¿”å›æ—¶ä¿ç•™ content å­—æ®µ
            if not chunk.get('edited_content'):
                chunk['edited_content'] = chunk['content']

            chunks.append(chunk)

        return chunks


def update_chunk(
    chunk_id: int,
    *,
    edited_content: Optional[str] = None,
    status: Optional[int] = None,
    content_tags: Optional[List[str]] = None,
    user_tag: Optional[str] = None,
    last_editor_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    æ›´æ–°chunkå¹¶è‡ªåŠ¨å¢åŠ ç‰ˆæœ¬å·
    è¿”å›æ›´æ–°å‰çš„æ•°æ®ç”¨äºè®°å½•æ—¥å¿—
    """
    # å…ˆè·å–æ—§æ•°æ®
    old_chunk = get_chunk(chunk_id)
    if not old_chunk:
        raise ValueError(f"Chunk {chunk_id} not found")

    fields = []
    params = []

    if edited_content is not None:
        fields.append("edited_content = ?")
        params.append(edited_content)

    if status is not None:
        fields.append("status = ?")
        params.append(status)

    if content_tags is not None:
        fields.append("content_tags = ?")
        params.append(_json_dump(content_tags))

    if user_tag is not None:
        fields.append("user_tag = ?")
        params.append(user_tag)

    if last_editor_id is not None:
        fields.append("last_editor_id = ?")
        params.append(last_editor_id)

    if not fields:
        return old_chunk

    # è‡ªåŠ¨å¢åŠ ç‰ˆæœ¬å·
    fields.append("version = COALESCE(version, 0) + 1")
    params.append(chunk_id)

    with _DB_LOCK:
        with get_connection() as conn:
            conn.execute(
                f"UPDATE document_chunks SET {', '.join(fields)} WHERE id = ?",
                params
            )

    return old_chunk


# ============================================
# Log æ“ä½œ
# ============================================

def insert_log(
    *,
    document_id: int,
    action: str,
    message: Optional[str] = None,
    chunk_id: Optional[int] = None,
    user_id: Optional[str] = None,
    payload: Optional[Dict[str, Any]] = None
) -> int:
    """æ’å…¥æ“ä½œæ—¥å¿—"""
    with _DB_LOCK:
        with get_connection() as conn:
            cursor = conn.execute(
                """
                INSERT INTO document_logs (
                    document_id, chunk_id, action, message, user_id, payload
                ) VALUES (?, ?, ?, ?, ?, ?)
                """,
                (
                    document_id,
                    chunk_id,
                    action,
                    message,
                    user_id,
                    _json_dump(payload)
                )
            )
            return cursor.lastrowid


def get_chunk_logs(chunk_id: int, limit: int = 50) -> List[Dict[str, Any]]:
    """è·å–chunkçš„å˜æ›´å†å²"""
    with get_connection() as conn:
        rows = conn.execute(
            """
            SELECT id, document_id, chunk_id, action, message,
                   user_id, created_at, payload
            FROM document_logs
            WHERE chunk_id = ?
            ORDER BY created_at DESC
            LIMIT ?
            """,
            (chunk_id, limit)
        ).fetchall()

        logs = []
        for row in rows:
            log = dict(row)
            log['payload'] = _json_load(log.get('payload'))
            logs.append(log)

        return logs


def get_document_logs(document_id: int, limit: int = 100) -> List[Dict[str, Any]]:
    """è·å–æ–‡æ¡£çš„æ‰€æœ‰æ—¥å¿—"""
    with get_connection() as conn:
        rows = conn.execute(
            """
            SELECT id, document_id, chunk_id, action, message,
                   user_id, created_at, payload
            FROM document_logs
            WHERE document_id = ?
            ORDER BY created_at DESC
            LIMIT ?
            """,
            (document_id, limit)
        ).fetchall()

        logs = []
        for row in rows:
            log = dict(row)
            log['payload'] = _json_load(log.get('payload'))
            logs.append(log)

        return logs


# ============================================
# å·¥å…·å‡½æ•°
# ============================================

def import_json_to_db(json_file: Path, filename: str) -> int:
    """
    ä»JSONæ–‡ä»¶å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“
    è¿”å›document_id
    """
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    metadata = data.get('metadata', {})
    chunks = data.get('chunks', [])

    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
    existing_doc = get_document_by_filename(filename)
    if existing_doc:
        document_id = existing_doc['id']
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€
        update_document(
            document_id,
            status='completed',
            total_chunks=len(chunks),
            total_tokens=metadata.get('statistics', {}).get('total_tokens', 0),
            processed_at=datetime.utcnow()
        )
    else:
        # åˆ›å»ºæ–°æ–‡æ¡£
        document_id = create_document(
            filename=filename,
            source_path=metadata.get('source_file', ''),
            status='completed'
        )
        update_document(
            document_id,
            total_chunks=len(chunks),
            total_tokens=metadata.get('statistics', {}).get('total_tokens', 0),
            processed_at=datetime.utcnow()
        )

    # åˆ é™¤æ—§chunksï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    with _DB_LOCK:
        with get_connection() as conn:
            conn.execute("DELETE FROM document_chunks WHERE document_id = ?", (document_id,))

    # æ’å…¥æ‰€æœ‰chunks
    for chunk in chunks:
        db_chunk_id = create_chunk(
            document_id=document_id,
            chunk_id=chunk.get('chunk_id'),
            content=chunk.get('content', ''),
            token_start=chunk.get('token_start', 0),
            token_end=chunk.get('token_end', 0),
            token_count=chunk.get('token_count', 0),
            char_start=chunk.get('char_start'),
            char_end=chunk.get('char_end'),
            user_tag=chunk.get('user_tag'),
            content_tags=chunk.get('content_tags', []),
            is_atomic=chunk.get('is_atomic', False),
            atomic_type=chunk.get('atomic_type'),
            status=chunk.get('status', 0)
        )

        # è®°å½•å¯¼å…¥æ—¥å¿—
        insert_log(
            document_id=document_id,
            chunk_id=db_chunk_id,
            action='create',
            message='ä»JSONå¯¼å…¥',
            user_id='system',
            payload={'source': str(json_file)}
        )

    print(f"âœ… Imported {len(chunks)} chunks from {json_file}")
    return document_id


# ============================================
# æ ‡ç­¾ç®¡ç†ç›¸å…³å‡½æ•°
# ============================================

def get_document_tags(document_id: int) -> List[str]:
    """è·å–æ–‡æ¡£çš„æ‰€æœ‰æ ‡ç­¾"""
    with get_connection() as conn:
        rows = conn.execute(
            """
            SELECT tag_text FROM document_tags
            WHERE document_id = ?
            ORDER BY tag_order ASC, id ASC
            """,
            (document_id,)
        ).fetchall()
        return [row['tag_text'] for row in rows]


def add_document_tag(document_id: int, tag_text: str) -> bool:
    """æ·»åŠ æ–‡æ¡£æ ‡ç­¾"""
    tag_text = tag_text.strip()
    if not tag_text:
        return False

    with get_connection() as conn:
        try:
            # è·å–å½“å‰æœ€å¤§çš„ tag_order
            max_order_row = conn.execute(
                "SELECT MAX(tag_order) as max_order FROM document_tags WHERE document_id = ?",
                (document_id,)
            ).fetchone()
            next_order = (max_order_row['max_order'] or 0) + 1

            conn.execute(
                """
                INSERT INTO document_tags (document_id, tag_text, tag_order)
                VALUES (?, ?, ?)
                """,
                (document_id, tag_text, next_order)
            )
            return True
        except sqlite3.IntegrityError:
            # æ ‡ç­¾å·²å­˜åœ¨
            return False


def remove_document_tag(document_id: int, tag_text: str) -> bool:
    """åˆ é™¤æ–‡æ¡£æ ‡ç­¾"""
    with get_connection() as conn:
        cursor = conn.execute(
            "DELETE FROM document_tags WHERE document_id = ? AND tag_text = ?",
            (document_id, tag_text)
        )
        return cursor.rowcount > 0


def get_tags_by_filename(filename: str) -> List[str]:
    """æ ¹æ®æ–‡ä»¶åè·å–æ ‡ç­¾"""
    doc = get_document_by_filename(filename)
    if not doc:
        return []
    return get_document_tags(doc['id'])


def add_tag_by_filename(filename: str, tag_text: str) -> bool:
    """æ ¹æ®æ–‡ä»¶åæ·»åŠ æ ‡ç­¾"""
    doc = get_document_by_filename(filename)
    if not doc:
        return False
    return add_document_tag(doc['id'], tag_text)


def remove_tag_by_filename(filename: str, tag_text: str) -> bool:
    """æ ¹æ®æ–‡ä»¶ååˆ é™¤æ ‡ç­¾"""
    doc = get_document_by_filename(filename)
    if not doc:
        return False
    return remove_document_tag(doc['id'], tag_text)


# ============================================
# å‘é‡åŒ–ç›¸å…³æ“ä½œ
# ============================================

def update_chunk_milvus_id(chunk_id: int, milvus_id: str):
    """
    æ›´æ–° chunk çš„ Milvus ID å’ŒçŠ¶æ€

    Args:
        chunk_id: chunk æ•°æ®åº“ ID
        milvus_id: Milvus å‘é‡ ID
    """
    with _DB_LOCK:
        with get_connection() as conn:
            conn.execute(
                """
                UPDATE document_chunks
                SET milvus_id = ?, status = 2
                WHERE id = ?
                """,
                (milvus_id, chunk_id)
            )


def get_vectorizable_chunks(document_id: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    è·å–å¯å‘é‡åŒ–çš„ chunksï¼ˆstatus != -1 ä¸” status != 2ï¼‰

    Args:
        document_id: å¯é€‰çš„æ–‡æ¡£ IDï¼Œç”¨äºè¿‡æ»¤ç‰¹å®šæ–‡æ¡£

    Returns:
        å¯å‘é‡åŒ–çš„ chunk åˆ—è¡¨
    """
    query = """
        SELECT dc.*, d.filename as source_file
        FROM document_chunks dc
        JOIN documents d ON dc.document_id = d.id
        WHERE dc.status != -1 AND dc.status != 2
    """
    params = []

    if document_id:
        query += " AND dc.document_id = ?"
        params.append(document_id)

    query += " ORDER BY dc.document_id, dc.chunk_id"

    with get_connection() as conn:
        rows = conn.execute(query, params).fetchall()

        chunks = []
        for row in rows:
            chunk = dict(row)
            chunk['content_tags'] = _json_load(chunk.get('content_tags'))
            chunks.append(chunk)

        return chunks


def get_vectorization_stats() -> Dict[str, int]:
    """
    è·å–å‘é‡åŒ–ç»Ÿè®¡ä¿¡æ¯

    Returns:
        ç»Ÿè®¡å­—å…¸: {
            'total': æ€»chunkæ•°,
            'vectorized': å·²å‘é‡åŒ–æ•°é‡,
            'pending': å¾…å‘é‡åŒ–æ•°é‡,
            'deprecated': åºŸå¼ƒæ•°é‡,
            'total_documents': æ€»æ–‡æ¡£æ•°,
            'total_tokens': æ€»tokenæ•°
        }
    """
    with get_connection() as conn:
        # è·å– chunk ç»Ÿè®¡
        stats = conn.execute("""
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN status = 2 THEN 1 ELSE 0 END) as vectorized,
                SUM(CASE WHEN status IN (0, 1) THEN 1 ELSE 0 END) as pending,
                SUM(CASE WHEN status = -1 THEN 1 ELSE 0 END) as deprecated,
                SUM(token_count) as total_tokens
            FROM document_chunks
        """).fetchone()
        
        # è·å–æ–‡æ¡£æ•°
        doc_count = conn.execute("""
            SELECT COUNT(DISTINCT document_id) as total_documents
            FROM document_chunks
        """).fetchone()

        return {
            'total': stats['total'] or 0,
            'vectorized': stats['vectorized'] or 0,
            'pending': stats['pending'] or 0,
            'deprecated': stats['deprecated'] or 0,
            'total_documents': doc_count['total_documents'] or 0,
            'total_tokens': stats['total_tokens'] or 0
        }


def get_chunk_by_milvus_id(milvus_id: str) -> Optional[Dict[str, Any]]:
    """
    æ ¹æ® Milvus ID è·å– chunk

    Args:
        milvus_id: Milvus å‘é‡ ID

    Returns:
        chunk å­—å…¸æˆ– None
    """
    with get_connection() as conn:
        row = conn.execute(
            "SELECT * FROM document_chunks WHERE milvus_id = ?",
            (milvus_id,)
        ).fetchone()

        if not row:
            return None

        chunk = dict(row)
        chunk['content_tags'] = _json_load(chunk.get('content_tags'))
        return chunk


# ============================================
# å…¨å±€æ ‡ç­¾ç®¡ç†
# ============================================

def get_all_tags_with_stats() -> List[Dict[str, Any]]:
    """
    è·å–æ‰€æœ‰æ ‡ç­¾åŠå…¶ç»Ÿè®¡ä¿¡æ¯

    Returns:
        æ ‡ç­¾åˆ—è¡¨ï¼Œæ¯ä¸ªæ ‡ç­¾åŒ…å«ï¼š
        - name: æ ‡ç­¾åç§°
        - type: æ ‡ç­¾ç±»å‹ (user_tag | content_tag | both)
        - count: ä½¿ç”¨æ¬¡æ•°ï¼ˆchunk æ•°é‡ï¼‰
        - chunk_ids: åŒ…å«è¯¥æ ‡ç­¾çš„ chunk IDs
    """
    with get_connection() as conn:
        # æ”¶é›†æ‰€æœ‰ user_tag
        user_tag_rows = conn.execute("""
            SELECT user_tag, GROUP_CONCAT(id) as chunk_ids, COUNT(*) as count
            FROM document_chunks
            WHERE user_tag IS NOT NULL AND user_tag != ''
            GROUP BY user_tag
        """).fetchall()

        # æ”¶é›†æ‰€æœ‰ content_tags
        content_tag_rows = conn.execute("""
            SELECT id, content_tags
            FROM document_chunks
            WHERE content_tags IS NOT NULL AND content_tags != '[]'
        """).fetchall()

    # ç»Ÿè®¡æ ‡ç­¾
    tag_stats = {}

    # å¤„ç† user_tags
    for row in user_tag_rows:
        tag_name = row['user_tag']
        chunk_ids = [int(x) for x in row['chunk_ids'].split(',')]
        tag_stats[tag_name] = {
            'name': tag_name,
            'type': 'user_tag',
            'count': row['count'],
            'chunk_ids': chunk_ids
        }

    # å¤„ç† content_tags
    for row in content_tag_rows:
        chunk_id = row['id']
        tags = _json_load(row['content_tags']) or []

        for tag in tags:
            # ç§»é™¤ @ å‰ç¼€ï¼ˆäººå·¥æ ‡ç­¾ï¼‰
            clean_tag = tag.lstrip('@') if isinstance(tag, str) else tag
            if not clean_tag:
                continue

            if clean_tag in tag_stats:
                # æ ‡ç­¾å·²å­˜åœ¨ï¼ˆå¯èƒ½æ¥è‡ª user_tagï¼‰
                if tag_stats[clean_tag]['type'] == 'user_tag':
                    tag_stats[clean_tag]['type'] = 'both'
                tag_stats[clean_tag]['count'] += 1
                if chunk_id not in tag_stats[clean_tag]['chunk_ids']:
                    tag_stats[clean_tag]['chunk_ids'].append(chunk_id)
            else:
                tag_stats[clean_tag] = {
                    'name': clean_tag,
                    'type': 'content_tag',
                    'count': 1,
                    'chunk_ids': [chunk_id]
                }

    # è¿”å›æ’åºåçš„åˆ—è¡¨ï¼ˆæŒ‰ä½¿ç”¨æ¬¡æ•°é™åºï¼‰
    return sorted(tag_stats.values(), key=lambda x: x['count'], reverse=True)


def delete_tag_from_all_chunks(tag_name: str) -> int:
    """
    ä»æ‰€æœ‰ chunks ä¸­åˆ é™¤æŒ‡å®šæ ‡ç­¾

    Args:
        tag_name: è¦åˆ é™¤çš„æ ‡ç­¾åç§°

    Returns:
        å—å½±å“çš„ chunk æ•°é‡
    """
    affected_count = 0

    with _DB_LOCK:
        with get_connection() as conn:
            # 1. ä» user_tag ä¸­åˆ é™¤
            user_tag_result = conn.execute("""
                UPDATE document_chunks
                SET user_tag = NULL
                WHERE user_tag = ?
            """, (tag_name,))
            affected_count += user_tag_result.rowcount

            # 2. ä» content_tags ä¸­åˆ é™¤
            chunks_with_content_tags = conn.execute("""
                SELECT id, content_tags
                FROM document_chunks
                WHERE content_tags IS NOT NULL AND content_tags != '[]'
            """).fetchall()

            for chunk in chunks_with_content_tags:
                chunk_id = chunk['id']
                tags = _json_load(chunk['content_tags']) or []

                # æ¸…ç†æ ‡ç­¾åï¼ˆç§»é™¤ @ å‰ç¼€ï¼‰å¹¶è¿‡æ»¤
                original_len = len(tags)
                new_tags = [
                    tag for tag in tags
                    if (tag.lstrip('@') if isinstance(tag, str) else tag) != tag_name
                ]

                if len(new_tags) < original_len:
                    # æ ‡ç­¾è¢«åˆ é™¤ï¼Œæ›´æ–°æ•°æ®åº“
                    conn.execute("""
                        UPDATE document_chunks
                        SET content_tags = ?
                        WHERE id = ?
                    """, (_json_dump(new_tags), chunk_id))
                    affected_count += 1

    return affected_count


def rename_tag_in_all_chunks(old_name: str, new_name: str) -> int:
    """
    åœ¨æ‰€æœ‰ chunks ä¸­é‡å‘½åæ ‡ç­¾

    Args:
        old_name: æ—§æ ‡ç­¾å
        new_name: æ–°æ ‡ç­¾å

    Returns:
        å—å½±å“çš„ chunk æ•°é‡
    """
    affected_count = 0

    with _DB_LOCK:
        with get_connection() as conn:
            # 1. é‡å‘½å user_tag
            user_tag_result = conn.execute("""
                UPDATE document_chunks
                SET user_tag = ?
                WHERE user_tag = ?
            """, (new_name, old_name))
            affected_count += user_tag_result.rowcount

            # 2. é‡å‘½å content_tags ä¸­çš„æ ‡ç­¾
            chunks_with_content_tags = conn.execute("""
                SELECT id, content_tags
                FROM document_chunks
                WHERE content_tags IS NOT NULL AND content_tags != '[]'
            """).fetchall()

            for chunk in chunks_with_content_tags:
                chunk_id = chunk['id']
                tags = _json_load(chunk['content_tags']) or []

                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦é‡å‘½åçš„æ ‡ç­¾
                modified = False
                new_tags = []
                for tag in tags:
                    # å¤„ç†å¸¦ @ å‰ç¼€çš„æ ‡ç­¾
                    if isinstance(tag, str):
                        prefix = '@' if tag.startswith('@') else ''
                        clean_tag = tag.lstrip('@')

                        if clean_tag == old_name:
                            new_tags.append(f"{prefix}{new_name}")
                            modified = True
                        else:
                            new_tags.append(tag)
                    else:
                        new_tags.append(tag)

                if modified:
                    conn.execute("""
                        UPDATE document_chunks
                        SET content_tags = ?
                        WHERE id = ?
                    """, (_json_dump(new_tags), chunk_id))
                    affected_count += 1

    return affected_count


def merge_tags_in_all_chunks(source_tags: List[str], target_tag: str) -> Dict[str, int]:
    """
    åˆå¹¶å¤šä¸ªæ ‡ç­¾ä¸ºä¸€ä¸ªæ ‡ç­¾

    Args:
        source_tags: æºæ ‡ç­¾åˆ—è¡¨ï¼ˆè¦è¢«åˆå¹¶çš„æ ‡ç­¾ï¼‰
        target_tag: ç›®æ ‡æ ‡ç­¾ï¼ˆåˆå¹¶åçš„æ ‡ç­¾åï¼‰

    Returns:
        {
            'affected_chunks': å—å½±å“çš„ chunk æ•°é‡,
            'merged_count': è¢«åˆå¹¶çš„æ ‡ç­¾æ•°é‡
        }
    """
    affected_chunks = set()

    with _DB_LOCK:
        with get_connection() as conn:
            # 1. å¤„ç† user_tag
            for source_tag in source_tags:
                chunks = conn.execute("""
                    SELECT id FROM document_chunks
                    WHERE user_tag = ?
                """, (source_tag,)).fetchall()

                if chunks:
                    chunk_ids = [c['id'] for c in chunks]
                    conn.execute(f"""
                        UPDATE document_chunks
                        SET user_tag = ?
                        WHERE id IN ({','.join('?' * len(chunk_ids))})
                    """, [target_tag] + chunk_ids)
                    affected_chunks.update(chunk_ids)

            # 2. å¤„ç† content_tags
            chunks_with_content_tags = conn.execute("""
                SELECT id, content_tags
                FROM document_chunks
                WHERE content_tags IS NOT NULL AND content_tags != '[]'
            """).fetchall()

            for chunk in chunks_with_content_tags:
                chunk_id = chunk['id']
                tags = _json_load(chunk['content_tags']) or []

                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦åˆå¹¶çš„æ ‡ç­¾
                has_source_tags = False
                new_tags = []
                target_added = False

                for tag in tags:
                    if isinstance(tag, str):
                        prefix = '@' if tag.startswith('@') else ''
                        clean_tag = tag.lstrip('@')

                        if clean_tag in source_tags:
                            # é‡åˆ°æºæ ‡ç­¾
                            has_source_tags = True
                            if not target_added:
                                # ç¬¬ä¸€æ¬¡é‡åˆ°æºæ ‡ç­¾æ—¶ï¼Œæ·»åŠ ç›®æ ‡æ ‡ç­¾
                                new_tags.append(f"{prefix}{target_tag}")
                                target_added = True
                            # å¦åˆ™è·³è¿‡ï¼ˆå»é‡ï¼‰
                        elif clean_tag == target_tag:
                            # å·²ç»å­˜åœ¨ç›®æ ‡æ ‡ç­¾ï¼Œæ ‡è®°ä¸ºå·²æ·»åŠ 
                            new_tags.append(tag)
                            target_added = True
                        else:
                            # ä¿ç•™å…¶ä»–æ ‡ç­¾
                            new_tags.append(tag)
                    else:
                        new_tags.append(tag)

                if has_source_tags:
                    # å»é‡
                    final_tags = []
                    seen = set()
                    for tag in new_tags:
                        clean = tag.lstrip('@') if isinstance(tag, str) else tag
                        if clean not in seen:
                            final_tags.append(tag)
                            seen.add(clean)

                    conn.execute("""
                        UPDATE document_chunks
                        SET content_tags = ?
                        WHERE id = ?
                    """, (_json_dump(final_tags), chunk_id))
                    affected_chunks.add(chunk_id)

    return {
        'affected_chunks': len(affected_chunks),
        'merged_count': len(source_tags)
    }


if __name__ == '__main__':
    # æµ‹è¯•ï¼šåˆå§‹åŒ–æ•°æ®åº“
    init_database()
    print("\nğŸ” æµ‹è¯•æ•°æ®åº“æ“ä½œ:")

    # æµ‹è¯•ï¼šæŸ¥è¯¢ç¤ºä¾‹æ–‡æ¡£
    doc = get_document_by_filename('example.md')
    if doc:
        print(f"  æ–‡æ¡£: {doc['filename']}, çŠ¶æ€: {doc['status']}, Chunks: {doc['total_chunks']}")

        # æŸ¥è¯¢chunks
        chunks = get_chunks_by_document(doc['id'])
        print(f"  å…±æœ‰ {len(chunks)} ä¸ªåˆ‡ç‰‡")
        for chunk in chunks[:2]:
            print(f"    - Chunk #{chunk['chunk_id']}: {chunk['content'][:30]}...")
