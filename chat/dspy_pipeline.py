"""
DSPy RAG Pipeline å®ç°
æ•´åˆæ„å›¾è¯†åˆ«ã€æŸ¥è¯¢æ”¹å†™ã€æ£€ç´¢ã€è¯„ä¼°å’Œå›å¤ç”Ÿæˆ
"""

import dspy
import json
import os
import requests
import asyncio
import uuid
from loguru import logger
from typing import List, Dict, Any, Optional, Tuple, AsyncGenerator

from .dspy_signatures import (
    IntentClassification,
    QueryRewrite,
    TagIdentification,
    ConfidenceEvaluation,
    ClarificationGeneration,
    ResponseGeneration
)
from .memory_cache import ConversationMemoryCache


class DSPyRAGPipeline:
    """DSPy RAG Pipeline ä¸»ç±»"""

    def __init__(
        self,
        vector_store=None,
        llm_model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        confidence_threshold: float = None
    ):
        """
        åˆå§‹åŒ– DSPy Pipeline

        Args:
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆMilvusï¼‰
            llm_model: LLM æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        self.vector_store = vector_store

        # è¯»å–é—²èŠæ¨¡å¼é…ç½®
        self.enable_chat_mode = os.getenv('ENABLE_CHAT_MODE', 'false').lower() == 'true'
        self.chat_mode_threshold = float(os.getenv('CHAT_MODE_THRESHOLD', '0.7'))

        # è¯»å–è‡ªåŠ¨æ ‡ç­¾è¯†åˆ«é…ç½®
        self.enable_auto_tag_filter = os.getenv('ENABLE_AUTO_TAG_FILTER', 'true').lower() == 'true'
        self.auto_tag_filter_threshold = float(os.getenv('AUTO_TAG_FILTER_THRESHOLD', '0.5'))

        # è¯»å– RAG Pipeline é˜ˆå€¼é…ç½®
        self.confidence_threshold = confidence_threshold or float(os.getenv('RAG_CONFIDENCE_THRESHOLD', '0.5'))

        # ç›¸å…³æ€§é˜ˆå€¼
        self.rerank_score_threshold = float(os.getenv('RAG_RERANK_SCORE_THRESHOLD', '0.2'))
        self.l2_distance_threshold = float(os.getenv('RAG_L2_DISTANCE_THRESHOLD', '1.2'))

        # æ£€ç´¢è´¨é‡é˜ˆå€¼
        self.rerank_good_threshold = float(os.getenv('RAG_RERANK_GOOD_THRESHOLD', '0.2'))
        self.rerank_excellent_threshold = float(os.getenv('RAG_RERANK_EXCELLENT_THRESHOLD', '0.3'))
        self.l2_good_threshold = float(os.getenv('RAG_L2_GOOD_THRESHOLD', '1.2'))
        self.l2_excellent_threshold = float(os.getenv('RAG_L2_EXCELLENT_THRESHOLD', '1.0'))

        # æ£€ç´¢æ•°é‡é…ç½®
        self.entity_top_k = int(os.getenv('RAG_ENTITY_TOP_K', '3'))
        self.multi_entity_dedup_limit = int(os.getenv('RAG_MULTI_ENTITY_DEDUP_LIMIT', '15'))
        self.rerank_top_n = int(os.getenv('RAG_RERANK_TOP_N', '8'))
        self.single_query_top_k = int(os.getenv('RAG_SINGLE_QUERY_TOP_K', '8'))
        self.files_display_limit = int(os.getenv('RAG_FILES_DISPLAY_LIMIT', '5'))

        # é…ç½® DSPy LLM
        self._configure_dspy(llm_model, temperature)

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.intent_classifier = dspy.ChainOfThought(IntentClassification)
        self.query_rewriter = dspy.ChainOfThought(QueryRewrite)
        self.tag_identifier = dspy.ChainOfThought(TagIdentification)
        self.confidence_evaluator = dspy.ChainOfThought(ConfidenceEvaluation)
        self.clarification_generator = dspy.ChainOfThought(ClarificationGeneration)
        self.response_generator = dspy.ChainOfThought(ResponseGeneration)
        memory_window = int(os.getenv('CHAT_MEMORY_MAX_ITEMS', '3'))
        self.memory_cache = ConversationMemoryCache(max_items=memory_window)

        # ç¼“å­˜æ ‡ç­¾åˆ—è¡¨ï¼ˆé¿å…æ¯æ¬¡æŸ¥è¯¢éƒ½è¯»æ•°æ®åº“ï¼‰
        self._available_tags_cache = None
        self._tags_cache_time = 0

        logger.info(f"âœ… DSPy RAG Pipeline initialized with model: {llm_model}")
        logger.info(f"   é—²èŠæ¨¡å¼: {'å¼€å¯' if self.enable_chat_mode else 'å…³é—­'} (ç½®ä¿¡åº¦é˜ˆå€¼: {self.chat_mode_threshold})")
        logger.info(f"   è‡ªåŠ¨æ ‡ç­¾ç­›é€‰: {'å¼€å¯' if self.enable_auto_tag_filter else 'å…³é—­'} (ç½®ä¿¡åº¦é˜ˆå€¼: {self.auto_tag_filter_threshold})")
        logger.info(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold}")
        logger.info(f"   ç›¸å…³æ€§é˜ˆå€¼: Rerank={self.rerank_score_threshold}, L2={self.l2_distance_threshold}")
        logger.info(f"   æ£€ç´¢æ•°é‡: å•å®ä½“={self.entity_top_k}, å»é‡={self.multi_entity_dedup_limit}, é‡æ’={self.rerank_top_n}")
        logger.info(f"   å¯¹è¯è®°å¿†ç¼“å­˜çª—å£: {memory_window}")

    @staticmethod
    def _safe_parse_confidence(value: Any, default: float = 0.5) -> float:
        """
        å®‰å…¨åœ°è§£æç½®ä¿¡åº¦å€¼ï¼Œå®¹é”™å¤„ç†å„ç§æ ¼å¼

        Args:
            value: å¾…è§£æçš„å€¼ï¼ˆå¯èƒ½æ˜¯æ•°å­—ã€å­—ç¬¦ä¸²ã€æˆ–åŒ…å«æ•°å­—çš„æ–‡æœ¬ï¼‰
            default: è§£æå¤±è´¥æ—¶çš„é»˜è®¤å€¼

        Returns:
            è§£æåçš„æµ®ç‚¹æ•°
        """
        try:
            # å¦‚æœå·²ç»æ˜¯æ•°å­—ç±»å‹ï¼Œç›´æ¥è¿”å›
            if isinstance(value, (int, float)):
                return float(value)

            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
            str_value = str(value).strip()

            # å°è¯•ç›´æ¥è½¬æ¢
            try:
                return float(str_value)
            except ValueError:
                pass

            # å°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—ï¼ˆå¦‚ "0.75" æˆ– "High - 0.75" æˆ– "ç½®ä¿¡åº¦: 0.75"ï¼‰
            import re
            # åŒ¹é… 0.0 åˆ° 1.0 èŒƒå›´çš„å°æ•°
            match = re.search(r'\b([0-1]?\.\d+|0|1)\b', str_value)
            if match:
                confidence = float(match.group(1))
                # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
                if 0.0 <= confidence <= 1.0:
                    logger.debug(f"ä»æ–‡æœ¬ '{str_value[:50]}...' ä¸­æå–ç½®ä¿¡åº¦: {confidence}")
                    return confidence

            # å°è¯•æ˜ å°„æ–‡æœ¬æè¿°åˆ°æ•°å€¼
            str_lower = str_value.lower()
            if 'high' in str_lower or 'é«˜' in str_lower:
                return 0.8
            elif 'medium' in str_lower or 'ä¸­' in str_lower:
                return 0.5
            elif 'low' in str_lower or 'ä½' in str_lower:
                return 0.3

            logger.warning(f"æ— æ³•è§£æç½®ä¿¡åº¦å€¼: '{str_value[:100]}', ä½¿ç”¨é»˜è®¤å€¼ {default}")
            return default

        except Exception as e:
            logger.warning(f"è§£æç½®ä¿¡åº¦æ—¶å‡ºé”™: {e}, ä½¿ç”¨é»˜è®¤å€¼ {default}")
            return default

    @staticmethod
    def _build_history_chunk(conversation_history: str) -> Optional[Dict[str, Any]]:
        """å°†çº¯æ–‡æœ¬å¯¹è¯å†å²åŒ…è£…æˆæ£€ç´¢ç‰‡æ®µï¼Œç”¨äºå›é€€"""
        cleaned = (conversation_history or "").strip()
        if not cleaned:
            return None

        max_length = 2000
        if len(cleaned) > max_length:
            cleaned = cleaned[-max_length:]

        return {
            "chunk_id": f"history-{uuid.uuid4()}",
            "content": cleaned,
            "document": "conversation_history",
            "score": 0.0,
            "metadata": {
                "type": "conversation_history",
                "source": "conversation_manager"
            }
        }

    def _configure_dspy(self, model: str, temperature: float):
        """é…ç½® DSPy çš„ LLM"""
        try:
            import os

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Azure OpenAI
            llm_provider = os.getenv('LLM_PROVIDER', 'openai')

            if llm_provider == 'azure':
                # Azure OpenAI é…ç½®
                azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
                azure_key = os.getenv('AZURE_OPENAI_API_KEY')
                azure_deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4o')
                azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')

                # ä½¿ç”¨ azure/ å‰ç¼€æ ¼å¼
                model_name = f"azure/{azure_deployment}"

                lm = dspy.LM(
                    model=model_name,
                    api_base=azure_endpoint,
                    api_key=azure_key,
                    api_version=azure_api_version,
                    temperature=temperature,
                    max_tokens=2000
                )
                logger.info(f"âœ… DSPy configured with Azure OpenAI: {azure_deployment}")

            elif llm_provider == 'dashscope':
                # DashScope é…ç½®ï¼ˆé˜¿é‡Œäº‘çµç§¯ï¼‰
                # ä½¿ç”¨ OpenAI å…¼å®¹æ¨¡å¼ï¼Œé€šè¿‡ api_base æŒ‡å®š DashScope ç«¯ç‚¹
                dashscope_key = os.getenv('DASHSCOPE_API_KEY')
                dashscope_model = os.getenv('DASHSCOPE_MODEL', 'qwen-max')
                dashscope_endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1"

                # ä½¿ç”¨ openai/ å‰ç¼€ + api_base æ¥è°ƒç”¨ OpenAI å…¼å®¹ç«¯ç‚¹
                model_name = f"openai/{dashscope_model}"

                lm = dspy.LM(
                    model=model_name,
                    api_key=dashscope_key,
                    api_base=dashscope_endpoint,
                    temperature=temperature,
                    max_tokens=2000
                )
                logger.info(f"âœ… DSPy configured with DashScope (OpenAI-compatible): {dashscope_model}")

            else:
                # æ ‡å‡† OpenAI é…ç½®
                api_base = os.getenv('OPENAI_API_BASE') or os.getenv('API_BASE')
                api_key = os.getenv('OPENAI_API_KEY') or os.getenv('API_KEY')
                model = os.getenv('OPENAI_MODEL') or os.getenv('MODEL_NAME', 'gpt-4o')

                # å¦‚æœæ¨¡å‹åä¸æ˜¯ä»¥ openai/ å¼€å¤´ï¼Œæ·»åŠ å‰ç¼€ä»¥é¿å…è¢« LiteLLM è¯¯è¯†åˆ«
                # ä¾‹å¦‚ claude-sonnet-4-20250514 ä¼šè¢«è¯†åˆ«ä¸º Anthropicï¼Œéœ€è¦åŠ  openai/ å‰ç¼€
                if not model.startswith('openai/'):
                    model = f'openai/{model}'
                    logger.info(f"ğŸ”„ æ·»åŠ  openai/ å‰ç¼€ä»¥é¿å…æ¨¡å‹åè¢«è¯¯è¯†åˆ«: {model}")

                lm = dspy.LM(model=model, api_base=api_base, api_key=api_key, temperature=temperature, max_tokens=2000)

                if api_base:
                    logger.info(f"âœ… DSPy configured with OpenAI-compatible endpoint: {model} @ {api_base}")
                else:
                    logger.info(f"âœ… DSPy configured with OpenAI: {model}")

            dspy.configure(lm=lm)

        except Exception as e:
            logger.error(f"âŒ Failed to configure DSPy: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def classify_intent(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤1: è¯†åˆ«ç”¨æˆ·æ„å›¾

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ„å›¾åˆ†ç±»ç»“æœ
        """
        try:
            result = self.intent_classifier(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query
            )

            return {
                "intent": result.intent.lower(),
                "confidence": self._safe_parse_confidence(result.confidence),
                "business_relevance": result.business_relevance.lower(),
                "reasoning": result.reasoning
            }
        except Exception as e:
            logger.error(f"âŒ Intent classification failed: {e}")
            # é»˜è®¤è®¤ä¸ºæ˜¯é—®ç­”
            return {
                "intent": "question",
                "confidence": 0.5,
                "business_relevance": "medium",
                "reasoning": f"åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤ä¸ºé—®ç­”: {e}"
            }

    def rewrite_query(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤2: æ”¹å†™æŸ¥è¯¢

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ”¹å†™ç»“æœ
        """
        try:
            result = self.query_rewriter(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query
            )

            # è§£æ key_entities
            try:
                key_entities = json.loads(result.key_entities)
            except:
                key_entities = [result.key_entities]

            return {
                "rewritten_query": result.rewritten_query,
                "key_entities": key_entities,
                "search_strategy": result.search_strategy
            }
        except Exception as e:
            logger.error(f"âŒ Query rewrite failed: {e}")
            return {
                "rewritten_query": user_query,
                "key_entities": [],
                "search_strategy": "semantic"
            }

    def _get_available_tags(self) -> List[str]:
        """
        è·å–ç³»ç»Ÿå¯ç”¨æ ‡ç­¾åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰
        ç¼“å­˜5åˆ†é’Ÿï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
        """
        import time
        current_time = time.time()

        # ç¼“å­˜5åˆ†é’Ÿ
        if self._available_tags_cache and (current_time - self._tags_cache_time) < 300:
            return self._available_tags_cache

        try:
            # ä»æ•°æ®åº“è·å–æ ‡ç­¾
            import sys
            from pathlib import Path
            sys.path.append(str(Path(__file__).parent.parent))
            from database import get_connection

            with get_connection() as conn:
                # è·å– user_tags
                user_tags = conn.execute("""
                    SELECT DISTINCT user_tag FROM document_chunks
                    WHERE user_tag IS NOT NULL AND user_tag != ''
                """).fetchall()

                # è·å– content_tags
                content_tags_rows = conn.execute("""
                    SELECT DISTINCT content_tags FROM document_chunks
                    WHERE content_tags IS NOT NULL AND content_tags != '[]'
                """).fetchall()

                # è·å–æ–‡æ¡£çº§æ ‡ç­¾
                doc_tags = conn.execute("""
                    SELECT DISTINCT tag_text FROM document_tags
                """).fetchall()

            tags = set()

            # å¤„ç† user_tags
            for row in user_tags:
                if row['user_tag']:
                    tags.add(row['user_tag'].lstrip('@'))

            # å¤„ç† content_tags
            for row in content_tags_rows:
                try:
                    tag_list = json.loads(row['content_tags'])
                    if isinstance(tag_list, list):
                        for tag in tag_list:
                            clean_tag = tag.lstrip('@') if isinstance(tag, str) else tag
                            if clean_tag:
                                tags.add(clean_tag)
                except:
                    continue

            # å¤„ç†æ–‡æ¡£çº§æ ‡ç­¾
            for row in doc_tags:
                if row['tag_text']:
                    tags.add(row['tag_text'].strip().lstrip('@'))

            tag_list = sorted(list(tags))
            self._available_tags_cache = tag_list
            self._tags_cache_time = current_time

            logger.debug(f"Loaded {len(tag_list)} available tags")
            return tag_list

        except Exception as e:
            logger.error(f"Failed to load tags: {e}")
            return []

    def identify_relevant_tags(self, user_query: str) -> Dict[str, Any]:
        """
        è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢ä¸­çš„ç›¸å…³æ ‡ç­¾

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æ ‡ç­¾è¯†åˆ«ç»“æœ
        """
        try:
            available_tags = self._get_available_tags()

            if not available_tags:
                logger.warning("No available tags found")
                return {
                    "relevant_tags": [],
                    "confidence": 0.0,
                    "reasoning": "ç³»ç»Ÿæ ‡ç­¾åº“ä¸ºç©º"
                }

            # è°ƒç”¨ DSPy æ ‡ç­¾è¯†åˆ«
            tags_json = json.dumps(available_tags, ensure_ascii=False)

            result = self.tag_identifier(
                user_query=user_query,
                available_tags=tags_json
            )

            # è§£æ relevant_tags
            try:
                relevant_tags = json.loads(result.relevant_tags)
                if not isinstance(relevant_tags, list):
                    relevant_tags = []
            except:
                relevant_tags = []

            return {
                "relevant_tags": relevant_tags,
                "confidence": self._safe_parse_confidence(result.confidence),
                "reasoning": result.reasoning
            }

        except Exception as e:
            logger.error(f"âŒ Tag identification failed: {e}")
            return {
                "relevant_tags": [],
                "confidence": 0.0,
                "reasoning": f"æ ‡ç­¾è¯†åˆ«å¤±è´¥: {e}"
            }

    def retrieve_chunks(
        self,
        query: str,
        top_k: int = 5,
        filters: Dict = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3: å‘é‡æ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not self.vector_store:
            logger.warning("âš ï¸  No vector store configured")
            return []

        try:
            # ä½¿ç”¨ search_with_score æ–¹æ³•ï¼ˆè¿”å›å¸¦åˆ†æ•°çš„ç»“æœï¼‰
            results = self.vector_store.search_with_score(
                query=query,
                k=top_k,
                filters=filters
            )

            # æ ¼å¼åŒ–ç»“æœ - search_with_score è¿”å› [(Document, score), ...]
            # Document æ˜¯ LangChain Documentï¼Œæœ‰ .page_content å’Œ .metadata
            chunks = []
            for doc, score in results:
                metadata = doc.metadata
                chunks.append({
                    "chunk_db_id": metadata.get("chunk_db_id") or metadata.get("pk"),  # æ•°æ®åº“ä¸»é”®ID
                    "chunk_sequence": metadata.get("chunk_sequence", 0),  # æ–‡æ¡£å†…é¡ºåºç¼–å·
                    "content": metadata.get("original_content") or doc.page_content,
                    "score": score,
                    "document": metadata.get("source_file", "Unknown"),
                    "metadata": metadata
                })

            logger.info(f"âœ… Retrieved {len(chunks)} chunks")
            return chunks

        except Exception as e:
            logger.error(f"âŒ Retrieval failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡å¹¶æŒ‰åˆ†æ•°æ’åºï¼ˆL2è·ç¦»è¶Šå°è¶Šå¥½ï¼‰

        Args:
            chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨

        Returns:
            å»é‡åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°å‡åºæ’åº
        """
        seen_ids = set()
        unique_chunks = []

        for chunk in chunks:
            chunk_id = chunk.get('chunk_id')
            if chunk_id and chunk_id not in seen_ids:
                seen_ids.add(chunk_id)
                unique_chunks.append(chunk)
            elif not chunk_id:
                # å¦‚æœæ²¡æœ‰chunk_idï¼Œç”¨contentçš„hashä½œä¸ºæ ‡è¯†
                content_hash = hash(chunk.get('content', ''))
                if content_hash not in seen_ids:
                    seen_ids.add(content_hash)
                    unique_chunks.append(chunk)

        # æŒ‰L2è·ç¦»å‡åºæ’åºï¼ˆåˆ†æ•°è¶Šå°è¶Šå¥½ï¼‰
        unique_chunks.sort(key=lambda x: x.get('score', float('inf')))
        return unique_chunks

    def rerank_chunks(
        self,
        query: str,
        chunks: List[Dict[str, Any]],
        top_n: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ DashScope é‡æ’åºæ£€ç´¢ç»“æœ

        Args:
            query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªç»“æœ

        Returns:
            é‡æ’åºåçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not chunks:
            return chunks

        try:
            api_key = os.getenv("DASHSCOPE_API_KEY")
            if not api_key:
                logger.warning("âš ï¸  DASHSCOPE_API_KEY not found, skipping rerank")
                return chunks[:top_n]

            # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
            documents = [chunk.get('content', '') for chunk in chunks]

            # è°ƒç”¨ DashScope rerank API
            url = "https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank"
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "gte-rerank-v2",
                "input": {
                    "query": query,
                    "documents": documents
                },
                "parameters": {
                    "return_documents": True,
                    "top_n": top_n
                }
            }

            response = requests.post(url, headers=headers, json=payload, timeout=10)
            response.raise_for_status()

            result = response.json()

            # è§£æé‡æ’åºç»“æœ - DashScope æˆåŠŸæ—¶è¿”å› output å­—æ®µ
            if "output" in result and "results" in result["output"]:
                reranked_results = result["output"]["results"]
                reranked_chunks = []

                for item in reranked_results:
                    original_index = item.get("index")
                    relevance_score = item.get("relevance_score")

                    if original_index is not None and original_index < len(chunks):
                        chunk = chunks[original_index].copy()
                        chunk['rerank_score'] = relevance_score
                        reranked_chunks.append(chunk)

                logger.info(f"âœ… Reranked {len(reranked_chunks)} chunks (scores: {[round(c.get('rerank_score', 0), 3) for c in reranked_chunks[:3]]}...)")
                return reranked_chunks
            else:
                logger.warning(f"âš ï¸  Rerank API returned unexpected format: {result}")
                return chunks[:top_n]

        except Exception as e:
            logger.error(f"âŒ Rerank failed: {e}")
            # é™çº§ï¼šè¿”å›åŸå§‹æ’åºç»“æœ
            return chunks[:top_n]

    def evaluate_confidence(
        self,
        user_query: str,
        retrieved_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤4: è¯„ä¼°æ£€ç´¢ç»“æœçš„å……åˆ†æ€§

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ

        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            # æ ¼å¼åŒ– chunks ä¸ºæ–‡æœ¬
            chunks_text = json.dumps(
                [{"content": c["content"], "source": c.get("document", "")}
                 for c in retrieved_chunks[:5]],  # åªå–å‰3ä¸ªé¿å…å¤ªé•¿
                ensure_ascii=False,
                indent=2
            )

            result = self.confidence_evaluator(
                user_query=user_query,
                retrieved_chunks=chunks_text
            )

            return {
                "is_sufficient": result.is_sufficient.lower() == "yes",
                "has_ambiguity": result.has_ambiguity.lower() == "yes",
                "confidence": self._safe_parse_confidence(result.confidence),
                "ambiguity_type": result.ambiguity_type,
                "clarification_hint": result.clarification_hint,
                "reasoning": result.reasoning
            }
        except Exception as e:
            logger.error(f"âŒ Confidence evaluation failed: {e}")
            # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œä¿å®ˆåœ°è®¤ä¸ºä¿¡æ¯å……åˆ†ï¼ˆé¿å…è¿‡åº¦åé—®ï¼‰
            return {
                "is_sufficient": True,
                "has_ambiguity": False,
                "confidence": 0.5,
                "ambiguity_type": "none",
                "clarification_hint": "",
                "reasoning": f"è¯„ä¼°å¤±è´¥: {e}"
            }

    def generate_clarification(
        self,
        user_query: str,
        missing_info: List[str],
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤5: ç”Ÿæˆæ¾„æ¸…é—®é¢˜

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            missing_info: ç¼ºå¤±çš„ä¿¡æ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ¾„æ¸…é—®é¢˜
        """
        try:
            missing_info_text = json.dumps(missing_info, ensure_ascii=False)

            result = self.clarification_generator(
                user_query=user_query,
                missing_info=missing_info_text,
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯"
            )

            # è§£æé€‰é¡¹
            try:
                options = json.loads(result.suggested_options)
            except:
                options = []

            return {
                "question": result.clarification_question,
                "options": options
            }
        except Exception as e:
            logger.error(f"âŒ Clarification generation failed: {e}")
            return {
                "question": f"æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚æ‚¨èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹ {missing_info[0] if missing_info else 'æ‚¨çš„éœ€æ±‚'} å—ï¼Ÿ",
                "options": []
            }

    def generate_chitchat_response(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> str:
        """
        ç”Ÿæˆé—²èŠå›å¤ï¼ˆä½¿ç”¨ LLMï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            ç”Ÿæˆçš„é—²èŠå›å¤
        """
        try:
            # ä½¿ç”¨ dspy.LM ç›´æ¥ç”Ÿæˆå›å¤
            lm = dspy.settings.lm

            system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„æ–‡æ¡£åŠ©æ‰‹ã€‚å½“ç”¨æˆ·è¿›è¡Œé—²èŠæ—¶ï¼Œè¯·ï¼š
1. ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„æ€åº¦
2. ç®€çŸ­å›å¤ï¼Œä¸è¦è¿‡äºå†—é•¿
3. é€‚å½“å¼•å¯¼ç”¨æˆ·æå‡ºæ–‡æ¡£ç›¸å…³é—®é¢˜
4. å¦‚æœç”¨æˆ·é—®å€™ï¼Œå¯ä»¥ä»‹ç»ä½ çš„åŠŸèƒ½"""

            # æ„å»ºå®Œæ•´çš„æç¤ºæ–‡æœ¬
            full_prompt = system_prompt + "\n\n"

            # æ·»åŠ å¯¹è¯å†å²
            if conversation_history:
                full_prompt += f"å¯¹è¯å†å²:\n{conversation_history}\n\n"

            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            full_prompt += f"ç”¨æˆ·: {user_query}\nåŠ©æ‰‹:"

            # è°ƒç”¨ LLMï¼ˆç›´æ¥ä¼ å…¥æ–‡æœ¬è€Œä¸æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼‰
            response = lm(full_prompt)

            # æå–å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæ˜¯åˆ—è¡¨åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰
            if isinstance(response, list) and len(response) > 0:
                response_text = response[0]
            elif isinstance(response, str):
                response_text = response
            else:
                response_text = str(response)

            return response_text

        except Exception as e:
            logger.error(f"âŒ Chitchat response generation failed: {e}")
            # é™çº§åˆ°é»˜è®¤å›å¤
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¡£åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ‚¨æŸ¥æ‰¾å’Œç†è§£æ–‡æ¡£å†…å®¹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"

    def generate_response(
        self,
        user_query: str,
        retrieved_chunks: List[Dict[str, Any]],
        conversation_history: str = "",
        clarification_hint: str = "",
        intent_note: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆå›å¤

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
            conversation_history: å¯¹è¯å†å²
            clarification_hint: å¯é€‰çš„æ¾„æ¸…æç¤ºï¼ˆç”¨äº"å…ˆç­”åé—®"ï¼‰
            intent_note: æ„å›¾è¯†åˆ«å¤‡æ³¨

        Returns:
            ç”Ÿæˆçš„å›å¤
        """
        try:
            # æ ¼å¼åŒ– chunks
            chunks_text = json.dumps(
                [{
                    "id": c.get("chunk_id"),
                    "content": c["content"],
                    "source": c.get("document", ""),
                    "score": c.get("score", 0.0)
                } for c in retrieved_chunks],
                ensure_ascii=False,
                indent=2
            )

            result = self.response_generator(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query,
                retrieved_chunks=chunks_text,
                clarification_hint=clarification_hint,
                intent_note=intent_note
            )

            # è§£æ source_ids
            try:
                source_ids = json.loads(result.source_ids)
            except:
                source_ids = [c.get("chunk_id") for c in retrieved_chunks[:3]]

            return {
                "response": result.response,
                "source_ids": source_ids,
                "confidence": self._safe_parse_confidence(result.confidence),
                "sources": retrieved_chunks
            }
        except Exception as e:
            logger.error(f"âŒ Response generation failed: {e}")
            return {
                "response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚",
                "source_ids": [],
                "confidence": 0.0,
                "sources": []
            }

    def process_query(
        self,
        user_query: str,
        conversation_history: str = "",
        filters: Dict = None,
        force_answer: bool = False,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä¸»å…¥å£ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²
            filters: æ£€ç´¢è¿‡æ»¤æ¡ä»¶
            force_answer: æ˜¯å¦å¼ºåˆ¶å›ç­”ï¼ˆä¸ç”Ÿæˆæ¾„æ¸…é—®é¢˜ï¼‰
            session_id: ä¼šè¯IDï¼Œç”¨äºè®°å¿†ç¼“å­˜

        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸ” Processing query: {user_query[:100]}...")

        # 1. æ„å›¾è¯†åˆ«
        intent_result = self.classify_intent(user_query, conversation_history)
        logger.info(f"  Intent: {intent_result['intent']} (confidence: {intent_result['confidence']})")

        # å¦‚æœæ˜¯é—²èŠä¸”ç½®ä¿¡åº¦é«˜ï¼Œæ ¹æ®é…ç½®å†³å®šå›å¤æ–¹å¼
        if intent_result['intent'] == 'chitchat' and intent_result['confidence'] >= self.chat_mode_threshold:
            # æ£€æŸ¥æ˜¯å¦å¼€å¯é—²èŠæ¨¡å¼
            if self.enable_chat_mode:
                logger.info(f"  é—²èŠæ¨¡å¼å·²å¼€å¯ï¼Œç½®ä¿¡åº¦ {intent_result['confidence']:.2f} >= {self.chat_mode_threshold}ï¼Œä½¿ç”¨ LLM å›å¤")
                llm_response = self.generate_chitchat_response(user_query, conversation_history)
                # åœ¨å›å¤æœ«å°¾æ·»åŠ æ˜æ˜¾çš„ AI ç”Ÿæˆæé†’
                llm_response_with_notice = f"{llm_response}\n\n---\nâš ï¸ **æç¤º**ï¼šæ­¤å›å¤ç”± AI ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œè¯·æ³¨æ„ç”„åˆ«ã€‚"
                self.memory_cache.add_exchange(session_id, user_query, llm_response_with_notice)
                return {
                    "type": "chitchat",
                    "response": llm_response_with_notice,
                    "intent": intent_result,
                    "chat_mode": "llm"
                }
            else:
                # æœªå¼€å¯é—²èŠæ¨¡å¼ï¼Œä½¿ç”¨å›ºå®šå›å¤
                logger.info(f"  é—²èŠæ¨¡å¼æœªå¼€å¯ï¼Œä½¿ç”¨å›ºå®šå›å¤")
                fixed_reply = "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¡£åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ‚¨æŸ¥æ‰¾å’Œç†è§£æ–‡æ¡£å†…å®¹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"
                self.memory_cache.add_exchange(session_id, user_query, fixed_reply)
                return {
                    "type": "chitchat",
                    "response": fixed_reply,
                    "intent": intent_result,
                    "chat_mode": "fixed"
                }

        # ç½®ä¿¡åº¦ä¸è¶³æˆ–éé—²èŠæ„å›¾ï¼Œç»§ç»­è¿›è¡ŒçŸ¥è¯†åº“æ£€ç´¢
        intent_note = ""
        if intent_result['intent'] == 'chitchat' and intent_result['confidence'] < self.chat_mode_threshold:
            logger.info(f"  é—²èŠæ„å›¾ç½®ä¿¡åº¦ä¸è¶³ ({intent_result['confidence']:.2f} < {self.chat_mode_threshold})ï¼Œç»§ç»­æ£€ç´¢çŸ¥è¯†åº“")
            intent_note = "æ£€æµ‹åˆ°æ‚¨çš„é—®é¢˜å¯èƒ½åå‘é—²èŠï¼Œä½†æˆ‘å°è¯•åœ¨çŸ¥è¯†åº“ä¸­ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³å†…å®¹"

        # 2. å¹¶è¡Œæ‰§è¡Œï¼šæŸ¥è¯¢æ”¹å†™ + æ ‡ç­¾è¯†åˆ«ï¼ˆå¦‚æœå¼€å¯ï¼‰
        if self.enable_auto_tag_filter:
            logger.info("  ğŸ”„ Parallel processing: Query rewrite + Tag identification...")
        else:
            logger.info("  ğŸ”„ Query rewrite...")

        import concurrent.futures
        import threading

        rewrite_result = None
        tag_result = None
        rewrite_error = None
        tag_error = None

        def do_rewrite():
            nonlocal rewrite_result, rewrite_error
            try:
                rewrite_result = self.rewrite_query(user_query, conversation_history)
            except Exception as e:
                rewrite_error = e

        def do_tag_identification():
            nonlocal tag_result, tag_error
            try:
                tag_result = self.identify_relevant_tags(user_query)
            except Exception as e:
                tag_error = e

        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¹¶è¡Œæ‰§è¡Œæ ‡ç­¾è¯†åˆ«
        if self.enable_auto_tag_filter:
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œ
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                future_rewrite = executor.submit(do_rewrite)
                future_tags = executor.submit(do_tag_identification)

                # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
                concurrent.futures.wait([future_rewrite, future_tags])
        else:
            # åªæ‰§è¡ŒæŸ¥è¯¢æ”¹å†™
            do_rewrite()
            tag_result = {
                "relevant_tags": [],
                "confidence": 0.0,
                "reasoning": "è‡ªåŠ¨æ ‡ç­¾è¯†åˆ«å·²å…³é—­"
            }

        # å¤„ç†æŸ¥è¯¢æ”¹å†™ç»“æœ
        if rewrite_error:
            logger.error(f"âŒ Query rewrite failed: {rewrite_error}")
            rewrite_result = {
                "rewritten_query": user_query,
                "key_entities": [],
                "search_strategy": "semantic"
            }

        logger.info(f"  âœ… Rewritten query: {rewrite_result['rewritten_query'][:100]}...")
        optimized_query = rewrite_result['rewritten_query']

        # å¤„ç†æ ‡ç­¾è¯†åˆ«ç»“æœï¼ˆä»…å½“å¼€å¯è‡ªåŠ¨æ ‡ç­¾è¯†åˆ«æ—¶ï¼‰
        if self.enable_auto_tag_filter:
            if tag_error:
                logger.error(f"âŒ Tag identification failed: {tag_error}")
                tag_result = {
                    "relevant_tags": [],
                    "confidence": 0.0,
                    "reasoning": f"æ ‡ç­¾è¯†åˆ«å¤±è´¥: {tag_error}"
                }

            relevant_tags = tag_result.get('relevant_tags', [])
            tag_confidence = tag_result.get('confidence', 0.0)

            # å¦‚æœè¯†åˆ«åˆ°æ ‡ç­¾ä¸”ç½®ä¿¡åº¦è¶³å¤Ÿï¼Œæ·»åŠ åˆ° filters
            if relevant_tags and tag_confidence >= self.auto_tag_filter_threshold:
                logger.info(f"  ğŸ·ï¸  Identified tags: {relevant_tags} (confidence: {tag_confidence:.2f})")
                if not filters:
                    filters = {}
                # ä½¿ç”¨ content_tags ç­›é€‰ï¼ˆå› ä¸ºå®ƒä¼šåŒæ—¶åŒ¹é… user_tag å’Œ content_tagsï¼‰
                filters['content_tags'] = relevant_tags
            elif relevant_tags:
                logger.info(f"  ğŸ·ï¸  Tags identified but low confidence: {relevant_tags} (confidence: {tag_confidence:.2f} < {self.auto_tag_filter_threshold}), not using for filtering")
            else:
                logger.info(f"  ğŸ·ï¸  No relevant tags identified")
        else:
            logger.debug("  ğŸ·ï¸  Auto tag filter is disabled")

        # 3. å‘é‡æ£€ç´¢ - åˆ©ç”¨ key_entities åšæ··åˆæ£€ç´¢
        key_entities = rewrite_result.get('key_entities', [])

        if key_entities and len(key_entities) > 1:
            # å¤šå®ä½“ï¼šå¯¹æ¯ä¸ªå…³é”®å®ä½“æ£€ç´¢ï¼Œç„¶ååˆå¹¶å»é‡
            logger.info(f"  ğŸ” Multi-entity search with: {key_entities}")
            all_chunks = []
            for entity in key_entities:
                entity_chunks = self.retrieve_chunks(entity, top_k=3, filters=filters)
                all_chunks.extend(entity_chunks)
                logger.info(f"    - Entity '{entity}': found {len(entity_chunks)} chunks")

            # å»é‡å¹¶æŒ‰åˆ†æ•°æ’åºï¼Œå–æ›´å¤šå€™é€‰ï¼ˆç”¨äºé‡æ’ï¼‰
            chunks = self._deduplicate_chunks(all_chunks)[:15]
            logger.info(f"  âœ… After deduplication: {len(chunks)} unique chunks")

            # ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œé‡æ’åº
            logger.info(f"  ğŸ”„ Reranking with original query: {user_query[:50]}...")
            chunks = self.rerank_chunks(user_query, chunks, top_n=8)
        else:
            # å•å®ä½“æˆ–æ— å®ä½“ï¼šç›´æ¥ç”¨æ”¹å†™åçš„æŸ¥è¯¢
            logger.info(f"  ğŸ” Single query search: {optimized_query[:50]}...")
            chunks = self.retrieve_chunks(optimized_query, top_k=8, filters=filters)

        memory_fallback = False
        if not chunks:
            memory_chunks = self.memory_cache.get_context_chunks(session_id)
            logger.info(f"  Memory fallback check (no retrieval): {len(memory_chunks)} cached exchanges")
            if memory_chunks:
                logger.info("  ğŸ” No retrieval results, falling back to conversation memory")
                chunks = memory_chunks
                memory_fallback = True
            else:
                history_chunk = self._build_history_chunk(conversation_history)
                if history_chunk:
                    logger.info("  ğŸ” No retrieval results, falling back to formatted conversation history")
                    chunks = [history_chunk]
                    memory_fallback = True
                else:
                    return {
                        "type": "no_results",
                        "response": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚æ‚¨å¯ä»¥æ¢ä¸ªæ–¹å¼æé—®å—ï¼Ÿ",
                        "intent": intent_result,
                        "rewrite": rewrite_result,
                        "sources": []
                    }

        # 3.5. æ£€æŸ¥ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¿‡æ»¤æ‰ä¸ç›¸å…³çš„æ£€ç´¢ç»“æœ
        has_rerank = not memory_fallback and any('rerank_score' in c for c in chunks)
        best_score = 1.0

        if not memory_fallback:
            if has_rerank:
                best_score = max(c.get('rerank_score', 0) for c in chunks) if chunks else 0
                score_threshold = self.rerank_score_threshold
                is_relevant = best_score >= score_threshold
                score_type = "rerank"
            else:
                best_score = min(c.get('score', float('inf')) for c in chunks) if chunks else float('inf')
                score_threshold = self.l2_distance_threshold
                is_relevant = best_score <= score_threshold
                score_type = "L2"

            if not is_relevant:
                memory_chunks = self.memory_cache.get_context_chunks(session_id)
                logger.info(f"  Memory fallback check (low relevance): {len(memory_chunks)} cached exchanges")
                if memory_chunks:
                    logger.info(f"  {score_type} score below threshold ({best_score:.3f}); using conversation memory fallback")
                    chunks = memory_chunks
                    memory_fallback = True
                    has_rerank = False
                else:
                    history_chunk = self._build_history_chunk(conversation_history)
                    if history_chunk:
                        logger.info(f"  {score_type} score below threshold ({best_score:.3f}); using conversation history fallback")
                        chunks = [history_chunk]
                        memory_fallback = True
                        has_rerank = False
                    else:
                        logger.info(f"  {score_type} score indicates no relevant results: {best_score:.3f}")
                        return {
                            "type": "no_results",
                            "response": "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚æˆ‘åªèƒ½å›ç­”ä¸å·²æœ‰æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ã€‚",
                            "intent": intent_result,
                            "rewrite": rewrite_result,
                            "sources": []
                        }

        if memory_fallback:
            best_score = 1.0
            eval_result = {
                "is_sufficient": True,
                "has_ambiguity": False,
                "confidence": 0.65,
                "ambiguity_type": "none",
                "clarification_hint": "",
                "reasoning": "ä½¿ç”¨å¯¹è¯è®°å¿†ç”Ÿæˆå›å¤"
            }
            logger.info("  Using conversation memory, skip relevance evaluation")
        else:
            eval_result = self.evaluate_confidence(user_query, chunks)
            logger.info(f"  Evaluation: sufficient={eval_result['is_sufficient']}, ambiguity={eval_result['has_ambiguity']} ({eval_result['ambiguity_type']}), score: {best_score:.3f}")

        # 5. åˆ¤æ–­æ£€ç´¢è´¨é‡
        if memory_fallback:
            retrieval_is_good = True
            retrieval_is_excellent = True
        elif has_rerank:
            retrieval_is_good = best_score >= self.rerank_good_threshold
            retrieval_is_excellent = best_score >= self.rerank_excellent_threshold
        else:
            retrieval_is_good = best_score < self.l2_good_threshold
            retrieval_is_excellent = best_score < self.l2_excellent_threshold

        # 6. å†³å®šå›ç­”ç­–ç•¥
        # æ–°ç­–ç•¥ï¼šåªè¦æ£€ç´¢åˆ°å†…å®¹ï¼Œå°±åŸºäºå†…å®¹å›ç­”ï¼ˆå¯å¸¦æ¾„æ¸…ï¼‰
        # - æ£€ç´¢åˆ°å†…å®¹ + æœ‰äºŒä¹‰æ€§ â†’ å…ˆç­”åé—® (answer_with_clarification)
        # - æ£€ç´¢åˆ°å†…å®¹ + ä¿¡æ¯ä¸å®Œæ•´ â†’ ç»™å‡ºåŸºç¡€ç­”æ¡ˆ + å¼•å¯¼è¡¥å……ä¿¡æ¯
        # - æ£€ç´¢åˆ°å†…å®¹ â†’ ç›´æ¥å›ç­”
        # - ä¸šåŠ¡ç›¸å…³æ€§ä½ + æ£€ç´¢æå·® â†’ è¶…å‡ºèŒƒå›´

        business_relevance = intent_result.get('business_relevance', 'medium')

        # åªæœ‰ä¸šåŠ¡ç›¸å…³æ€§ä½ä¸”æ£€ç´¢æå·®æ—¶æ‰è¯´è¶…å‡ºèŒƒå›´
        if business_relevance == 'low' and not retrieval_is_good:
            logger.info(f"  Query has low business relevance and very poor retrieval, out of scope")
            return {
                "type": "out_of_scope",
                "response": "æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜ä¼¼ä¹è¶…å‡ºäº†æˆ‘çš„çŸ¥è¯†èŒƒå›´ã€‚æˆ‘ä¸»è¦å¸®åŠ©è§£ç­”äº§å“æ‰‹å†Œã€å®‰è£…æ–‡æ¡£ã€è¯´æ˜ä¹¦ç­‰ç›¸å…³é—®é¢˜ã€‚æ‚¨å¯ä»¥æ¢ä¸ªäº§å“ç›¸å…³çš„é—®é¢˜è¯•è¯•ï¼Ÿ",
                "intent": intent_result,
                "sources": []
            }

        # å†³å®šæ˜¯å¦éœ€è¦æ¾„æ¸…æç¤º
        clarification_hint = ""
        response_type = "answer"

        # æƒ…å†µ1ï¼šæ£€ç´¢è´¨é‡å¥½ + æœ‰äºŒä¹‰æ€§ â†’ å…ˆç­”åé—®
        if retrieval_is_good and eval_result['has_ambiguity'] and eval_result['clarification_hint']:
            logger.info(f"  Good retrieval + ambiguity ({eval_result['ambiguity_type']}) â†’ answer with clarification")
            clarification_hint = eval_result['clarification_hint']
            response_type = "answer_with_clarification"

        # æƒ…å†µ2ï¼šæ£€ç´¢è´¨é‡ä¸€èˆ¬ + ä¿¡æ¯ä¸å®Œæ•´ â†’ ç»™åŸºç¡€ç­”æ¡ˆå¹¶å¼•å¯¼è¡¥å……
        elif retrieval_is_good and not eval_result['is_sufficient'] and eval_result.get('clarification_hint'):
            logger.info(f"  Moderate retrieval + insufficient info â†’ answer with hint for more details")
            clarification_hint = eval_result['clarification_hint']
            response_type = "answer_with_clarification"

        # æƒ…å†µ3ï¼šå…¶ä»–æƒ…å†µç›´æ¥å›ç­”
        else:
            logger.info(f"  Retrieval quality: {'excellent' if retrieval_is_excellent else 'good'}, direct answer")

        # 7. ç”Ÿæˆå›å¤ï¼ˆå¸¦æˆ–ä¸å¸¦æ¾„æ¸…æç¤ºï¼‰
        generation_chunks = chunks
        if not memory_fallback:
            history_chunk = self._build_history_chunk(conversation_history)
            if history_chunk:
                generation_chunks = chunks + [history_chunk]

        response_result = self.generate_response(
            user_query,
            generation_chunks,
            conversation_history,
            clarification_hint=clarification_hint,
            intent_note=intent_note
        )

        self.memory_cache.add_exchange(session_id, user_query, response_result['response'])

        return {
            "type": response_type,
            "response": response_result['response'],
            "confidence": response_result['confidence'],
            "sources": generation_chunks,
            "source_ids": response_result['source_ids'],
            "intent": intent_result,
            "rewrite": rewrite_result,
            "evaluation": eval_result
        }

    async def process_query_stream(
        self,
        user_query: str,
        conversation_history: str = "",
        filters: Dict = None,
        force_answer: bool = False,
        session_id: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œæ¯ä¸ªæ­¥éª¤å®Œæˆæ—¶ç«‹å³ yield ç»“æœ

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²
            filters: æ£€ç´¢è¿‡æ»¤æ¡ä»¶
            force_answer: æ˜¯å¦å¼ºåˆ¶å›ç­”
            session_id: ä¼šè¯IDï¼Œç”¨äºè®°å¿†ç¼“å­˜

        Yields:
            æ¯ä¸ªå¤„ç†æ­¥éª¤çš„ä¸­é—´ç»“æœ
        """
        logger.info(f"ğŸ” [STREAM] Processing query: {user_query[:100]}...")

        # æ­¥éª¤1: æ„å›¾è¯†åˆ«
        # å‘é€ reasoning æ¶ˆæ¯ï¼ˆç¬¦åˆå‰ç«¯è§„èŒƒï¼‰
        yield {"type": "reasoning", "content": "ğŸ” æ­£åœ¨è¯†åˆ«æ„å›¾..."}
        await asyncio.sleep(0)

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è°ƒç”¨ï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
        loop = asyncio.get_event_loop()
        intent_result = await loop.run_in_executor(
            None,
            self.classify_intent,
            user_query,
            conversation_history
        )
        logger.info(f"  Intent: {intent_result['intent']} (confidence: {intent_result['confidence']})")

        # æ„å»ºè¯¦ç»†çš„æ„å›¾è¯†åˆ«ç»“æœæ¶ˆæ¯
        intent_msg = f"âœ“ æ„å›¾è¯†åˆ«å®Œæˆ\n"
        intent_msg += f"  â€¢ æ„å›¾ç±»å‹: {intent_result['intent']}\n"
        intent_msg += f"  â€¢ ç½®ä¿¡åº¦: {intent_result['confidence']:.2f}\n"
        intent_msg += f"  â€¢ ä¸šåŠ¡ç›¸å…³æ€§: {intent_result['business_relevance']}"

        yield {"type": "reasoning", "content": intent_msg}
        await asyncio.sleep(0)

        # å¦‚æœæ˜¯é—²èŠä¸”ç½®ä¿¡åº¦é«˜ï¼Œæ ¹æ®é…ç½®å†³å®šå›å¤æ–¹å¼
        if intent_result['intent'] == 'chitchat' and intent_result['confidence'] >= self.chat_mode_threshold:
            if self.enable_chat_mode:
                yield {"type": "reasoning", "content": "æ£€æµ‹åˆ°é—²èŠæ„å›¾ï¼Œæ­£åœ¨ç”Ÿæˆå›å¤..."}
                await asyncio.sleep(0)

                llm_response = self.generate_chitchat_response(user_query, conversation_history)
                llm_response_with_notice = f"{llm_response}\n\n---\nâš ï¸ **æç¤º**ï¼šæ­¤å›å¤ç”± AI ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œè¯·æ³¨æ„ç”„åˆ«ã€‚"
                self.memory_cache.add_exchange(session_id, user_query, llm_response_with_notice)

                yield {"type": "content", "content": llm_response_with_notice}
                await asyncio.sleep(0)

                yield {"type": "done"}
                await asyncio.sleep(0)
                return
            else:
                fixed_reply = "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¡£åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ‚¨æŸ¥æ‰¾å’Œç†è§£æ–‡æ¡£å†…å®¹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"

                yield {"type": "content", "content": fixed_reply}
                await asyncio.sleep(0)

                yield {"type": "done"}
                await asyncio.sleep(0)

                self.memory_cache.add_exchange(session_id, user_query, fixed_reply)
                return

        # æ­¥éª¤2: æŸ¥è¯¢æ”¹å†™
        intent_note = ""
        if intent_result['intent'] == 'chitchat' and intent_result['confidence'] < self.chat_mode_threshold:
            logger.info(f"  é—²èŠæ„å›¾ç½®ä¿¡åº¦ä¸è¶³ï¼Œç»§ç»­æ£€ç´¢çŸ¥è¯†åº“")
            intent_note = "æ£€æµ‹åˆ°æ‚¨çš„é—®é¢˜å¯èƒ½åå‘é—²èŠï¼Œä½†æˆ‘å°è¯•åœ¨çŸ¥è¯†åº“ä¸­ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³å†…å®¹"

        yield {"type": "reasoning", "content": "ğŸ“ æ­£åœ¨ä¼˜åŒ–æŸ¥è¯¢..."}
        await asyncio.sleep(0)

        rewrite_result = await loop.run_in_executor(
            None,
            self.rewrite_query,
            user_query,
            conversation_history
        )
        optimized_query = rewrite_result['rewritten_query']
        logger.info(f"  Rewritten query: {optimized_query[:100]}...")

        # æ„å»ºè¯¦ç»†çš„æŸ¥è¯¢æ”¹å†™ç»“æœæ¶ˆæ¯
        rewrite_msg = f"âœ“ æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ\n"
        rewrite_msg += f"  â€¢ åŸå§‹æŸ¥è¯¢: {user_query}\n"
        rewrite_msg += f"  â€¢ ä¼˜åŒ–åæŸ¥è¯¢: {optimized_query}\n"
        key_entities = rewrite_result.get('key_entities', [])
        if key_entities:
            rewrite_msg += f"  â€¢ å…³é”®å®ä½“: {', '.join(key_entities)}\n"
        rewrite_msg += f"  â€¢ æ£€ç´¢ç­–ç•¥: {rewrite_result.get('search_strategy', 'semantic')}"

        yield {"type": "reasoning", "content": rewrite_msg}
        await asyncio.sleep(0)

        # æ­¥éª¤3: å‘é‡æ£€ç´¢
        yield {"type": "reasoning", "content": "ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."}
        await asyncio.sleep(0)

        key_entities = rewrite_result.get('key_entities', [])

        if key_entities and len(key_entities) > 1:
            logger.info(f"  ğŸ” Multi-entity search with: {key_entities}")
            all_chunks = []
            for entity in key_entities:
                entity_chunks = await loop.run_in_executor(
                    None,
                    self.retrieve_chunks,
                    entity,
                    self.entity_top_k,
                    filters
                )
                all_chunks.extend(entity_chunks)
                logger.info(f"    - Entity '{entity}': found {len(entity_chunks)} chunks")

            chunks = self._deduplicate_chunks(all_chunks)[:self.multi_entity_dedup_limit]
            logger.info(f"  âœ… After deduplication: {len(chunks)} unique chunks")

            yield {"type": "reasoning", "content": "ğŸ”„ æ­£åœ¨é‡æ’åºæ£€ç´¢ç»“æœ..."}
            await asyncio.sleep(0)
            chunks = await loop.run_in_executor(
                None,
                self.rerank_chunks,
                user_query,
                chunks,
                self.rerank_top_n
            )
        else:
            logger.info(f"  ğŸ” Single query search: {optimized_query[:50]}...")
            chunks = await loop.run_in_executor(
                None,
                self.retrieve_chunks,
                optimized_query,
                self.single_query_top_k,
                filters
            )

        memory_fallback = False
        if not chunks:
            memory_chunks = self.memory_cache.get_context_chunks(session_id)
            logger.info(f"  Memory fallback check (no retrieval): {len(memory_chunks)} cached exchanges")
            if memory_chunks:
                memory_fallback = True
                chunks = memory_chunks
                logger.info("  ğŸ” No retrieval results, falling back to conversation memory")
                yield {"type": "reasoning", "content": "ğŸ“ æœªæ£€ç´¢åˆ°æ–‡æ¡£ï¼Œæ”¹ç”¨æœ€è¿‘çš„å¯¹è¯è®°å¿†ç»§ç»­å›ç­”"}
                await asyncio.sleep(0)
            else:
                history_chunk = self._build_history_chunk(conversation_history)
                if history_chunk:
                    memory_fallback = True
                    chunks = [history_chunk]
                    logger.info("  ğŸ” No retrieval results, falling back to formatted conversation history")
                    yield {"type": "reasoning", "content": "ğŸ“ æœªæ£€ç´¢åˆ°æ–‡æ¡£ï¼Œæ”¹ç”¨ä¹‹å‰çš„å¯¹è¯å†…å®¹ç»§ç»­å›ç­”"}
                    await asyncio.sleep(0)

        # æ„å»ºè¯¦ç»†çš„æ£€ç´¢ç»“æœæ¶ˆæ¯
        if memory_fallback:
            retrieval_msg = f"âœ“ æœªå‘½ä¸­æ–‡æ¡£ç‰‡æ®µï¼Œä½¿ç”¨æœ€è¿‘ {len(chunks)} æ¡å¯¹è¯è®°å¿†ç»§ç»­æ¨ç†"
        else:
            retrieval_msg = f"âœ“ æ£€ç´¢å®Œæˆ\n  â€¢ æ‰¾åˆ° {len(chunks)} ä¸ªç›¸å…³ç‰‡æ®µ"
            if chunks:
                # æ˜¾ç¤ºå‰3ä¸ªç‰‡æ®µçš„æ¥æºå’Œåˆ†æ•°
                for i, chunk in enumerate(chunks[:3], 1):
                    score = chunk.get('rerank_score') or chunk.get('score', 0)
                    score_type = 'rerank' if 'rerank_score' in chunk else 'L2'
                    doc_name = chunk.get('document', 'Unknown')[:40]
                    retrieval_msg += f"\n  â€¢ [{i}] {doc_name} ({score:.3f})"
                if len(chunks) > 3:
                    retrieval_msg += f"\n  â€¢ ... è¿˜æœ‰ {len(chunks) - 3} ä¸ªç‰‡æ®µ"

        yield {"type": "reasoning", "content": retrieval_msg}
        await asyncio.sleep(0)

        # å‘é€æ£€ç´¢ç»“æœçš„ canvas è¡¨æ ¼å¯è§†åŒ–
        if chunks and len(chunks) > 0 and not memory_fallback:
            # æ„å»º Markdown è¡¨æ ¼
            table_md = "## ğŸ“š æ£€ç´¢ç»“æœè¯¦æƒ…\n\n"
            table_md += "| æ’å | æ–‡æ¡£æ¥æº | ç›¸å…³æ€§ | å†…å®¹é¢„è§ˆ |\n"
            table_md += "|------|----------|--------|----------|\n"

            for i, chunk in enumerate(chunks[:5], 1):
                score = chunk.get('rerank_score') or chunk.get('score', 0)
                score_type = 'rerank' if 'rerank_score' in chunk else 'L2'
                doc_name = chunk.get('document', 'Unknown')[:30]
                content_preview = chunk.get('content', '')[:60].replace('\n', ' ').replace('|', '\\|')
                table_md += f"| {i} | {doc_name} | {score:.3f} ({score_type}) | {content_preview}... |\n"

            yield {
                "type": "canvas",
                "content": {
                    "canvas-type": "markdown",
                    "canvas-source": table_md
                }
            }
            await asyncio.sleep(0)

        if not chunks:
            # è¾“å‡ºæœªæ‰¾åˆ°ä»»ä½•å†…å®¹çš„è¯¦ç»†ä¿¡æ¯
            no_results_msg = f"âŒ æ£€ç´¢ç»“æœ: æœªæ‰¾åˆ°ä»»ä½•å†…å®¹\n"
            no_results_msg += f"  â€¢ ä½¿ç”¨çš„æŸ¥è¯¢: {optimized_query}\n"
            if key_entities:
                no_results_msg += f"  â€¢ å°è¯•çš„å…³é”®è¯: {', '.join(key_entities)}\n"
            no_results_msg += f"  â€¢ æ£€ç´¢ç­–ç•¥: {rewrite_result.get('search_strategy', 'semantic')}\n"
            no_results_msg += f"\nğŸ’¡ å¯èƒ½çš„åŸå› :\n"
            no_results_msg += f"  â€¢ çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³æ–‡æ¡£\n"
            no_results_msg += f"  â€¢ å…³é”®è¯ä¸åŒ¹é…\n"
            no_results_msg += f"  â€¢ é—®é¢˜è¶…å‡ºæ–‡æ¡£èŒƒå›´"

            yield {"type": "reasoning", "content": no_results_msg}
            await asyncio.sleep(0)

            yield {
                "type": "content",
                "content": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚\n\nå»ºè®®ï¼š\n- å°è¯•ä½¿ç”¨å…¶ä»–å…³é”®è¯\n- ç®€åŒ–æˆ–å…·ä½“åŒ–æ‚¨çš„é—®é¢˜\n- ç¡®è®¤é—®é¢˜æ˜¯å¦å±äºæ–‡æ¡£æ¶µç›–çš„èŒƒå›´"
            }
            await asyncio.sleep(0)

            yield {"type": "done"}
            await asyncio.sleep(0)
            return

        # æ­¥éª¤4: è¯„ä¼°ç›¸ä¼¼åº¦å’Œç½®ä¿¡åº¦
        yield {"type": "reasoning", "content": "âš–ï¸  æ­£åœ¨è¯„ä¼°æ£€ç´¢ç»“æœè´¨é‡..."}
        await asyncio.sleep(0)

        # è®¡ç®—ç›¸ä¼¼åº¦
        has_rerank = not memory_fallback and any('rerank_score' in c for c in chunks)
        best_score = 1.0

        if not memory_fallback:
            if has_rerank:
                best_score = max(c.get('rerank_score', 0) for c in chunks) if chunks else 0
                score_threshold = self.rerank_score_threshold
                is_relevant = best_score >= score_threshold
                score_type = "rerank"
            else:
                best_score = min(c.get('score', float('inf')) for c in chunks) if chunks else float('inf')
                score_threshold = self.l2_distance_threshold
                is_relevant = best_score <= score_threshold
                score_type = "L2"

            if not is_relevant:
                memory_chunks = self.memory_cache.get_context_chunks(session_id)
                logger.info(f"  Memory fallback check (low relevance): {len(memory_chunks)} cached exchanges")
                if memory_chunks:
                    memory_fallback = True
                    chunks = memory_chunks
                    has_rerank = False
                    original_score = best_score
                    best_score = 1.0
                    logger.info(f"  {score_type} score below threshold ({original_score:.3f}); using conversation memory fallback")
                    yield {"type": "reasoning", "content": "ğŸ“ æ£€ç´¢ç›¸å…³åº¦åä½ï¼Œæ”¹ç”¨æœ€è¿‘çš„å¯¹è¯è®°å¿†ç»§ç»­å›ç­”"}
                    await asyncio.sleep(0)
                else:
                    history_chunk = self._build_history_chunk(conversation_history)
                    if history_chunk:
                        memory_fallback = True
                        chunks = [history_chunk]
                        has_rerank = False
                        best_score = 1.0
                        logger.info(f"  {score_type} score below threshold ({best_score:.3f}); using conversation history fallback")
                        yield {"type": "reasoning", "content": "ğŸ“ æ£€ç´¢ç›¸å…³åº¦åä½ï¼Œæ”¹ç”¨ä¹‹å‰çš„å¯¹è¯å†…å®¹ç»§ç»­å›ç­”"}
                        await asyncio.sleep(0)
                    else:
                        logger.info(f"  {score_type} score indicates no relevant results: {best_score:.3f}")

                        # è¾“å‡ºç›¸å…³åº¦è¿‡ä½çš„è¯¦ç»†ä¿¡æ¯
                        low_relevance_msg = f"âš ï¸  ç›¸å…³åº¦è¯„ä¼°: æœªè¾¾åˆ°é˜ˆå€¼\n"
                        low_relevance_msg += f"  â€¢ æ‰¾åˆ° {len(chunks)} ä¸ªç‰‡æ®µï¼Œä½†ç›¸å…³åº¦è¿‡ä½\n"
                        low_relevance_msg += f"  â€¢ æœ€ä½³ç›¸å…³åº¦åˆ†æ•°: {best_score:.3f} ({score_type})\n"
                        low_relevance_msg += f"  â€¢ é˜ˆå€¼è¦æ±‚: {score_threshold:.3f}\n"
                        low_relevance_msg += f"  â€¢ æœ€ç›¸å…³çš„æ–‡æ¡£:\n"

                        for i, chunk in enumerate(chunks[:3], 1):
                            score = chunk.get('rerank_score') or chunk.get('score', 0)
                            doc_name = chunk.get('document', 'Unknown')[:40]
                            low_relevance_msg += f"    [{i}] {doc_name} (åˆ†æ•°: {score:.3f})\n"

                        low_relevance_msg += f"\nğŸ’¡ å»ºè®®ï¼šè¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ï¼Œæˆ–æä¾›æ›´å…·ä½“çš„å…³é”®è¯"

                        yield {"type": "reasoning", "content": low_relevance_msg}
                        await asyncio.sleep(0)

                        # å‘é€æœ€ç»ˆå›å¤
                        yield {
                            "type": "content",
                            "content": "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†åº“ä¸­è™½ç„¶æ‰¾åˆ°äº†ä¸€äº›æ–‡æ¡£ç‰‡æ®µï¼Œä½†å®ƒä»¬ä¸æ‚¨çš„é—®é¢˜ç›¸å…³åº¦è¿‡ä½ï¼Œæ— æ³•ç»™å‡ºå¯é çš„å›ç­”ã€‚\n\nå»ºè®®ï¼š\n- å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n- æä¾›æ›´å…·ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n- ç¡®è®¤é—®é¢˜æ˜¯å¦åœ¨æ–‡æ¡£æ¶µç›–èŒƒå›´å†…"
                        }
                        await asyncio.sleep(0)

                        yield {"type": "done"}
                        await asyncio.sleep(0)
                        return

        if memory_fallback:
            eval_result = {
                "is_sufficient": True,
                "has_ambiguity": False,
                "confidence": 0.65,
                "ambiguity_type": "none",
                "clarification_hint": "",
                "reasoning": "ä½¿ç”¨å¯¹è¯è®°å¿†ç”Ÿæˆå›å¤"
            }
            logger.info("  Using conversation memory, skip confidence evaluation")
        else:
            eval_result = await loop.run_in_executor(
                None,
                self.evaluate_confidence,
                user_query,
                chunks
            )
            logger.info(f"  Evaluation: sufficient={eval_result['is_sufficient']}, ambiguity={eval_result['has_ambiguity']}")

        # æ„å»ºè¯¦ç»†çš„è¯„ä¼°ç»“æœæ¶ˆæ¯
        eval_msg = f"âœ“ è¯„ä¼°å®Œæˆ\n"
        eval_msg += f"  â€¢ ç½®ä¿¡åº¦: {eval_result['confidence']:.2f}\n"
        eval_msg += f"  â€¢ ä¿¡æ¯å……åˆ†æ€§: {'å……åˆ†' if eval_result['is_sufficient'] else 'ä¸å……åˆ†'}\n"
        eval_msg += f"  â€¢ æ˜¯å¦æœ‰æ­§ä¹‰: {'æ˜¯' if eval_result['has_ambiguity'] else 'å¦'}"
        if eval_result.get('ambiguity_type') and eval_result['ambiguity_type'] != 'none':
            eval_msg += f"\n  â€¢ æ­§ä¹‰ç±»å‹: {eval_result['ambiguity_type']}"

        yield {"type": "reasoning", "content": eval_msg}
        await asyncio.sleep(0)

        # æ­¥éª¤5: åˆ¤æ–­å›ç­”ç­–ç•¥
        if memory_fallback:
            retrieval_is_good = True
            retrieval_is_excellent = True
        elif has_rerank:
            retrieval_is_good = best_score >= self.rerank_good_threshold
            retrieval_is_excellent = best_score >= self.rerank_excellent_threshold
        else:
            retrieval_is_good = best_score < self.l2_good_threshold
            retrieval_is_excellent = best_score < self.l2_excellent_threshold

        business_relevance = intent_result.get('business_relevance', 'medium')

        if business_relevance == 'low' and not retrieval_is_good:
            yield {
                "type": "content",
                "content": "æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜ä¼¼ä¹è¶…å‡ºäº†æˆ‘çš„çŸ¥è¯†èŒƒå›´ã€‚æˆ‘ä¸»è¦å¸®åŠ©è§£ç­”äº§å“æ‰‹å†Œã€å®‰è£…æ–‡æ¡£ã€è¯´æ˜ä¹¦ç­‰ç›¸å…³é—®é¢˜ã€‚æ‚¨å¯ä»¥æ¢ä¸ªäº§å“ç›¸å…³çš„é—®é¢˜è¯•è¯•ï¼Ÿ"
            }
            await asyncio.sleep(0)

            yield {"type": "done"}
            await asyncio.sleep(0)
            return

        clarification_hint = ""
        response_type = "answer"

        if retrieval_is_good and eval_result['has_ambiguity'] and eval_result['clarification_hint']:
            clarification_hint = eval_result['clarification_hint']
            response_type = "answer_with_clarification"
        elif retrieval_is_good and not eval_result['is_sufficient'] and eval_result.get('clarification_hint'):
            clarification_hint = eval_result['clarification_hint']
            response_type = "answer_with_clarification"

        # æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆå›å¤
        yield {"type": "reasoning", "content": "ğŸ’¡ æ­£åœ¨ç”Ÿæˆå›å¤..."}
        await asyncio.sleep(0)

        generation_chunks = chunks
        if not memory_fallback:
            history_chunk = self._build_history_chunk(conversation_history)
            if history_chunk:
                generation_chunks = chunks + [history_chunk]

        response_result = await loop.run_in_executor(
            None,
            self.generate_response,
            user_query,
            generation_chunks,
            conversation_history,
            clarification_hint,
            intent_note
        )

        self.memory_cache.add_exchange(session_id, user_query, response_result['response'])

        # å‘é€æœ€ç»ˆå›å¤å†…å®¹
        yield {"type": "content", "content": response_result['response']}
        await asyncio.sleep(0)

        # å‘é€æ–‡ä»¶æºä¿¡æ¯
        if not memory_fallback:
            # ä»ç¯å¢ƒå˜é‡è·å– API åŸºç¡€ URL
            import os
            from urllib.parse import quote
            api_base_url = os.getenv("API_BASE_URL", "http://localhost:8086")

            for chunk in generation_chunks[:self.files_display_limit]:
                metadata_type = chunk.get('metadata', {}).get('type')
                if metadata_type in {"conversation_history", "conversation_memory"}:
                    continue
                doc_name = chunk.get('document', 'Unknown')
                chunk_db_id = chunk.get('chunk_db_id', '')  # ä½¿ç”¨æ•°æ®åº“ä¸»é”®ID

                # æ„é€ å®Œæ•´çš„ API å¯¼èˆª URL
                if chunk_db_id and doc_name and doc_name != 'Unknown':
                    # URL ç¼–ç æ–‡æ¡£åä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
                    encoded_doc_name = quote(doc_name)
                    file_path = f"{api_base_url}/api/view/document/{encoded_doc_name}/chunk/{chunk_db_id}"
                else:
                    # é™çº§ï¼šä½¿ç”¨ä¼ ç»Ÿæ ¼å¼
                    file_path = chunk.get('metadata', {}).get('file_path', '') or f"#chunk-{chunk_db_id}"

                if doc_name:
                    yield {
                        "type": "files",
                        "content": {
                            "fileName": doc_name,
                            "filePath": file_path,
                            "chunkDbId": chunk_db_id,  # ä¼ é€’æ•°æ®åº“ä¸»é”®ID
                            "sourceFile": doc_name  # ä¼ é€’æºæ–‡ä»¶å
                        }
                    }
                    await asyncio.sleep(0)
