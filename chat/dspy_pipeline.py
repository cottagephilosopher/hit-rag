"""
DSPy RAG Pipeline å®ç°
æ•´åˆæ„å›¾è¯†åˆ«ã€æŸ¥è¯¢æ”¹å†™ã€æ£€ç´¢ã€è¯„ä¼°å’Œå›å¤ç”Ÿæˆ
"""

import dspy
import json
import logging
from typing import List, Dict, Any, Optional, Tuple

from .dspy_signatures import (
    IntentClassification,
    QueryRewrite,
    ConfidenceEvaluation,
    ClarificationGeneration,
    ResponseGeneration
)

logger = logging.getLogger(__name__)


class DSPyRAGPipeline:
    """DSPy RAG Pipeline ä¸»ç±»"""

    def __init__(
        self,
        vector_store=None,
        llm_model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        confidence_threshold: float = 0.5
    ):
        """
        åˆå§‹åŒ– DSPy Pipeline

        Args:
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆMilvusï¼‰
            llm_model: LLM æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.vector_store = vector_store
        self.confidence_threshold = confidence_threshold

        # é…ç½® DSPy LLM
        self._configure_dspy(llm_model, temperature)

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.intent_classifier = dspy.ChainOfThought(IntentClassification)
        self.query_rewriter = dspy.ChainOfThought(QueryRewrite)
        self.confidence_evaluator = dspy.ChainOfThought(ConfidenceEvaluation)
        self.clarification_generator = dspy.ChainOfThought(ClarificationGeneration)
        self.response_generator = dspy.ChainOfThought(ResponseGeneration)

        logger.info(f"âœ… DSPy RAG Pipeline initialized with model: {llm_model}")

    def _configure_dspy(self, model: str, temperature: float):
        """é…ç½® DSPy çš„ LLM"""
        try:
            import os

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Azure OpenAI
            llm_provider = os.getenv('LLM_PROVIDER', 'openai')

            if llm_provider == 'azure':
                # Azure OpenAI é…ç½®
                azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
                azure_key = os.getenv('AZURE_OPENAI_API_KEY')
                azure_deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4o')
                azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')

                # ä½¿ç”¨ azure/ å‰ç¼€æ ¼å¼
                model_name = f"azure/{azure_deployment}"

                lm = dspy.LM(
                    model=model_name,
                    api_base=azure_endpoint,
                    api_key=azure_key,
                    api_version=azure_api_version,
                    temperature=temperature,
                    max_tokens=2000
                )
                logger.info(f"âœ… DSPy configured with Azure OpenAI: {azure_deployment}")
            else:
                api_base = os.getenv('API_BASE')
                api_key = os.getenv('API_KEY')
                model = os.getenv('MODEL_NAME', 'gpt-4o')
                # æ ‡å‡† OpenAI é…ç½®
                lm = dspy.LM(model=model, api_base=api_base, api_key=api_key, temperature=temperature, max_tokens=2000)
                logger.info(f"âœ… DSPy configured with OpenAI: {model}")

            dspy.configure(lm=lm)

        except Exception as e:
            logger.error(f"âŒ Failed to configure DSPy: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def classify_intent(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤1: è¯†åˆ«ç”¨æˆ·æ„å›¾

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ„å›¾åˆ†ç±»ç»“æœ
        """
        try:
            result = self.intent_classifier(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query
            )

            return {
                "intent": result.intent.lower(),
                "confidence": float(result.confidence),
                "reasoning": result.reasoning
            }
        except Exception as e:
            logger.error(f"âŒ Intent classification failed: {e}")
            # é»˜è®¤è®¤ä¸ºæ˜¯é—®ç­”
            return {
                "intent": "question",
                "confidence": 0.5,
                "reasoning": f"åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤ä¸ºé—®ç­”: {e}"
            }

    def rewrite_query(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤2: æ”¹å†™æŸ¥è¯¢

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ”¹å†™ç»“æœ
        """
        try:
            result = self.query_rewriter(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query
            )

            # è§£æ key_entities
            try:
                key_entities = json.loads(result.key_entities)
            except:
                key_entities = [result.key_entities]

            return {
                "rewritten_query": result.rewritten_query,
                "key_entities": key_entities,
                "search_strategy": result.search_strategy
            }
        except Exception as e:
            logger.error(f"âŒ Query rewrite failed: {e}")
            return {
                "rewritten_query": user_query,
                "key_entities": [],
                "search_strategy": "semantic"
            }

    def retrieve_chunks(
        self,
        query: str,
        top_k: int = 5,
        filters: Dict = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3: å‘é‡æ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not self.vector_store:
            logger.warning("âš ï¸  No vector store configured")
            return []

        try:
            # ä½¿ç”¨ search_with_score æ–¹æ³•ï¼ˆè¿”å›å¸¦åˆ†æ•°çš„ç»“æœï¼‰
            results = self.vector_store.search_with_score(
                query=query,
                k=top_k,
                filters=filters
            )

            # æ ¼å¼åŒ–ç»“æœ - search_with_score è¿”å› [(Document, score), ...]
            # Document æ˜¯ LangChain Documentï¼Œæœ‰ .page_content å’Œ .metadata
            chunks = []
            for doc, score in results:
                metadata = doc.metadata
                chunks.append({
                    "chunk_id": metadata.get("chunk_db_id") or metadata.get("pk"),
                    "content": metadata.get("original_content") or doc.page_content,
                    "score": score,
                    "document": metadata.get("source_file", "Unknown"),
                    "metadata": metadata
                })

            logger.info(f"âœ… Retrieved {len(chunks)} chunks")
            return chunks

        except Exception as e:
            logger.error(f"âŒ Retrieval failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def evaluate_confidence(
        self,
        user_query: str,
        retrieved_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤4: è¯„ä¼°æ£€ç´¢ç»“æœçš„å……åˆ†æ€§

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ

        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            # æ ¼å¼åŒ– chunks ä¸ºæ–‡æœ¬
            chunks_text = json.dumps(
                [{"content": c["content"], "source": c.get("document", "")}
                 for c in retrieved_chunks[:3]],  # åªå–å‰3ä¸ªé¿å…å¤ªé•¿
                ensure_ascii=False,
                indent=2
            )

            result = self.confidence_evaluator(
                user_query=user_query,
                retrieved_chunks=chunks_text
            )

            # è§£æ missing_info
            try:
                missing_info = json.loads(result.missing_info)
            except:
                missing_info = [result.missing_info] if result.missing_info else []

            return {
                "is_sufficient": result.is_sufficient.lower() == "yes",
                "confidence": float(result.confidence),
                "missing_info": missing_info,
                "reasoning": result.reasoning
            }
        except Exception as e:
            logger.error(f"âŒ Confidence evaluation failed: {e}")
            # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œä¿å®ˆåœ°è®¤ä¸ºä¿¡æ¯å……åˆ†ï¼ˆé¿å…è¿‡åº¦åé—®ï¼‰
            return {
                "is_sufficient": True,
                "confidence": 0.5,
                "missing_info": [],
                "reasoning": f"è¯„ä¼°å¤±è´¥: {e}"
            }

    def generate_clarification(
        self,
        user_query: str,
        missing_info: List[str],
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤5: ç”Ÿæˆæ¾„æ¸…é—®é¢˜

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            missing_info: ç¼ºå¤±çš„ä¿¡æ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ¾„æ¸…é—®é¢˜
        """
        try:
            missing_info_text = json.dumps(missing_info, ensure_ascii=False)

            result = self.clarification_generator(
                user_query=user_query,
                missing_info=missing_info_text,
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯"
            )

            # è§£æé€‰é¡¹
            try:
                options = json.loads(result.suggested_options)
            except:
                options = []

            return {
                "question": result.clarification_question,
                "options": options
            }
        except Exception as e:
            logger.error(f"âŒ Clarification generation failed: {e}")
            return {
                "question": f"æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚æ‚¨èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹ {missing_info[0] if missing_info else 'æ‚¨çš„éœ€æ±‚'} å—ï¼Ÿ",
                "options": []
            }

    def generate_response(
        self,
        user_query: str,
        retrieved_chunks: List[Dict[str, Any]],
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆå›å¤

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
            conversation_history: å¯¹è¯å†å²

        Returns:
            ç”Ÿæˆçš„å›å¤
        """
        try:
            # æ ¼å¼åŒ– chunks
            chunks_text = json.dumps(
                [{
                    "id": c.get("chunk_id"),
                    "content": c["content"],
                    "source": c.get("document", ""),
                    "score": c.get("score", 0.0)
                } for c in retrieved_chunks],
                ensure_ascii=False,
                indent=2
            )

            result = self.response_generator(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query,
                retrieved_chunks=chunks_text
            )

            # è§£æ source_ids
            try:
                source_ids = json.loads(result.source_ids)
            except:
                source_ids = [c.get("chunk_id") for c in retrieved_chunks[:3]]

            return {
                "response": result.response,
                "source_ids": source_ids,
                "confidence": float(result.confidence),
                "sources": retrieved_chunks
            }
        except Exception as e:
            logger.error(f"âŒ Response generation failed: {e}")
            return {
                "response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚",
                "source_ids": [],
                "confidence": 0.0,
                "sources": []
            }

    def process_query(
        self,
        user_query: str,
        conversation_history: str = "",
        filters: Dict = None,
        force_answer: bool = False
    ) -> Dict[str, Any]:
        """
        å®Œæ•´å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä¸»å…¥å£ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²
            filters: æ£€ç´¢è¿‡æ»¤æ¡ä»¶
            force_answer: æ˜¯å¦å¼ºåˆ¶å›ç­”ï¼ˆä¸ç”Ÿæˆæ¾„æ¸…é—®é¢˜ï¼‰

        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸ” Processing query: {user_query[:100]}...")

        # 1. æ„å›¾è¯†åˆ«
        intent_result = self.classify_intent(user_query, conversation_history)
        logger.info(f"  Intent: {intent_result['intent']} (confidence: {intent_result['confidence']})")

        # å¦‚æœæ˜¯é—²èŠï¼Œç›´æ¥ç®€å•å›å¤
        if intent_result['intent'] == 'chitchat':
            return {
                "type": "chitchat",
                "response": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¡£åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ‚¨æŸ¥æ‰¾å’Œç†è§£æ–‡æ¡£å†…å®¹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ",
                "intent": intent_result
            }

        # 2. æŸ¥è¯¢æ”¹å†™
        rewrite_result = self.rewrite_query(user_query, conversation_history)
        optimized_query = rewrite_result['rewritten_query']
        logger.info(f"  Rewritten query: {optimized_query[:100]}...")

        # 3. å‘é‡æ£€ç´¢
        chunks = self.retrieve_chunks(optimized_query, top_k=5, filters=filters)

        if not chunks:
            return {
                "type": "no_results",
                "response": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚æ‚¨å¯ä»¥æ¢ä¸ªæ–¹å¼æé—®å—ï¼Ÿ",
                "intent": intent_result,
                "rewrite": rewrite_result,
                "sources": []
            }

        # 3.5. æ£€æŸ¥ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¿‡æ»¤æ‰ä¸ç›¸å…³çš„æ£€ç´¢ç»“æœ
        # Milvus ä½¿ç”¨ L2 è·ç¦»ï¼šå€¼è¶Šå°è¶Šç›¸ä¼¼ï¼ˆ0=å®Œå…¨ç›¸åŒï¼‰
        # - ç›¸å…³æ–‡æ¡£é€šå¸¸ < 1.0
        # - ä¸ç›¸å…³æ–‡æ¡£é€šå¸¸ > 1.2
        min_score = min(c.get('score', float('inf')) for c in chunks) if chunks else float('inf')

        # è®¾ç½®é˜ˆå€¼ä¸º 1.2ï¼Œè¶…è¿‡æ­¤å€¼è®¤ä¸ºæ–‡æ¡£åº“ä¸­æ²¡æœ‰ç›¸å…³å†…å®¹
        if min_score > 1.2:
            logger.info(f"  Min L2 distance too high: {min_score:.3f}, treating as no relevant results")
            return {
                "type": "no_results",
                "response": "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚æˆ‘åªèƒ½å›ç­”ä¸å·²æœ‰æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ã€‚",
                "intent": intent_result,
                "rewrite": rewrite_result,
                "sources": []
            }

        # 4. è¯„ä¼°ç½®ä¿¡åº¦
        eval_result = self.evaluate_confidence(user_query, chunks)
        logger.info(f"  Confidence: {eval_result['confidence']} (sufficient: {eval_result['is_sufficient']}), Min L2 distance: {min_score:.3f}")

        # 5. åˆ¤æ–­æ˜¯å¦éœ€è¦æ¾„æ¸…
        # å¯¹äºéå¸¸çŸ­çš„æŸ¥è¯¢ä¸”æ£€ç´¢åˆ†æ•°è¾ƒå¥½ï¼ˆ<1.1ï¼‰ï¼Œä¼˜å…ˆç›´æ¥å›ç­”è€Œä¸æ¾„æ¸…
        # L2è·ç¦»: <0.8=å¾ˆç›¸å…³, <1.0=ç›¸å…³, <1.2=å¯èƒ½ç›¸å…³, >1.2=ä¸ç›¸å…³

        # åˆ¤æ–­æŸ¥è¯¢æ˜¯å¦ç®€çŸ­ï¼šä¸­æ–‡<=5å­— æˆ– è‹±æ–‡<=3ä¸ªå•è¯
        query_text = user_query.strip()
        chinese_char_count = sum(1 for c in query_text if '\u4e00' <= c <= '\u9fff')
        word_count = len(query_text.split())

        query_is_short = (
            chinese_char_count > 0 and chinese_char_count <= 5  # ä¸­æ–‡çŸ­æŸ¥è¯¢
        ) or (
            chinese_char_count == 0 and word_count <= 3  # è‹±æ–‡çŸ­æŸ¥è¯¢
        )

        retrieval_is_good = min_score < 1.1  # æ”¾å®½é˜ˆå€¼ä»¥åŒ…æ‹¬1.049è¿™æ ·çš„æƒ…å†µ

        should_skip_clarification = (
            force_answer or
            (query_is_short and retrieval_is_good) or
            eval_result['is_sufficient']
        )

        if not should_skip_clarification and eval_result['missing_info']:
            clarification = self.generate_clarification(
                user_query,
                eval_result['missing_info'],
                conversation_history
            )
            logger.info(f"  Generating clarification (query_length={len(user_query)}, min_score={min_score:.3f})")
            return {
                "type": "clarification",
                "question": clarification['question'],
                "options": clarification['options'],
                "intent": intent_result,
                "rewrite": rewrite_result,
                "evaluation": eval_result,
                "sources": chunks[:3]
            }

        # å¦‚æœè·³è¿‡æ¾„æ¸…ï¼Œè®°å½•åŸå› 
        if query_is_short and retrieval_is_good and not eval_result['is_sufficient']:
            logger.info(f"  Skipping clarification due to short query with good retrieval (query_length={len(user_query)}, min_score={min_score:.3f})")

        # 6. ç”Ÿæˆå›å¤
        response_result = self.generate_response(user_query, chunks, conversation_history)

        return {
            "type": "answer",
            "response": response_result['response'],
            "confidence": response_result['confidence'],
            "sources": chunks,
            "source_ids": response_result['source_ids'],
            "intent": intent_result,
            "rewrite": rewrite_result,
            "evaluation": eval_result
        }
