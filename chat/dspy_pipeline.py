"""
DSPy RAG Pipeline å®ç°
æ•´åˆæ„å›¾è¯†åˆ«ã€æŸ¥è¯¢æ”¹å†™ã€æ£€ç´¢ã€è¯„ä¼°å’Œå›å¤ç”Ÿæˆ
"""

import dspy
import json
import os
import requests
from loguru import logger
from typing import List, Dict, Any, Optional, Tuple

from .dspy_signatures import (
    IntentClassification,
    QueryRewrite,
    ConfidenceEvaluation,
    ClarificationGeneration,
    ResponseGeneration
)


class DSPyRAGPipeline:
    """DSPy RAG Pipeline ä¸»ç±»"""

    def __init__(
        self,
        vector_store=None,
        llm_model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        confidence_threshold: float = 0.5
    ):
        """
        åˆå§‹åŒ– DSPy Pipeline

        Args:
            vector_store: å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆMilvusï¼‰
            llm_model: LLM æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.vector_store = vector_store
        self.confidence_threshold = confidence_threshold

        # è¯»å–é—²èŠæ¨¡å¼é…ç½®
        self.enable_chat_mode = os.getenv('ENABLE_CHAT_MODE', 'false').lower() == 'true'
        self.chat_mode_threshold = float(os.getenv('CHAT_MODE_THRESHOLD', '0.7'))

        # é…ç½® DSPy LLM
        self._configure_dspy(llm_model, temperature)

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.intent_classifier = dspy.ChainOfThought(IntentClassification)
        self.query_rewriter = dspy.ChainOfThought(QueryRewrite)
        self.confidence_evaluator = dspy.ChainOfThought(ConfidenceEvaluation)
        self.clarification_generator = dspy.ChainOfThought(ClarificationGeneration)
        self.response_generator = dspy.ChainOfThought(ResponseGeneration)

        logger.info(f"âœ… DSPy RAG Pipeline initialized with model: {llm_model}")
        logger.info(f"   é—²èŠæ¨¡å¼: {'å¼€å¯' if self.enable_chat_mode else 'å…³é—­'} (ç½®ä¿¡åº¦é˜ˆå€¼: {self.chat_mode_threshold})")

    def _configure_dspy(self, model: str, temperature: float):
        """é…ç½® DSPy çš„ LLM"""
        try:
            import os

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Azure OpenAI
            llm_provider = os.getenv('LLM_PROVIDER', 'openai')

            if llm_provider == 'azure':
                # Azure OpenAI é…ç½®
                azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
                azure_key = os.getenv('AZURE_OPENAI_API_KEY')
                azure_deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4o')
                azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview')

                # ä½¿ç”¨ azure/ å‰ç¼€æ ¼å¼
                model_name = f"azure/{azure_deployment}"

                lm = dspy.LM(
                    model=model_name,
                    api_base=azure_endpoint,
                    api_key=azure_key,
                    api_version=azure_api_version,
                    temperature=temperature,
                    max_tokens=2000
                )
                logger.info(f"âœ… DSPy configured with Azure OpenAI: {azure_deployment}")
            else:
                api_base = os.getenv('API_BASE')
                api_key = os.getenv('API_KEY')
                model = os.getenv('MODEL_NAME', 'gpt-4o')
                # æ ‡å‡† OpenAI é…ç½®
                lm = dspy.LM(model=model, api_base=api_base, api_key=api_key, temperature=temperature, max_tokens=2000)
                logger.info(f"âœ… DSPy configured with OpenAI: {model}")

            dspy.configure(lm=lm)

        except Exception as e:
            logger.error(f"âŒ Failed to configure DSPy: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise

    def classify_intent(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤1: è¯†åˆ«ç”¨æˆ·æ„å›¾

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ„å›¾åˆ†ç±»ç»“æœ
        """
        try:
            result = self.intent_classifier(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query
            )

            return {
                "intent": result.intent.lower(),
                "confidence": float(result.confidence),
                "business_relevance": result.business_relevance.lower(),
                "reasoning": result.reasoning
            }
        except Exception as e:
            logger.error(f"âŒ Intent classification failed: {e}")
            # é»˜è®¤è®¤ä¸ºæ˜¯é—®ç­”
            return {
                "intent": "question",
                "confidence": 0.5,
                "business_relevance": "medium",
                "reasoning": f"åˆ†ç±»å¤±è´¥ï¼Œé»˜è®¤ä¸ºé—®ç­”: {e}"
            }

    def rewrite_query(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤2: æ”¹å†™æŸ¥è¯¢

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ”¹å†™ç»“æœ
        """
        try:
            result = self.query_rewriter(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query
            )

            # è§£æ key_entities
            try:
                key_entities = json.loads(result.key_entities)
            except:
                key_entities = [result.key_entities]

            return {
                "rewritten_query": result.rewritten_query,
                "key_entities": key_entities,
                "search_strategy": result.search_strategy
            }
        except Exception as e:
            logger.error(f"âŒ Query rewrite failed: {e}")
            return {
                "rewritten_query": user_query,
                "key_entities": [],
                "search_strategy": "semantic"
            }

    def retrieve_chunks(
        self,
        query: str,
        top_k: int = 5,
        filters: Dict = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3: å‘é‡æ£€ç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not self.vector_store:
            logger.warning("âš ï¸  No vector store configured")
            return []

        try:
            # ä½¿ç”¨ search_with_score æ–¹æ³•ï¼ˆè¿”å›å¸¦åˆ†æ•°çš„ç»“æœï¼‰
            results = self.vector_store.search_with_score(
                query=query,
                k=top_k,
                filters=filters
            )

            # æ ¼å¼åŒ–ç»“æœ - search_with_score è¿”å› [(Document, score), ...]
            # Document æ˜¯ LangChain Documentï¼Œæœ‰ .page_content å’Œ .metadata
            chunks = []
            for doc, score in results:
                metadata = doc.metadata
                chunks.append({
                    "chunk_id": metadata.get("chunk_db_id") or metadata.get("pk"),
                    "content": metadata.get("original_content") or doc.page_content,
                    "score": score,
                    "document": metadata.get("source_file", "Unknown"),
                    "metadata": metadata
                })

            logger.info(f"âœ… Retrieved {len(chunks)} chunks")
            return chunks

        except Exception as e:
            logger.error(f"âŒ Retrieval failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []

    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡å¹¶æŒ‰åˆ†æ•°æ’åºï¼ˆL2è·ç¦»è¶Šå°è¶Šå¥½ï¼‰

        Args:
            chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨

        Returns:
            å»é‡åçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨ï¼ŒæŒ‰åˆ†æ•°å‡åºæ’åº
        """
        seen_ids = set()
        unique_chunks = []

        for chunk in chunks:
            chunk_id = chunk.get('chunk_id')
            if chunk_id and chunk_id not in seen_ids:
                seen_ids.add(chunk_id)
                unique_chunks.append(chunk)
            elif not chunk_id:
                # å¦‚æœæ²¡æœ‰chunk_idï¼Œç”¨contentçš„hashä½œä¸ºæ ‡è¯†
                content_hash = hash(chunk.get('content', ''))
                if content_hash not in seen_ids:
                    seen_ids.add(content_hash)
                    unique_chunks.append(chunk)

        # æŒ‰L2è·ç¦»å‡åºæ’åºï¼ˆåˆ†æ•°è¶Šå°è¶Šå¥½ï¼‰
        unique_chunks.sort(key=lambda x: x.get('score', float('inf')))
        return unique_chunks

    def rerank_chunks(
        self,
        query: str,
        chunks: List[Dict[str, Any]],
        top_n: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ DashScope é‡æ’åºæ£€ç´¢ç»“æœ

        Args:
            query: åŸå§‹ç”¨æˆ·æŸ¥è¯¢
            chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªç»“æœ

        Returns:
            é‡æ’åºåçš„æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        if not chunks:
            return chunks

        try:
            api_key = os.getenv("DASHSCOPE_API_KEY")
            if not api_key:
                logger.warning("âš ï¸  DASHSCOPE_API_KEY not found, skipping rerank")
                return chunks[:top_n]

            # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
            documents = [chunk.get('content', '') for chunk in chunks]

            # è°ƒç”¨ DashScope rerank API
            url = "https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank"
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "gte-rerank-v2",
                "input": {
                    "query": query,
                    "documents": documents
                },
                "parameters": {
                    "return_documents": True,
                    "top_n": top_n
                }
            }

            response = requests.post(url, headers=headers, json=payload, timeout=10)
            response.raise_for_status()

            result = response.json()

            # è§£æé‡æ’åºç»“æœ - DashScope æˆåŠŸæ—¶è¿”å› output å­—æ®µ
            if "output" in result and "results" in result["output"]:
                reranked_results = result["output"]["results"]
                reranked_chunks = []

                for item in reranked_results:
                    original_index = item.get("index")
                    relevance_score = item.get("relevance_score")

                    if original_index is not None and original_index < len(chunks):
                        chunk = chunks[original_index].copy()
                        chunk['rerank_score'] = relevance_score
                        reranked_chunks.append(chunk)

                logger.info(f"âœ… Reranked {len(reranked_chunks)} chunks (scores: {[round(c.get('rerank_score', 0), 3) for c in reranked_chunks[:3]]}...)")
                return reranked_chunks
            else:
                logger.warning(f"âš ï¸  Rerank API returned unexpected format: {result}")
                return chunks[:top_n]

        except Exception as e:
            logger.error(f"âŒ Rerank failed: {e}")
            # é™çº§ï¼šè¿”å›åŸå§‹æ’åºç»“æœ
            return chunks[:top_n]

    def evaluate_confidence(
        self,
        user_query: str,
        retrieved_chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤4: è¯„ä¼°æ£€ç´¢ç»“æœçš„å……åˆ†æ€§

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ

        Returns:
            è¯„ä¼°ç»“æœ
        """
        try:
            # æ ¼å¼åŒ– chunks ä¸ºæ–‡æœ¬
            chunks_text = json.dumps(
                [{"content": c["content"], "source": c.get("document", "")}
                 for c in retrieved_chunks[:5]],  # åªå–å‰3ä¸ªé¿å…å¤ªé•¿
                ensure_ascii=False,
                indent=2
            )

            result = self.confidence_evaluator(
                user_query=user_query,
                retrieved_chunks=chunks_text
            )

            return {
                "is_sufficient": result.is_sufficient.lower() == "yes",
                "has_ambiguity": result.has_ambiguity.lower() == "yes",
                "confidence": float(result.confidence),
                "ambiguity_type": result.ambiguity_type,
                "clarification_hint": result.clarification_hint,
                "reasoning": result.reasoning
            }
        except Exception as e:
            logger.error(f"âŒ Confidence evaluation failed: {e}")
            # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œä¿å®ˆåœ°è®¤ä¸ºä¿¡æ¯å……åˆ†ï¼ˆé¿å…è¿‡åº¦åé—®ï¼‰
            return {
                "is_sufficient": True,
                "has_ambiguity": False,
                "confidence": 0.5,
                "ambiguity_type": "none",
                "clarification_hint": "",
                "reasoning": f"è¯„ä¼°å¤±è´¥: {e}"
            }

    def generate_clarification(
        self,
        user_query: str,
        missing_info: List[str],
        conversation_history: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤5: ç”Ÿæˆæ¾„æ¸…é—®é¢˜

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            missing_info: ç¼ºå¤±çš„ä¿¡æ¯
            conversation_history: å¯¹è¯å†å²

        Returns:
            æ¾„æ¸…é—®é¢˜
        """
        try:
            missing_info_text = json.dumps(missing_info, ensure_ascii=False)

            result = self.clarification_generator(
                user_query=user_query,
                missing_info=missing_info_text,
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯"
            )

            # è§£æé€‰é¡¹
            try:
                options = json.loads(result.suggested_options)
            except:
                options = []

            return {
                "question": result.clarification_question,
                "options": options
            }
        except Exception as e:
            logger.error(f"âŒ Clarification generation failed: {e}")
            return {
                "question": f"æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚æ‚¨èƒ½è¯¦ç»†è¯´æ˜ä¸€ä¸‹ {missing_info[0] if missing_info else 'æ‚¨çš„éœ€æ±‚'} å—ï¼Ÿ",
                "options": []
            }

    def generate_chitchat_response(
        self,
        user_query: str,
        conversation_history: str = ""
    ) -> str:
        """
        ç”Ÿæˆé—²èŠå›å¤ï¼ˆä½¿ç”¨ LLMï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²

        Returns:
            ç”Ÿæˆçš„é—²èŠå›å¤
        """
        try:
            # ä½¿ç”¨ dspy.LM ç›´æ¥ç”Ÿæˆå›å¤
            lm = dspy.settings.lm

            system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„æ–‡æ¡£åŠ©æ‰‹ã€‚å½“ç”¨æˆ·è¿›è¡Œé—²èŠæ—¶ï¼Œè¯·ï¼š
1. ä¿æŒå‹å¥½ã€ä¸“ä¸šçš„æ€åº¦
2. ç®€çŸ­å›å¤ï¼Œä¸è¦è¿‡äºå†—é•¿
3. é€‚å½“å¼•å¯¼ç”¨æˆ·æå‡ºæ–‡æ¡£ç›¸å…³é—®é¢˜
4. å¦‚æœç”¨æˆ·é—®å€™ï¼Œå¯ä»¥ä»‹ç»ä½ çš„åŠŸèƒ½"""

            # æ„å»ºå®Œæ•´çš„æç¤ºæ–‡æœ¬
            full_prompt = system_prompt + "\n\n"

            # æ·»åŠ å¯¹è¯å†å²
            if conversation_history:
                full_prompt += f"å¯¹è¯å†å²:\n{conversation_history}\n\n"

            # æ·»åŠ å½“å‰ç”¨æˆ·æŸ¥è¯¢
            full_prompt += f"ç”¨æˆ·: {user_query}\nåŠ©æ‰‹:"

            # è°ƒç”¨ LLMï¼ˆç›´æ¥ä¼ å…¥æ–‡æœ¬è€Œä¸æ˜¯æ¶ˆæ¯åˆ—è¡¨ï¼‰
            response = lm(full_prompt)

            # æå–å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæ˜¯åˆ—è¡¨åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰
            if isinstance(response, list) and len(response) > 0:
                response_text = response[0]
            elif isinstance(response, str):
                response_text = response
            else:
                response_text = str(response)

            return response_text

        except Exception as e:
            logger.error(f"âŒ Chitchat response generation failed: {e}")
            # é™çº§åˆ°é»˜è®¤å›å¤
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¡£åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ‚¨æŸ¥æ‰¾å’Œç†è§£æ–‡æ¡£å†…å®¹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"

    def generate_response(
        self,
        user_query: str,
        retrieved_chunks: List[Dict[str, Any]],
        conversation_history: str = "",
        clarification_hint: str = "",
        intent_note: str = ""
    ) -> Dict[str, Any]:
        """
        æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆå›å¤

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_chunks: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µ
            conversation_history: å¯¹è¯å†å²
            clarification_hint: å¯é€‰çš„æ¾„æ¸…æç¤ºï¼ˆç”¨äº"å…ˆç­”åé—®"ï¼‰
            intent_note: æ„å›¾è¯†åˆ«å¤‡æ³¨

        Returns:
            ç”Ÿæˆçš„å›å¤
        """
        try:
            # æ ¼å¼åŒ– chunks
            chunks_text = json.dumps(
                [{
                    "id": c.get("chunk_id"),
                    "content": c["content"],
                    "source": c.get("document", ""),
                    "score": c.get("score", 0.0)
                } for c in retrieved_chunks],
                ensure_ascii=False,
                indent=2
            )

            result = self.response_generator(
                conversation_history=conversation_history or "æ— å†å²å¯¹è¯",
                user_query=user_query,
                retrieved_chunks=chunks_text,
                clarification_hint=clarification_hint,
                intent_note=intent_note
            )

            # è§£æ source_ids
            try:
                source_ids = json.loads(result.source_ids)
            except:
                source_ids = [c.get("chunk_id") for c in retrieved_chunks[:3]]

            return {
                "response": result.response,
                "source_ids": source_ids,
                "confidence": float(result.confidence),
                "sources": retrieved_chunks
            }
        except Exception as e:
            logger.error(f"âŒ Response generation failed: {e}")
            return {
                "response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç”Ÿæˆå›å¤æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚",
                "source_ids": [],
                "confidence": 0.0,
                "sources": []
            }

    def process_query(
        self,
        user_query: str,
        conversation_history: str = "",
        filters: Dict = None,
        force_answer: bool = False
    ) -> Dict[str, Any]:
        """
        å®Œæ•´å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆä¸»å…¥å£ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            conversation_history: å¯¹è¯å†å²
            filters: æ£€ç´¢è¿‡æ»¤æ¡ä»¶
            force_answer: æ˜¯å¦å¼ºåˆ¶å›ç­”ï¼ˆä¸ç”Ÿæˆæ¾„æ¸…é—®é¢˜ï¼‰

        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸ” Processing query: {user_query[:100]}...")

        # 1. æ„å›¾è¯†åˆ«
        intent_result = self.classify_intent(user_query, conversation_history)
        logger.info(f"  Intent: {intent_result['intent']} (confidence: {intent_result['confidence']})")

        # å¦‚æœæ˜¯é—²èŠä¸”ç½®ä¿¡åº¦é«˜ï¼Œæ ¹æ®é…ç½®å†³å®šå›å¤æ–¹å¼
        if intent_result['intent'] == 'chitchat' and intent_result['confidence'] >= self.chat_mode_threshold:
            # æ£€æŸ¥æ˜¯å¦å¼€å¯é—²èŠæ¨¡å¼
            if self.enable_chat_mode:
                logger.info(f"  é—²èŠæ¨¡å¼å·²å¼€å¯ï¼Œç½®ä¿¡åº¦ {intent_result['confidence']:.2f} >= {self.chat_mode_threshold}ï¼Œä½¿ç”¨ LLM å›å¤")
                llm_response = self.generate_chitchat_response(user_query, conversation_history)
                # åœ¨å›å¤æœ«å°¾æ·»åŠ æ˜æ˜¾çš„ AI ç”Ÿæˆæé†’
                llm_response_with_notice = f"{llm_response}\n\n---\nâš ï¸ **æç¤º**ï¼šæ­¤å›å¤ç”± AI ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œè¯·æ³¨æ„ç”„åˆ«ã€‚"
                return {
                    "type": "chitchat",
                    "response": llm_response_with_notice,
                    "intent": intent_result,
                    "chat_mode": "llm"
                }
            else:
                # æœªå¼€å¯é—²èŠæ¨¡å¼ï¼Œä½¿ç”¨å›ºå®šå›å¤
                logger.info(f"  é—²èŠæ¨¡å¼æœªå¼€å¯ï¼Œä½¿ç”¨å›ºå®šå›å¤")
                return {
                    "type": "chitchat",
                    "response": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ–‡æ¡£åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ‚¨æŸ¥æ‰¾å’Œç†è§£æ–‡æ¡£å†…å®¹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ",
                    "intent": intent_result,
                    "chat_mode": "fixed"
                }

        # ç½®ä¿¡åº¦ä¸è¶³æˆ–éé—²èŠæ„å›¾ï¼Œç»§ç»­è¿›è¡ŒçŸ¥è¯†åº“æ£€ç´¢
        intent_note = ""
        if intent_result['intent'] == 'chitchat' and intent_result['confidence'] < self.chat_mode_threshold:
            logger.info(f"  é—²èŠæ„å›¾ç½®ä¿¡åº¦ä¸è¶³ ({intent_result['confidence']:.2f} < {self.chat_mode_threshold})ï¼Œç»§ç»­æ£€ç´¢çŸ¥è¯†åº“")
            intent_note = "æ£€æµ‹åˆ°æ‚¨çš„é—®é¢˜å¯èƒ½åå‘é—²èŠï¼Œä½†æˆ‘å°è¯•åœ¨çŸ¥è¯†åº“ä¸­ä¸ºæ‚¨æŸ¥æ‰¾ç›¸å…³å†…å®¹"

        # 2. æŸ¥è¯¢æ”¹å†™
        rewrite_result = self.rewrite_query(user_query, conversation_history)
        logger.info(f"  --- Rewritten query: {rewrite_result}")
        optimized_query = rewrite_result['rewritten_query']
        logger.info(f"  Rewritten query: {optimized_query[:100]}...")

        # 3. å‘é‡æ£€ç´¢ - åˆ©ç”¨ key_entities åšæ··åˆæ£€ç´¢
        key_entities = rewrite_result.get('key_entities', [])

        if key_entities and len(key_entities) > 1:
            # å¤šå®ä½“ï¼šå¯¹æ¯ä¸ªå…³é”®å®ä½“æ£€ç´¢ï¼Œç„¶ååˆå¹¶å»é‡
            logger.info(f"  ğŸ” Multi-entity search with: {key_entities}")
            all_chunks = []
            for entity in key_entities:
                entity_chunks = self.retrieve_chunks(entity, top_k=3, filters=filters)
                all_chunks.extend(entity_chunks)
                logger.info(f"    - Entity '{entity}': found {len(entity_chunks)} chunks")

            # å»é‡å¹¶æŒ‰åˆ†æ•°æ’åºï¼Œå–æ›´å¤šå€™é€‰ï¼ˆç”¨äºé‡æ’ï¼‰
            chunks = self._deduplicate_chunks(all_chunks)[:15]
            logger.info(f"  âœ… After deduplication: {len(chunks)} unique chunks")

            # ä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œé‡æ’åº
            logger.info(f"  ğŸ”„ Reranking with original query: {user_query[:50]}...")
            chunks = self.rerank_chunks(user_query, chunks, top_n=8)
        else:
            # å•å®ä½“æˆ–æ— å®ä½“ï¼šç›´æ¥ç”¨æ”¹å†™åçš„æŸ¥è¯¢
            logger.info(f"  ğŸ” Single query search: {optimized_query[:50]}...")
            chunks = self.retrieve_chunks(optimized_query, top_k=8, filters=filters)

        if not chunks:
            return {
                "type": "no_results",
                "response": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚æ‚¨å¯ä»¥æ¢ä¸ªæ–¹å¼æé—®å—ï¼Ÿ",
                "intent": intent_result,
                "rewrite": rewrite_result,
                "sources": []
            }

        # 3.5. æ£€æŸ¥ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œè¿‡æ»¤æ‰ä¸ç›¸å…³çš„æ£€ç´¢ç»“æœ
        # ä¼˜å…ˆä½¿ç”¨é‡æ’åˆ†æ•°ï¼ˆrerank_scoreï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ L2 è·ç¦»ï¼ˆscoreï¼‰
        # - rerank_score: 0.3+ ç›¸å…³ï¼Œ0.5+ å¾ˆç›¸å…³
        # - L2è·ç¦»: <0.8 å¾ˆç›¸å…³, <1.0 ç›¸å…³, <1.2 å¯èƒ½ç›¸å…³, >1.2 ä¸ç›¸å…³

        # è®¡ç®—æœ€ä½³ç›¸ä¼¼åº¦æŒ‡æ ‡
        has_rerank = any('rerank_score' in c for c in chunks)

        if has_rerank:
            # ä½¿ç”¨é‡æ’åˆ†æ•°ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
            best_score = max(c.get('rerank_score', 0) for c in chunks) if chunks else 0
            score_threshold = 0.2  # rerank_score ä½äº 0.2 è®¤ä¸ºä¸ç›¸å…³
            is_relevant = best_score >= score_threshold
            score_type = "rerank"
        else:
            # ä½¿ç”¨ L2 è·ç¦»ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            best_score = min(c.get('score', float('inf')) for c in chunks) if chunks else float('inf')
            score_threshold = 1.2  # L2 è·ç¦»é«˜äº 1.2 è®¤ä¸ºä¸ç›¸å…³
            is_relevant = best_score <= score_threshold
            score_type = "L2"

        if not is_relevant:
            logger.info(f"  {score_type} score indicates no relevant results: {best_score:.3f}")
            return {
                "type": "no_results",
                "response": "æŠ±æ­‰ï¼Œæˆ‘çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚æˆ‘åªèƒ½å›ç­”ä¸å·²æœ‰æ–‡æ¡£ç›¸å…³çš„é—®é¢˜ã€‚",
                "intent": intent_result,
                "rewrite": rewrite_result,
                "sources": []
            }

        # 4. è¯„ä¼°ç½®ä¿¡åº¦å’ŒäºŒä¹‰æ€§
        eval_result = self.evaluate_confidence(user_query, chunks)
        logger.info(f"  Evaluation: sufficient={eval_result['is_sufficient']}, ambiguity={eval_result['has_ambiguity']} ({eval_result['ambiguity_type']}), score: {best_score:.3f}")

        # 5. åˆ¤æ–­æ£€ç´¢è´¨é‡
        if has_rerank:
            # rerank_score: 0.2+ å¯ç”¨ï¼Œ0.3+ è‰¯å¥½ï¼Œ0.5+ ä¼˜ç§€
            retrieval_is_good = best_score >= 0.2
            retrieval_is_excellent = best_score >= 0.3
        else:
            # L2 è·ç¦»: <1.2 å¯ç”¨ï¼Œ<1.0 è‰¯å¥½ï¼Œ<0.8 ä¼˜ç§€
            retrieval_is_good = best_score < 1.2
            retrieval_is_excellent = best_score < 1.0

        # 6. å†³å®šå›ç­”ç­–ç•¥
        # æ–°ç­–ç•¥ï¼šåªè¦æ£€ç´¢åˆ°å†…å®¹ï¼Œå°±åŸºäºå†…å®¹å›ç­”ï¼ˆå¯å¸¦æ¾„æ¸…ï¼‰
        # - æ£€ç´¢åˆ°å†…å®¹ + æœ‰äºŒä¹‰æ€§ â†’ å…ˆç­”åé—® (answer_with_clarification)
        # - æ£€ç´¢åˆ°å†…å®¹ + ä¿¡æ¯ä¸å®Œæ•´ â†’ ç»™å‡ºåŸºç¡€ç­”æ¡ˆ + å¼•å¯¼è¡¥å……ä¿¡æ¯
        # - æ£€ç´¢åˆ°å†…å®¹ â†’ ç›´æ¥å›ç­”
        # - ä¸šåŠ¡ç›¸å…³æ€§ä½ + æ£€ç´¢æå·® â†’ è¶…å‡ºèŒƒå›´

        business_relevance = intent_result.get('business_relevance', 'medium')

        # åªæœ‰ä¸šåŠ¡ç›¸å…³æ€§ä½ä¸”æ£€ç´¢æå·®æ—¶æ‰è¯´è¶…å‡ºèŒƒå›´
        if business_relevance == 'low' and not retrieval_is_good:
            logger.info(f"  Query has low business relevance and very poor retrieval, out of scope")
            return {
                "type": "out_of_scope",
                "response": "æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜ä¼¼ä¹è¶…å‡ºäº†æˆ‘çš„çŸ¥è¯†èŒƒå›´ã€‚æˆ‘ä¸»è¦å¸®åŠ©è§£ç­”äº§å“æ‰‹å†Œã€å®‰è£…æ–‡æ¡£ã€è¯´æ˜ä¹¦ç­‰ç›¸å…³é—®é¢˜ã€‚æ‚¨å¯ä»¥æ¢ä¸ªäº§å“ç›¸å…³çš„é—®é¢˜è¯•è¯•ï¼Ÿ",
                "intent": intent_result,
                "sources": []
            }

        # å†³å®šæ˜¯å¦éœ€è¦æ¾„æ¸…æç¤º
        clarification_hint = ""
        response_type = "answer"

        # æƒ…å†µ1ï¼šæ£€ç´¢è´¨é‡å¥½ + æœ‰äºŒä¹‰æ€§ â†’ å…ˆç­”åé—®
        if retrieval_is_good and eval_result['has_ambiguity'] and eval_result['clarification_hint']:
            logger.info(f"  Good retrieval + ambiguity ({eval_result['ambiguity_type']}) â†’ answer with clarification")
            clarification_hint = eval_result['clarification_hint']
            response_type = "answer_with_clarification"

        # æƒ…å†µ2ï¼šæ£€ç´¢è´¨é‡ä¸€èˆ¬ + ä¿¡æ¯ä¸å®Œæ•´ â†’ ç»™åŸºç¡€ç­”æ¡ˆå¹¶å¼•å¯¼è¡¥å……
        elif retrieval_is_good and not eval_result['is_sufficient'] and eval_result.get('clarification_hint'):
            logger.info(f"  Moderate retrieval + insufficient info â†’ answer with hint for more details")
            clarification_hint = eval_result['clarification_hint']
            response_type = "answer_with_clarification"

        # æƒ…å†µ3ï¼šå…¶ä»–æƒ…å†µç›´æ¥å›ç­”
        else:
            logger.info(f"  Retrieval quality: {'excellent' if retrieval_is_excellent else 'good'}, direct answer")

        # 7. ç”Ÿæˆå›å¤ï¼ˆå¸¦æˆ–ä¸å¸¦æ¾„æ¸…æç¤ºï¼‰
        response_result = self.generate_response(
            user_query,
            chunks,
            conversation_history,
            clarification_hint=clarification_hint,
            intent_note=intent_note
        )

        return {
            "type": response_type,
            "response": response_result['response'],
            "confidence": response_result['confidence'],
            "sources": chunks,
            "source_ids": response_result['source_ids'],
            "intent": intent_result,
            "rewrite": rewrite_result,
            "evaluation": eval_result
        }
